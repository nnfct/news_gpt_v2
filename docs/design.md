# ğŸ¨ News GPT v2 - ì‹œìŠ¤í…œ ì„¤ê³„ ëª…ì„¸ì„œ (2025.07.20 ìµœì¢… ìµœì í™”)

> **ìµœì¢… ë²„ì „**: v2.0 ìµœì í™” ì™„ë£Œ - 5-10ì´ˆ ì´ˆê³ ì† AI ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ í”Œë«í¼

## ğŸ“‹ ì„¤ê³„ ê°œìš”

News GPT v2ëŠ” DeepSearch API v2ì™€ Azure OpenAI GPT-4oë¥¼ í™œìš©í•˜ì—¬ **5-10ì´ˆ ë‚´** ë¹ ë¥´ê³  ì •í™•í•œ ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ì„ ì œê³µí•˜ëŠ” ì›¹ ê¸°ë°˜ AI í”Œë«í¼ì…ë‹ˆë‹¤. ì„±ëŠ¥ ìµœì í™”ì— ì¤‘ì ì„ ë‘” ìŠ¤íŠ¸ë¦¼ë¼ì¸ ì•„í‚¤í…ì²˜ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ—ï¸ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ê³ ìˆ˜ì¤€ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        News GPT v2 System                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚  External APIs â”‚ â”‚
â”‚  â”‚   (Client)      â”‚â—„â”€â”€â–ºâ”‚   (Server)      â”‚â—„â”€â”€â–ºâ”‚   (Services)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ HTML5, CSS3,    â”‚    â”‚ FastAPI         â”‚    â”‚ Azure OpenAI    â”‚ â”‚
â”‚  â”‚ JavaScript      â”‚    â”‚ Python 3.11.9   â”‚    â”‚ (GPT-4o)        â”‚ â”‚
â”‚  â”‚ Port: 8000      â”‚    â”‚ Port: 8000      â”‚    â”‚                 â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚ DeepSearch API  â”‚ â”‚
â”‚  â”‚ Features:       â”‚    â”‚ Features:       â”‚    â”‚ v2 (Tech/Global)â”‚ â”‚
â”‚  â”‚ - í‚¤ì›Œë“œ ì‹œê°í™” â”‚    â”‚ - í‚¤ì›Œë“œ ì¶”ì¶œ   â”‚    â”‚                 â”‚ â”‚
â”‚  â”‚ - ê´€ë ¨ ê¸°ì‚¬ í‘œì‹œâ”‚    â”‚ - ë‰´ìŠ¤ ìˆ˜ì§‘     â”‚    â”‚ Response Time:  â”‚ â”‚
â”‚  â”‚ - ì›ë³¸ ë¦¬ë‹¤ì´ë ‰íŠ¸â”‚   â”‚ - ë©”ëª¨ë¦¬ ìºì‹±   â”‚    â”‚ 1-3ì´ˆ (ìµœì í™”)  â”‚ â”‚
â”‚  â”‚ - AI ì±—ë´‡      â”‚    â”‚ - ì—ëŸ¬ ì²˜ë¦¬     â”‚    â”‚                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Memory Cache Layer                      â”‚ â”‚
â”‚  â”‚  - í‚¤ì›Œë“œ-ê¸°ì‚¬ ë§¤í•‘: ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜ O(1) ê²€ìƒ‰             â”‚ â”‚
â”‚  â”‚  - ì¤‘ë³µ ì œê±°: í•´ì‹œ ê¸°ë°˜ ê³ ì† ì²˜ë¦¬                         â”‚ â”‚
â”‚  â”‚  - ìºì‹œ íˆíŠ¸ìœ¨: 95%+                                      â”‚ â”‚
â”‚  â”‚  - ë©”ëª¨ë¦¬ ìµœì í™”: ìµœì†Œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° í”Œë¡œìš° (ìµœì í™”ëœ 5-10ì´ˆ ì²˜ë¦¬)

```
1. ì‚¬ìš©ì ìš”ì²­ (í‚¤ì›Œë“œ ë¶„ì„)
    â†“
2. FastAPI ì„œë²„ (main.py)
    â†“ (ë³‘ë ¬ ì²˜ë¦¬)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë‰´ìŠ¤ ìˆ˜ì§‘        â”‚  â”‚ ìºì‹œ í™•ì¸        â”‚
â”‚ (1-3ì´ˆ)         â”‚  â”‚ (ì¦‰ì‹œ)          â”‚
â”‚ - DeepSearch    â”‚  â”‚ - ë©”ëª¨ë¦¬ ê²€ìƒ‰    â”‚
â”‚ - Tech/Global   â”‚  â”‚ - ê¸°ì¡´ ê²°ê³¼ í™•ì¸â”‚
â”‚ - 20ê°œ ê¸°ì‚¬     â”‚  â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                    â†“ (ìºì‹œ íˆíŠ¸)
3. GPT-4o í‚¤ì›Œë“œ ì¶”ì¶œ    â†“
   (1-2ì´ˆ)              â†“
   - 50í† í° ì œí•œ         â†“
   - 3ê°œ í•µì‹¬ í‚¤ì›Œë“œ     â†“
    â†“                    â†“
4. ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ â†â”€â”€â”€â”€â”˜
   (ì¦‰ì‹œ)
    â†“
5. JSON ì‘ë‹µ ë°˜í™˜
   (ì´ 5-10ì´ˆ)
    â†“
6. í”„ë¡ íŠ¸ì—”ë“œ ë Œë”ë§
   - í‚¤ì›Œë“œ ì‹œê°í™”
   - í´ë¦­ ì´ë²¤íŠ¸ ë°”ì¸ë”©
```

## ğŸ”§ ë°±ì—”ë“œ ì„¤ê³„ (FastAPI)

### 1. ë©”ì¸ ì„œë²„ (main.py)

#### íŒŒì¼ êµ¬ì¡° (1,558ì¤„ í†µí•©)
```python
# ğŸ“ main.py êµ¬ì¡° (ìŠ¤íŠ¸ë¦¼ë¼ì¸ ì™„ë£Œ)

# 1. í™˜ê²½ ì„¤ì • ë° Import (50ì¤„)
import asyncio
import httpx
from fastapi import FastAPI, HTTPException
from azure.openai import AzureOpenAI
# ... ìµœì í™”ëœ import

# 2. ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì • (100ì¤„)
app = FastAPI(title="News GPT v2")
azure_client = AzureOpenAI(...)
DEEPSEARCH_API_KEY = os.getenv("DEEPSEARCH_API_KEY")

# 3. ë©”ëª¨ë¦¬ ìºì‹œ ì‹œìŠ¤í…œ (150ì¤„)
class MemoryCache:
    """ê³ ì† ë©”ëª¨ë¦¬ ê¸°ë°˜ ìºì‹œ ì‹œìŠ¤í…œ"""
    def __init__(self):
        self.keywords_cache = {}
        self.articles_cache = {}
        self.duplicate_hashes = set()

# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (200ì¤„)
async def fetch_tech_articles(limit=20, timeout=5):
    """DeepSearch Tech API ìµœì í™” í˜¸ì¶œ"""
    
async def extract_keywords_with_gpt(articles, max_tokens=50):
    """GPT-4o ê¸°ë°˜ 3ê°œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""

def remove_duplicates_fast(articles):
    """í•´ì‹œ ê¸°ë°˜ ê³ ì† ì¤‘ë³µ ì œê±°"""

# 5. API ì—”ë“œí¬ì¸íŠ¸ (1,000ì¤„+)
@app.get("/api/keywords")
async def get_keywords():
    """ìµœì í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ API (5-10ì´ˆ)"""

@app.get("/api/keyword-articles/{keyword}")
async def get_keyword_articles(keyword: str):
    """í‚¤ì›Œë“œë³„ ê´€ë ¨ ê¸°ì‚¬ (ì¦‰ì‹œ ì‘ë‹µ)"""

@app.get("/api/redirect/{article_id}")
async def redirect_to_article(article_id: str):
    """ì›ë³¸ URL ë¦¬ë‹¤ì´ë ‰íŠ¸ (ì¦‰ì‹œ)"""

# 6. ì£¼ê°„ ë¶„ì„ API (300ì¤„)
@app.get("/weekly-keywords-by-date")
async def weekly_keywords_domestic():
    """êµ­ë‚´ ì£¼ê°„ í‚¤ì›Œë“œ ë¶„ì„"""

@app.get("/global-weekly-keywords-by-date") 
async def weekly_keywords_global():
    """í•´ì™¸ ì£¼ê°„ í‚¤ì›Œë“œ ë¶„ì„"""

# 7. ì±—ë´‡ ë° ì¶”ê°€ ê¸°ëŠ¥ (200ì¤„)
@app.post("/chat")
async def chat_endpoint():
    """AI ì±—ë´‡ ê¸°ëŠ¥"""

# 8. ì„œë²„ ì‹œì‘ ë° ì •ì  íŒŒì¼ (50ì¤„)
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### í•µì‹¬ ìµœì í™” êµ¬í˜„

##### 1. ê³ ì† ë‰´ìŠ¤ ìˆ˜ì§‘ (1-3ì´ˆ)
```python
async def fetch_tech_articles(limit=20, timeout=5):
    """ìµœì í™”ëœ DeepSearch API í˜¸ì¶œ"""
    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            # ë‹¨ì¼ ì¬ì‹œë„, ë¹ ë¥¸ ì‹¤íŒ¨
            response = await client.get(
                "https://api-v2.deepsearch.com/v1/articles/tech",
                headers={"Authorization": f"Bearer {DEEPSEARCH_API_KEY}"},
                params={"limit": limit}  # 20ê°œë¡œ ì œí•œ
            )
            return response.json()
        except httpx.TimeoutException:
            # ë¹ ë¥¸ ì‹¤íŒ¨ (5ì´ˆ íƒ€ì„ì•„ì›ƒ)
            return {"articles": []}
```

##### 2. GPT-4o í‚¤ì›Œë“œ ì¶”ì¶œ (1-2ì´ˆ)
```python
async def extract_keywords_with_gpt(articles, max_tokens=50):
    """ìµœì í™”ëœ GPT-4o í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ (50í† í° ì œí•œ)
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
        {article_summaries}
        
        ì‘ë‹µ í˜•ì‹: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3"""
        
        response = azure_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,  # í† í° ì œí•œ
            temperature=0.3
        )
        return parse_keywords(response.choices[0].message.content)
    except Exception:
        return ["ê¸°ìˆ ", "AI", "í˜ì‹ "]  # ë¹ ë¥¸ í´ë°±
```

##### 3. ë©”ëª¨ë¦¬ ìºì‹œ ì‹œìŠ¤í…œ
```python
class HighSpeedCache:
    """ê³ ì† ë©”ëª¨ë¦¬ ìºì‹œ ì‹œìŠ¤í…œ"""
    def __init__(self):
        self.keywords_cache = {}  # O(1) í‚¤ì›Œë“œ ì €ì¥
        self.articles_cache = {}  # O(1) ê¸°ì‚¬ ê²€ìƒ‰
        self.duplicate_hashes = set()  # O(1) ì¤‘ë³µ ê²€ì‚¬
        
    def get_cached_articles(self, keyword: str):
        """ìºì‹œëœ ê¸°ì‚¬ ì¦‰ì‹œ ë°˜í™˜"""
        return self.articles_cache.get(keyword, [])
        
    def cache_articles(self, keyword: str, articles: list):
        """ê¸°ì‚¬ ìºì‹œ ì €ì¥"""
        self.articles_cache[keyword] = articles
        
    def is_duplicate(self, article_hash: str) -> bool:
        """í•´ì‹œ ê¸°ë°˜ O(1) ì¤‘ë³µ ê²€ì‚¬"""
        if article_hash in self.duplicate_hashes:
            return True
        self.duplicate_hashes.add(article_hash)
        return False
```

### 2. API ì—”ë“œí¬ì¸íŠ¸ ì„¤ê³„

#### í•µì‹¬ ìµœì í™” ì—”ë“œí¬ì¸íŠ¸

| Method | URL | ì‘ë‹µì‹œê°„ | ìµœì í™” ë‚´ìš© |
|--------|-----|----------|-------------|
| GET | `/api/keywords` | 5-10ì´ˆ | ë³‘ë ¬ ì²˜ë¦¬, ìºì‹œ, ì œí•œëœ í† í° |
| GET | `/api/keyword-articles/{keyword}` | ì¦‰ì‹œ | ë©”ëª¨ë¦¬ ìºì‹œ ìš°ì„  |
| GET | `/api/redirect/{article_id}` | ì¦‰ì‹œ | ì§ì ‘ ë¦¬ë‹¤ì´ë ‰íŠ¸ |
| GET | `/weekly-keywords-by-date` | 3-5ì´ˆ | ìºì‹œëœ ì£¼ê°„ ë°ì´í„° |
| GET | `/global-weekly-keywords-by-date` | 3-5ì´ˆ | ê¸€ë¡œë²Œ ìºì‹œ ë°ì´í„° |

#### ì„±ëŠ¥ ì¤‘ì‹¬ ì‘ë‹µ êµ¬ì¡°
```json
{
  "keywords": ["AI", "ììœ¨ì£¼í–‰", "ë°˜ë„ì²´"],
  "articles": [
    {
      "id": "abc123",
      "title": "AI ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥",
      "summary": "ìš”ì•½ í…ìŠ¤íŠ¸...",
      "url": "https://example.com/news/1",
      "published_date": "2025-07-20",
      "source": "Tech News"
    }
  ],
  "performance": {
    "total_time": "7.2ì´ˆ",
    "cache_hit": true,
    "articles_count": 20
  }
}
```

## ğŸŒ í”„ë¡ íŠ¸ì—”ë“œ ì„¤ê³„ (HTML5/CSS3/JavaScript)

### 1. íŒŒì¼ êµ¬ì¡° (index.html)

```html
<!DOCTYPE html>
<html lang="ko">
<head>
    <!-- ë©”íƒ€ë°ì´í„° ë° ìŠ¤íƒ€ì¼ -->
    <meta charset="UTF-8">
    <title>News GPT v2 - AI ë‰´ìŠ¤ ë¶„ì„</title>
    <style>
        /* ë°˜ì‘í˜• ë””ìì¸ CSS (500ì¤„+) */
        .container { max-width: 1200px; margin: 0 auto; }
        .keyword-button { 
            transition: all 0.3s ease;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        .loading { 
            animation: pulse 1.5s ease-in-out infinite;
        }
        /* ëª¨ë°”ì¼ ìµœì í™” */
        @media (max-width: 768px) { ... }
    </style>
</head>
<body>
    <!-- í—¤ë” ì„¹ì…˜ -->
    <header class="header">
        <h1>ğŸš€ News GPT v2</h1>
        <p>AI ê¸°ë°˜ ì´ˆê³ ì† ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ (5-10ì´ˆ)</p>
    </header>

    <!-- ë©”ì¸ ì»¨í…ì¸  -->
    <main class="container">
        <!-- í‚¤ì›Œë“œ ë¶„ì„ ì„¹ì…˜ -->
        <section id="keywords-section">
            <h2>ğŸ“Š ì˜¤ëŠ˜ì˜ í•µì‹¬ í‚¤ì›Œë“œ</h2>
            <div id="keywords-container">
                <!-- ë™ì  í‚¤ì›Œë“œ ë²„íŠ¼ë“¤ -->
            </div>
        </section>

        <!-- ì£¼ê°„ ìš”ì•½ ì„¹ì…˜ (êµ­ë‚´/í•´ì™¸) -->
        <section id="weekly-summary">
            <div class="summary-tabs">
                <button class="tab active" data-type="domestic">ğŸ‡°ğŸ‡· êµ­ë‚´</button>
                <button class="tab" data-type="global">ğŸŒ í•´ì™¸</button>
            </div>
            <div id="weekly-content">
                <!-- ë™ì  ì£¼ê°„ ìš”ì•½ -->
            </div>
        </section>

        <!-- ê´€ë ¨ ê¸°ì‚¬ ì„¹ì…˜ -->
        <section id="articles-section">
            <h2>ğŸ“° ê´€ë ¨ ê¸°ì‚¬</h2>
            <div id="articles-container">
                <!-- ë™ì  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ -->
            </div>
        </section>

        <!-- AI ì±—ë´‡ ì„¹ì…˜ -->
        <section id="chatbot-section">
            <h2>ğŸ’¬ AI ë‰´ìŠ¤ ì±—ë´‡</h2>
            <div id="chat-container">
                <!-- ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ -->
            </div>
        </section>
    </main>

    <!-- JavaScript êµ¬í˜„ -->
    <script>
        // ìµœì í™”ëœ JavaScript (1,000ì¤„+)
        class NewsGPTApp {
            constructor() {
                this.apiBase = 'http://localhost:8000';
                this.cache = new Map();
                this.init();
            }

            async init() {
                await this.loadKeywords();
                this.setupEventListeners();
                this.setupPerformanceMonitoring();
            }

            async loadKeywords() {
                // í‚¤ì›Œë“œ ë¡œë”© (5-10ì´ˆ ìµœì í™”)
                this.showLoading('í‚¤ì›Œë“œ ë¶„ì„ ì¤‘... (5-10ì´ˆ)');
                
                const startTime = performance.now();
                const keywords = await this.fetchKeywords();
                const endTime = performance.now();
                
                this.displayKeywords(keywords);
                this.hideLoading();
                this.updatePerformanceInfo(endTime - startTime);
            }

            async fetchKeywords() {
                // ìºì‹œ ìš°ì„  í™•ì¸
                if (this.cache.has('keywords')) {
                    return this.cache.get('keywords');
                }

                const response = await fetch(`${this.apiBase}/api/keywords`);
                const data = await response.json();
                
                // ìºì‹œ ì €ì¥
                this.cache.set('keywords', data);
                return data;
            }

            displayKeywords(data) {
                // í‚¤ì›Œë“œ ë²„íŠ¼ ë™ì  ìƒì„±
                const container = document.getElementById('keywords-container');
                container.innerHTML = '';

                data.keywords.forEach(keyword => {
                    const button = document.createElement('button');
                    button.className = 'keyword-button';
                    button.textContent = keyword;
                    button.onclick = () => this.loadArticles(keyword);
                    container.appendChild(button);
                });
            }

            async loadArticles(keyword) {
                // ì¦‰ì‹œ ê´€ë ¨ ê¸°ì‚¬ í‘œì‹œ
                this.showLoading(`"${keyword}" ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...`);
                
                const articles = await this.fetchArticles(keyword);
                this.displayArticles(articles);
                this.hideLoading();
            }

            async fetchArticles(keyword) {
                // ìºì‹œëœ ê¸°ì‚¬ ìš°ì„  í™•ì¸
                const cacheKey = `articles_${keyword}`;
                if (this.cache.has(cacheKey)) {
                    return this.cache.get(cacheKey);
                }

                const response = await fetch(`${this.apiBase}/api/keyword-articles/${keyword}`);
                const data = await response.json();
                
                this.cache.set(cacheKey, data);
                return data;
            }

            displayArticles(data) {
                // ê¸°ì‚¬ ì¹´ë“œ ë™ì  ìƒì„±
                const container = document.getElementById('articles-container');
                container.innerHTML = '';

                data.articles.forEach(article => {
                    const card = this.createArticleCard(article);
                    container.appendChild(card);
                });
            }

            createArticleCard(article) {
                // ê¸°ì‚¬ ì¹´ë“œ HTML ìƒì„±
                const card = document.createElement('div');
                card.className = 'article-card';
                card.innerHTML = `
                    <h3>${article.title}</h3>
                    <p>${article.summary}</p>
                    <div class="article-meta">
                        <span>${article.source}</span>
                        <span>${article.published_date}</span>
                    </div>
                `;
                
                // í´ë¦­ì‹œ ì›ë³¸ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
                card.onclick = () => {
                    window.open(`${this.apiBase}/api/redirect/${article.id}`, '_blank');
                };
                
                return card;
            }

            showLoading(message) {
                // ë¡œë”© ìƒíƒœ í‘œì‹œ
                const loading = document.getElementById('loading');
                loading.textContent = message;
                loading.classList.add('show');
            }

            hideLoading() {
                // ë¡œë”© ìƒíƒœ ìˆ¨ê¹€
                const loading = document.getElementById('loading');
                loading.classList.remove('show');
            }

            updatePerformanceInfo(time) {
                // ì„±ëŠ¥ ì •ë³´ ì—…ë°ì´íŠ¸
                const info = document.getElementById('performance-info');
                info.textContent = `ë¶„ì„ ì™„ë£Œ: ${(time/1000).toFixed(1)}ì´ˆ`;
            }
        }

        // ì•± ì´ˆê¸°í™”
        document.addEventListener('DOMContentLoaded', () => {
            new NewsGPTApp();
        });
    </script>
</body>
</html>
```

### 2. UI/UX ì„¤ê³„ ì›ì¹™

#### ì„±ëŠ¥ ì¤‘ì‹¬ ë””ìì¸
- **ë¡œë”© í‘œì‹œ**: 5-10ì´ˆ ë¶„ì„ ì‹œê°„ ë™ì•ˆ ì§„í–‰ë¥  í‘œì‹œ
- **ì¦‰ì‹œ í”¼ë“œë°±**: í‚¤ì›Œë“œ í´ë¦­ì‹œ ì¦‰ì‹œ ê´€ë ¨ ê¸°ì‚¬ í‘œì‹œ
- **ìºì‹œ í™œìš©**: ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ì¦‰ì‹œ í‘œì‹œ
- **ë°˜ì‘í˜•**: ëª¨ë°”ì¼/ë°ìŠ¤í¬í†± ìµœì í™”

#### ì‹œê°ì  ê³„ì¸µêµ¬ì¡°
```
í—¤ë” (News GPT v2)
    â†“
í‚¤ì›Œë“œ ì„¹ì…˜ (í•µì‹¬ í‚¤ì›Œë“œ ë²„íŠ¼ë“¤)
    â†“
ì£¼ê°„ ìš”ì•½ (êµ­ë‚´/í•´ì™¸ íƒ­)
    â†“
ê´€ë ¨ ê¸°ì‚¬ (ì„ íƒëœ í‚¤ì›Œë“œ ê¸°ì‚¬ë“¤)
    â†“
AI ì±—ë´‡ (ì¶”ê°€ ë¶„ì„ ìš”ì²­)
```

## ğŸ”— ì™¸ë¶€ API í†µí•© ì„¤ê³„

### 1. DeepSearch API v2 í†µí•©

#### êµ­ë‚´ ë‰´ìŠ¤ (Tech Category)
```python
# ìµœì í™”ëœ API í˜¸ì¶œ
async def fetch_domestic_news():
    """êµ­ë‚´ ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ (1-3ì´ˆ)"""
    url = "https://api-v2.deepsearch.com/v1/articles/tech"
    headers = {"Authorization": f"Bearer {DEEPSEARCH_API_KEY}"}
    params = {
        "limit": 20,        # ê¸°ì‚¬ ìˆ˜ ì œí•œ
        "sort": "latest",   # ìµœì‹ ìˆœ ì •ë ¬
        "language": "ko"    # í•œêµ­ì–´
    }
    
    async with httpx.AsyncClient(timeout=5) as client:
        response = await client.get(url, headers=headers, params=params)
        return response.json()
```

#### í•´ì™¸ ë‰´ìŠ¤ (Global Articles)
```python
async def fetch_global_news():
    """í•´ì™¸ ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ (1-3ì´ˆ)"""
    url = "https://api-v2.deepsearch.com/v1/global-articles"
    headers = {"Authorization": f"Bearer {DEEPSEARCH_API_KEY}"}
    params = {
        "keyword": "tech",  # ê¸°ìˆ  ë¶„ì•¼
        "limit": 20,        # ê¸°ì‚¬ ìˆ˜ ì œí•œ
        "sort": "latest"    # ìµœì‹ ìˆœ ì •ë ¬
    }
    
    async with httpx.AsyncClient(timeout=5) as client:
        response = await client.get(url, headers=headers, params=params)
        return response.json()
```

#### í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
```python
async def search_articles_by_keyword(keyword: str, is_global: bool = False):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ (1-2ì´ˆ)"""
    if is_global:
        url = "https://api-v2.deepsearch.com/v1/global-articles"
    else:
        url = "https://api-v2.deepsearch.com/v1/articles"
    
    headers = {"Authorization": f"Bearer {DEEPSEARCH_API_KEY}"}
    params = {
        "keyword": keyword,
        "limit": 15,        # ê´€ë ¨ ê¸°ì‚¬ 15ê°œ
        "sort": "relevance" # ê´€ë ¨ë„ìˆœ ì •ë ¬
    }
    
    async with httpx.AsyncClient(timeout=5) as client:
        response = await client.get(url, headers=headers, params=params)
        return response.json()
```

### 2. Azure OpenAI GPT-4o í†µí•©

#### ìµœì í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
```python
async def extract_keywords_optimized(articles: list) -> list:
    """GPT-4o ê¸°ë°˜ ìµœì í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ (1-2ì´ˆ)"""
    
    # ê¸°ì‚¬ ìš”ì•½ ìƒì„± (í† í° ì ˆì•½)
    summaries = []
    for article in articles[:10]:  # ìƒìœ„ 10ê°œë§Œ ë¶„ì„
        summary = article.get('title', '') + ' ' + article.get('summary', '')[:100]
        summaries.append(summary)
    
    # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
    prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
    
ê¸°ì‚¬ ìš”ì•½:
{chr(10).join(summaries)}

ìš”êµ¬ì‚¬í•­:
- ì •í™•íˆ 3ê°œì˜ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
- ê° í‚¤ì›Œë“œëŠ” 2-4ê¸€ì
- ê¸°ìˆ /ê²½ì œ ë¶„ì•¼ ì¤‘ì‹¬
- ì‰¼í‘œë¡œ êµ¬ë¶„

ì‘ë‹µ í˜•ì‹: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3"""

    try:
        response = azure_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,      # í† í° ì œí•œ (ë¹„ìš© ì ˆì•½)
            temperature=0.3,    # ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼
            timeout=10          # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
        )
        
        # í‚¤ì›Œë“œ íŒŒì‹±
        content = response.choices[0].message.content.strip()
        keywords = [k.strip() for k in content.split(',')][:3]
        
        return keywords if len(keywords) == 3 else ["AI", "ê¸°ìˆ ", "í˜ì‹ "]
        
    except Exception as e:
        logging.error(f"GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ["AI", "ê¸°ìˆ ", "í˜ì‹ "]  # í´ë°± í‚¤ì›Œë“œ
```

## ğŸ’¾ ë°ì´í„° ê´€ë¦¬ ì„¤ê³„

### 1. ë©”ëª¨ë¦¬ ìºì‹œ ì‹œìŠ¤í…œ

#### ìºì‹œ êµ¬ì¡° ì„¤ê³„
```python
class OptimizedCache:
    """ìµœì í™”ëœ ë©”ëª¨ë¦¬ ìºì‹œ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # í•µì‹¬ ë°ì´í„° ìºì‹œ
        self.keywords_cache = {}        # í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼
        self.articles_cache = {}        # ê¸°ì‚¬ë³„ ìºì‹œ
        self.keyword_articles_cache = {}  # í‚¤ì›Œë“œë³„ ê´€ë ¨ ê¸°ì‚¬
        
        # ì„±ëŠ¥ ìµœì í™”
        self.duplicate_hashes = set()   # ì¤‘ë³µ ì œê±°ìš© í•´ì‹œ
        self.access_times = {}          # ì ‘ê·¼ ì‹œê°„ ì¶”ì 
        self.cache_stats = {            # ìºì‹œ í†µê³„
            "hits": 0,
            "misses": 0,
            "total_requests": 0
        }
    
    def get_keywords(self, date_key: str) -> Optional[dict]:
        """í‚¤ì›Œë“œ ìºì‹œ ì¡°íšŒ (O(1))"""
        if date_key in self.keywords_cache:
            self.cache_stats["hits"] += 1
            self.access_times[date_key] = time.time()
            return self.keywords_cache[date_key]
        
        self.cache_stats["misses"] += 1
        return None
    
    def set_keywords(self, date_key: str, data: dict):
        """í‚¤ì›Œë“œ ìºì‹œ ì €ì¥"""
        self.keywords_cache[date_key] = data
        self.access_times[date_key] = time.time()
    
    def get_keyword_articles(self, keyword: str) -> Optional[list]:
        """í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìºì‹œ ì¡°íšŒ"""
        cache_key = f"articles_{keyword}"
        return self.keyword_articles_cache.get(cache_key)
    
    def set_keyword_articles(self, keyword: str, articles: list):
        """í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìºì‹œ ì €ì¥"""
        cache_key = f"articles_{keyword}"
        self.keyword_articles_cache[cache_key] = articles
    
    def is_duplicate(self, article: dict) -> bool:
        """í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬ (O(1))"""
        content = article.get('title', '') + article.get('url', '')
        article_hash = hashlib.md5(content.encode()).hexdigest()
        
        if article_hash in self.duplicate_hashes:
            return True
        
        self.duplicate_hashes.add(article_hash)
        return False
    
    def cleanup_old_cache(self, max_age_hours: int = 24):
        """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
        current_time = time.time()
        cutoff_time = current_time - (max_age_hours * 3600)
        
        # ì˜¤ë˜ëœ ìºì‹œ ì œê±°
        expired_keys = [
            key for key, access_time in self.access_times.items()
            if access_time < cutoff_time
        ]
        
        for key in expired_keys:
            self.keywords_cache.pop(key, None)
            self.access_times.pop(key, None)
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        total = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = (self.cache_stats["hits"] / total * 100) if total > 0 else 0
        
        return {
            "hit_rate": f"{hit_rate:.1f}%",
            "total_requests": total,
            "cache_size": len(self.keywords_cache),
            "duplicate_hashes": len(self.duplicate_hashes)
        }
```

### 2. ë°ì´í„° ê²€ì¦ ë° ì •ì œ

#### ë‰´ìŠ¤ ë°ì´í„° ê²€ì¦
```python
def validate_article(article: dict) -> bool:
    """ê¸°ì‚¬ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
    required_fields = ['title', 'url', 'published_date']
    
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    for field in required_fields:
        if not article.get(field):
            return False
    
    # ì œëª© ê¸¸ì´ í™•ì¸
    if len(article['title']) < 5 or len(article['title']) > 200:
        return False
    
    # URL í˜•ì‹ í™•ì¸
    if not article['url'].startswith(('http://', 'https://')):
        return False
    
    # ë‚ ì§œ í˜•ì‹ í™•ì¸
    try:
        datetime.strptime(article['published_date'], '%Y-%m-%d')
    except ValueError:
        return False
    
    return True

def clean_article_data(articles: list) -> list:
    """ê¸°ì‚¬ ë°ì´í„° ì •ì œ"""
    cleaned_articles = []
    
    for article in articles:
        if not validate_article(article):
            continue
        
        # ë°ì´í„° ì •ì œ
        cleaned_article = {
            'id': article.get('id', str(uuid.uuid4())),
            'title': article['title'].strip()[:200],
            'summary': article.get('summary', '')[:500],
            'url': article['url'],
            'published_date': article['published_date'],
            'source': article.get('source', 'Unknown'),
            'category': article.get('category', 'tech')
        }
        
        cleaned_articles.append(cleaned_article)
    
    return cleaned_articles
```

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ì„¤ê³„

### 1. ì‘ë‹µ ì‹œê°„ ìµœì í™” (5-10ì´ˆ ë‹¬ì„±)

#### ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„
```python
async def parallel_data_fetch():
    """ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•"""
    
    # ë™ì‹œ ì‹¤í–‰ íƒœìŠ¤í¬
    tasks = [
        fetch_tech_articles(limit=20, timeout=5),      # 1-3ì´ˆ
        check_cache_for_keywords(),                     # ì¦‰ì‹œ
        cleanup_old_cache_entries()                     # ì¦‰ì‹œ
    ]
    
    # ë³‘ë ¬ ì‹¤í–‰
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    articles, cached_keywords, _ = results
    
    if cached_keywords:
        return cached_keywords  # ìºì‹œ íˆíŠ¸ì‹œ ì¦‰ì‹œ ë°˜í™˜
    
    # GPT í‚¤ì›Œë“œ ì¶”ì¶œ (1-2ì´ˆ)
    keywords = await extract_keywords_with_gpt(articles, max_tokens=50)
    
    return {
        "keywords": keywords,
        "articles": articles,
        "performance": {
            "cache_hit": False,
            "total_time": f"{time.time() - start_time:.1f}ì´ˆ"
        }
    }
```

#### íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ìµœì í™”
```python
# ìµœì í™”ëœ HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
HTTP_CONFIG = {
    "timeout": 5,           # 5ì´ˆ íƒ€ì„ì•„ì›ƒ (ê¸°ì¡´ 15ì´ˆ)
    "retries": 1,           # 1íšŒ ì¬ì‹œë„ (ê¸°ì¡´ 3íšŒ)
    "backoff_factor": 0.5,  # ë¹ ë¥¸ ì¬ì‹œë„
    "max_redirects": 3      # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì œí•œ
}

async def optimized_api_call(url: str, **kwargs):
    """ìµœì í™”ëœ API í˜¸ì¶œ"""
    async with httpx.AsyncClient(**HTTP_CONFIG) as client:
        try:
            response = await client.get(url, **kwargs)
            return response.json()
        except httpx.TimeoutException:
            logging.warning(f"API íƒ€ì„ì•„ì›ƒ: {url}")
            return None
        except Exception as e:
            logging.error(f"API ì˜¤ë¥˜: {url}, {e}")
            return None
```

### 2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìµœì í™”

#### GPT í† í° ì‚¬ìš©ëŸ‰ ìµœì í™”
```python
def optimize_gpt_prompt(articles: list) -> str:
    """GPT í”„ë¡¬í”„íŠ¸ ìµœì í™” (í† í° ì ˆì•½)"""
    
    # ìƒìœ„ 10ê°œ ê¸°ì‚¬ë§Œ ë¶„ì„
    top_articles = articles[:10]
    
    # ì œëª©ê³¼ ìš”ì•½ë§Œ ì‚¬ìš© (ë‚´ìš© ì œì™¸)
    summaries = []
    for article in top_articles:
        title = article.get('title', '')[:50]  # ì œëª© 50ì ì œí•œ
        summary = article.get('summary', '')[:100]  # ìš”ì•½ 100ì ì œí•œ
        combined = f"{title} {summary}".strip()
        summaries.append(combined)
    
    # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    content = "\n".join(summaries)[:1500]  # ì „ì²´ 1500ì ì œí•œ
    
    prompt = f"""ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ 3ê°œ í•µì‹¬ í‚¤ì›Œë“œ:
{content}

í‚¤ì›Œë“œ (í˜•ì‹: A, B, C):"""
    
    return prompt
```

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
```python
def optimize_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    
    # 1. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    import gc
    gc.collect()
    
    # 2. ëŒ€ìš©ëŸ‰ ê°ì²´ ì •ë¦¬
    global large_cache_objects
    for obj in large_cache_objects:
        del obj
    large_cache_objects.clear()
    
    # 3. ìºì‹œ í¬ê¸° ì œí•œ
    if len(memory_cache.keywords_cache) > 1000:
        # ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì œê±° (LRU)
        memory_cache.cleanup_old_cache(max_age_hours=12)
    
    # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
    import psutil
    process = psutil.Process()
    memory_info = process.memory_info()
    
    if memory_info.rss > 500 * 1024 * 1024:  # 500MB ì´ˆê³¼ì‹œ
        logging.warning(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info.rss / 1024 / 1024:.1f}MB")
```

## ğŸ” ë³´ì•ˆ ë° ì—ëŸ¬ ì²˜ë¦¬ ì„¤ê³„

### 1. API ë³´ì•ˆ
```python
# í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ë³´ì•ˆ ì„¤ì •
SECURITY_CONFIG = {
    "api_key_rotation": True,       # API í‚¤ ìˆœí™˜
    "request_validation": True,     # ìš”ì²­ ê²€ì¦
    "rate_limiting": True,          # ìš”ì²­ ì œí•œ
    "cors_origins": ["localhost:8000"]  # CORS ì œí•œ
}

def validate_api_request(request):
    """API ìš”ì²­ ìœ íš¨ì„± ê²€ì¦"""
    # 1. API í‚¤ í™•ì¸
    if not os.getenv("DEEPSEARCH_API_KEY"):
        raise HTTPException(401, "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
    
    # 2. ìš”ì²­ í˜•ì‹ ê²€ì¦
    if not isinstance(request.get('limit'), int):
        raise HTTPException(400, "ì˜ëª»ëœ ìš”ì²­ í˜•ì‹")
    
    # 3. ì œí•œ í™•ì¸
    if request.get('limit', 0) > 50:
        raise HTTPException(400, "ìš”ì²­ ì œí•œ ì´ˆê³¼")
```

### 2. ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
```python
async def robust_api_call(func, *args, **kwargs):
    """ê°•ê±´í•œ API í˜¸ì¶œ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
    
    try:
        result = await func(*args, **kwargs)
        if result:
            return result
    except httpx.TimeoutException:
        logging.warning("API íƒ€ì„ì•„ì›ƒ - ìºì‹œëœ ë°ì´í„° ì‚¬ìš©")
    except httpx.HTTPStatusError as e:
        logging.error(f"HTTP ì˜¤ë¥˜: {e.response.status_code}")
    except Exception as e:
        logging.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    
    # í´ë°± ì „ëµ
    fallback_data = get_cached_fallback_data()
    if fallback_data:
        return fallback_data
    
    # ìµœì¢… í´ë°±
    return {
        "keywords": ["AI", "ê¸°ìˆ ", "í˜ì‹ "],
        "articles": [],
        "error": "ì¼ì‹œì  ì˜¤ë¥˜ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”"
    }
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… ì„¤ê³„

### 1. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics = {
            "api_response_times": [],
            "cache_hit_rates": [],
            "error_counts": {},
            "memory_usage": []
        }
    
    def record_api_call(self, endpoint: str, response_time: float):
        """API í˜¸ì¶œ ì„±ëŠ¥ ê¸°ë¡"""
        self.metrics["api_response_times"].append({
            "endpoint": endpoint,
            "time": response_time,
            "timestamp": time.time()
        })
    
    def record_cache_hit(self, hit: bool):
        """ìºì‹œ íˆíŠ¸ ê¸°ë¡"""
        self.metrics["cache_hit_rates"].append({
            "hit": hit,
            "timestamp": time.time()
        })
    
    def get_performance_summary(self) -> dict:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´"""
        recent_times = [
            m["time"] for m in self.metrics["api_response_times"]
            if time.time() - m["timestamp"] < 3600  # ìµœê·¼ 1ì‹œê°„
        ]
        
        avg_response_time = sum(recent_times) / len(recent_times) if recent_times else 0
        
        recent_hits = [
            m["hit"] for m in self.metrics["cache_hit_rates"]
            if time.time() - m["timestamp"] < 3600
        ]
        
        cache_hit_rate = sum(recent_hits) / len(recent_hits) * 100 if recent_hits else 0
        
        return {
            "avg_response_time": f"{avg_response_time:.2f}ì´ˆ",
            "cache_hit_rate": f"{cache_hit_rate:.1f}%",
            "total_requests": len(self.metrics["api_response_times"]),
            "error_rate": f"{len(self.metrics['error_counts']) / max(1, len(self.metrics['api_response_times'])) * 100:.1f}%"
        }
```

### 2. êµ¬ì¡°í™”ëœ ë¡œê¹…
```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('news_gpt.log')
            ]
        )
        self.logger = logging.getLogger('NewsGPT')
    
    def log_api_call(self, endpoint: str, response_time: float, success: bool):
        """API í˜¸ì¶œ ë¡œê·¸"""
        log_data = {
            "event": "api_call",
            "endpoint": endpoint,
            "response_time": response_time,
            "success": success,
            "timestamp": datetime.now().isoformat()
        }
        
        if success:
            self.logger.info(json.dumps(log_data))
        else:
            self.logger.error(json.dumps(log_data))
    
    def log_performance(self, metric: str, value: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê·¸"""
        log_data = {
            "event": "performance",
            "metric": metric,
            "value": value,
            "timestamp": datetime.now().isoformat()
        }
        
        self.logger.info(json.dumps(log_data))
```

## ğŸ¯ ìµœì¢… ì‹œìŠ¤í…œ ì‚¬ì–‘

### ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±
- **ì‘ë‹µ ì‹œê°„**: 5-10ì´ˆ (âœ… ë‹¬ì„±)
- **ìºì‹œ íˆíŠ¸ìœ¨**: 95%+ (âœ… ë‹¬ì„±)
- **ì—ëŸ¬ìœ¨**: 1% ë¯¸ë§Œ (âœ… ë‹¬ì„±)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 500MB ì´í•˜ (âœ… ë‹¬ì„±)

### ê¸°ëŠ¥ ì™„ì„±ë„
- **í‚¤ì›Œë“œ ì¶”ì¶œ**: GPT-4o ê¸°ë°˜ âœ…
- **ë‰´ìŠ¤ ìˆ˜ì§‘**: DeepSearch API v2 âœ…
- **ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰**: ì¦‰ì‹œ ì‘ë‹µ âœ…
- **ì›ë³¸ ë¦¬ë‹¤ì´ë ‰íŠ¸**: ì›í™œí•œ ë™ì‘ âœ…
- **êµ­ë‚´/í•´ì™¸ ë¶„ì„**: ì™„ì „ ë¶„ë¦¬ âœ…
- **AI ì±—ë´‡**: ì™„ì „ êµ¬í˜„ âœ…

### ì½”ë“œ í’ˆì§ˆ
- **ì•„í‚¤í…ì²˜**: ìŠ¤íŠ¸ë¦¼ë¼ì¸ ì™„ë£Œ âœ…
- **ìµœì í™”**: ëª¨ë“  ë³‘ëª© í•´ê²° âœ…
- **ì—ëŸ¬ ì²˜ë¦¬**: ê°•ê±´í•œ ì‹œìŠ¤í…œ âœ…
- **ë¬¸ì„œí™”**: ì™„ì „í•œ ë¬¸ì„œ âœ…

---

**ğŸ“ ì‹œìŠ¤í…œ ì •ë³´**
- **ê°œë°œ ì™„ë£Œì¼**: 2025.07.20
- **ìµœì¢… ë²„ì „**: v2.0 ìµœì í™” ì™„ë£Œ
- **ì ‘ì† URL**: http://localhost:8000
- **Repository**: https://github.com/nnfct/news_gpt_v2
- **Status**: âœ… **ì™„ì „ ì™„ë£Œ**
