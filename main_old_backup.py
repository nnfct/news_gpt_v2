import os
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
from collections import Counter
import requests
import json
import time
from datetime import datetime, timedelta
import uvicorn
from error_logger import log_error, auto_log_errors
from typing import Optional

app = FastAPI()

# ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬ê¸° ì¶”ê°€
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ëª¨ë“  ì˜ˆì™¸ë¥¼ ìë™ìœ¼ë¡œ error_history.mdì— ê¸°ë¡"""
    
    # ì—ëŸ¬ ë¡œê¹…
    log_error(
        error=exc,
        file_name="main.py",
        function_name="global_exception_handler",
        context=f"ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬ - ìš”ì²­ URL: {request.url}",
        additional_info={
            "method": request.method,
            "url": str(request.url),
            "headers": dict(request.headers),
            "client_ip": request.client.host if request.client else "unknown"
        },
        severity="HIGH"
    )
    
    # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì—ëŸ¬ ì‘ë‹µ
    return JSONResponse(
        status_code=500,
        content={
            "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "message": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],  # ëª¨ë“  HTTP ë©”ì„œë“œ í—ˆìš©
    allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
)

load_dotenv()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_INDEX = os.getenv("AZURE_SEARCH_INDEX")
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")

# DeepSearch API ì„¤ì •
DEEPSEARCH_API_KEY = os.getenv("DEEPSEARCH_API_KEY")

if not all([AZURE_SEARCH_API_KEY, AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT]):
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜(.env) ê°’ì´ ëª¨ë‘ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = AzureOpenAI(
    api_key=str(AZURE_OPENAI_API_KEY),
    api_version="2024-02-15-preview",
    azure_endpoint=str(AZURE_OPENAI_ENDPOINT)
)

search_client = SearchClient(
    endpoint=str(AZURE_SEARCH_ENDPOINT),
    index_name=str(AZURE_SEARCH_INDEX),
    credential=AzureKeyCredential(str(AZURE_SEARCH_API_KEY))
)

def get_current_week_news_from_deepsearch(query, start_date=None, end_date=None):
    """DeepSearch APIë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ì£¼ê°„ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    if not DEEPSEARCH_API_KEY:
        print("DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    # í˜„ì¬ ì£¼ê°„ ë‚ ì§œ ì„¤ì • (2025ë…„ 7ì›” 3ì£¼ì°¨)
    if not start_date:
        start_date = "2025-07-14"  # 7ì›” 3ì£¼ì°¨ ì‹œì‘
    if not end_date:
        end_date = "2025-07-20"    # 7ì›” 3ì£¼ì°¨ ì¢…ë£Œ
    
    try:
        # ë¨¼ì € ë‹¨ìˆœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹œë„
        simple_query = query
        print(f"ğŸ” ë‹¨ìˆœ ê²€ìƒ‰ì–´ ì‹œë„: {simple_query}")
        
        # DeepSearch API í˜¸ì¶œ (í•œêµ­ì–´ ê¸°ì‚¬ ìš°ì„  ì‚¬ìš©)
        url = "https://api-v2.deepsearch.com/v1/articles"
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "q": simple_query,
            "limit": 20,  # ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ìˆ˜
            "start_date": start_date,
            "end_date": end_date,
            "sort": "published_at:desc"  # ìµœì‹ ìˆœ ì •ë ¬
        }
        
        response = requests.get(url, params=params)
        response.raise_for_status()
        
        data = response.json()
        articles = []
        
        # ë‹¨ìˆœ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        simple_articles = []
        for item in data.get("data", []):
            # ë‚ ì§œ í•„í„°ë§ (APIì—ì„œ ì´ë¯¸ í•„í„°ë§ë˜ì§€ë§Œ ì¶”ê°€ í™•ì¸)
            pub_date = item.get("published_at", "") or item.get("date", "")
            
            try:
                # ISO í˜•ì‹ ë‚ ì§œ íŒŒì‹±
                if "T" in pub_date:
                    article_date = pub_date.split("T")[0]  # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ
                else:
                    article_date = pub_date
                
                # ì§€ì •ëœ ì£¼ê°„ ë²”ìœ„ ë‚´ì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                if start_date <= article_date <= end_date:
                    # content_url í•„ë“œ ì‚¬ìš© (ì‹¤ì œ ê¸°ì‚¬ URL)
                    article_url = item.get("content_url", "") or item.get("url", "") or item.get("link", "")
                    
                    # ê¸°ì‚¬ ê´€ë ¨ì„± ê²€ì‚¬
                    title = item.get("title", "")
                    summary = item.get("summary", "") or (item.get("content", "") or "")[:200] + "..."
                    
                    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° í•„í„°ë§
                    relevance_score = calculate_article_relevance(title, summary, query)
                    
                    # ë” ê´€ëŒ€í•œ ê´€ë ¨ì„± í•„í„°ë§ ì ìš©
                    if relevance_score >= 0.1:  # ì„ê³„ê°’ì„ 0.1ë¡œ í•˜í–¥ ì¡°ì •
                        simple_articles.append({
                            "title": title,
                            "description": summary,
                            "link": article_url,
                            "pubDate": article_date,
                            "relevance": relevance_score
                        })
                        print(f"    ğŸ“° ê¸°ì‚¬ ì¶”ê°€: {title[:30]}... (ì ìˆ˜: {relevance_score:.2f})")
                    else:
                        print(f"    âŒ ê´€ë ¨ì„± ë‚®ìŒ: {title[:30]}... (ì ìˆ˜: {relevance_score:.2f})")
            except Exception as e:
                print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        # ë‹¨ìˆœ ê²€ìƒ‰ì—ì„œ ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ë©´ ì‚¬ìš©
        if len(simple_articles) >= 3:
            articles = simple_articles
            print(f"âœ… ë‹¨ìˆœ ê²€ìƒ‰ìœ¼ë¡œ {len(articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ë°œê²¬")
        else:
            print(f"âš ï¸ ë‹¨ìˆœ ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± ({len(simple_articles)}ê°œ), Enhanced ê²€ìƒ‰ ì‹œë„")
            
            # Enhanced ê²€ìƒ‰ ì‹œë„
            enhanced_query = enhance_company_search_query(query)
            print(f"ğŸ” í–¥ìƒëœ ê²€ìƒ‰ì–´: {query} â†’ {enhanced_query}")
            
            params["q"] = enhanced_query
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            enhanced_articles = []
            
            for item in data.get("data", []):
                pub_date = item.get("published_at", "") or item.get("date", "")
                
                try:
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    
                    if start_date <= article_date <= end_date:
                        article_url = item.get("content_url", "") or item.get("url", "") or item.get("link", "")
                        title = item.get("title", "")
                        summary = item.get("summary", "") or (item.get("content", "") or "")[:200] + "..."
                        
                        relevance_score = calculate_article_relevance(title, summary, query)
                        
                        if relevance_score >= 0.1:
                            enhanced_articles.append({
                                "title": title,
                                "description": summary,
                                "link": article_url,
                                "pubDate": article_date,
                                "relevance": relevance_score
                            })
                            print(f"    ğŸ“° Enhanced ê¸°ì‚¬ ì¶”ê°€: {title[:30]}... (ì ìˆ˜: {relevance_score:.2f})")
                        else:
                            print(f"    âŒ Enhanced ê´€ë ¨ì„± ë‚®ìŒ: {title[:30]}... (ì ìˆ˜: {relevance_score:.2f})")
                except Exception as e:
                    print(f"Enhanced ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            # ë‹¨ìˆœ ê²€ìƒ‰ê³¼ Enhanced ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
            all_articles = simple_articles + enhanced_articles
            
            # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
            seen_titles = set()
            articles = []
            for article in all_articles:
                if article["title"] not in seen_titles:
                    seen_titles.add(article["title"])
                    articles.append(article)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        articles.sort(key=lambda x: x.get('relevance', 0), reverse=True)
        
        return articles[:10]  # ìµœëŒ€ 10ê°œ ë°˜í™˜
        
    except Exception as e:
        log_error(
            error=e,
            file_name="main.py",
            function_name="get_current_week_news_from_deepsearch",
            context=f"DeepSearch API í˜¸ì¶œ ì˜¤ë¥˜ - ì¿¼ë¦¬: {query}, ê¸°ê°„: {start_date}~{end_date}",
            additional_info={
                "query": query,
                "start_date": start_date,
                "end_date": end_date,
                "has_api_key": bool(DEEPSEARCH_API_KEY)
            },
            severity="HIGH"
        )
        print(f"DeepSearch API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return []

def enhance_company_search_query(query):
    """ê¸°ì—…ëª… ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ í–¥ìƒ"""
    
    # ì£¼ìš” ê¸°ì—…ë³„ ê²€ìƒ‰ì–´ ë§¤í•‘
    company_queries = {
        "DB": '"DBê¸ˆìœµíˆ¬ì" OR "DBì†í•´ë³´í—˜" OR "DBê·¸ë£¹" OR "DBê¸ˆìœµ" OR "DBí•˜ì´í…"',
        "NAVER": '"ë„¤ì´ë²„" OR "NAVER" OR "ë„¤ì´ë²„ì›¹íˆ°" OR "ë„¤ì´ë²„í´ë¼ìš°ë“œ"',
        "ì‚¼ì„±ì „ì": '"ì‚¼ì„±ì „ì" OR "ì‚¼ì„±" OR "Samsung Electronics"',
        "í•œí™”": '"í•œí™”ê·¸ë£¹" OR "í•œí™”ì‹œìŠ¤í…œ" OR "í•œí™”ì˜¤ì…˜" OR "í•œí™”ìƒëª…"',
        "LG": '"LGì „ì" OR "LGí™”í•™" OR "LGì—ë„ˆì§€ì†”ë£¨ì…˜" OR "LGìœ í”ŒëŸ¬ìŠ¤"',
        "í˜„ëŒ€ì°¨": '"í˜„ëŒ€ìë™ì°¨" OR "í˜„ëŒ€ì°¨" OR "í˜„ëŒ€ëª¨ë¹„ìŠ¤"',
        "SK": '"SKí…”ë ˆì½¤" OR "SKí•˜ì´ë‹‰ìŠ¤" OR "SKT" OR "SKì´ë…¸ë² ì´ì…˜"',
        "NEW": '"NEW" OR "ë‰´ìŠ¤" OR "ì‹ ê·œ"'  # NEWëŠ” ë„ˆë¬´ ì¼ë°˜ì ì´ë¯€ë¡œ ì œì™¸
    }
    
    # ë§¤í•‘ëœ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
    return company_queries.get(query, f'"{query}"')

def calculate_article_relevance(title, content, keyword):
    """ê¸°ì‚¬ì™€ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
    
    # ê¸°ë³¸ ì ìˆ˜
    score = 0.0
    
    # ì œëª©ê³¼ ë‚´ìš©ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
    title_lower = title.lower()
    content_lower = content.lower()
    keyword_lower = keyword.lower()
    
    print(f"    ğŸ” ê´€ë ¨ì„± ê³„ì‚°: '{keyword}' vs '{title[:50]}...'")
    
    # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ë†’ì„)
    if keyword_lower in title_lower:
        score += 1.0  # ì œëª©ì— ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
        print(f"    âœ… ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë°œê²¬: +1.0")
    
    if keyword_lower in content_lower:
        score += 0.5  # ë‚´ìš©ì— ìˆìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
        print(f"    âœ… ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ë°œê²¬: +0.5")
    
    # íŠ¹ì • ê¸°ì—…ë³„ ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬ (ë” ê°•í™”)
    related_keywords = {
        "db": ["dbê¸ˆìœµíˆ¬ì", "dbì†í•´ë³´í—˜", "dbê·¸ë£¹", "dbìƒëª…", "dbí•˜ì´í…", "dbì¦ê¶Œ", "ë””ë¹„ê¸ˆìœµ", "ë””ë¹„ê·¸ë£¹"],
        "naver": ["ë„¤ì´ë²„", "naver", "ë„¤ì´ë²„ì›¹íˆ°", "ë„¤ì´ë²„í´ë¼ìš°ë“œ", "ë„¤ì´ë²„í˜ì´", "ë¼ì¸"],
        "ì‚¼ì„±ì „ì": ["ì‚¼ì„±ì „ì", "ì‚¼ì„±", "samsung", "ê°¤ëŸ­ì‹œ", "ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "ìŠ¤ë§ˆíŠ¸í°"],
        "í•œí™”": ["í•œí™”ê·¸ë£¹", "í•œí™”ì‹œìŠ¤í…œ", "í•œí™”ì˜¤ì…˜", "í•œí™”ìƒëª…", "í•œí™”í™”í•™", "í•œí™”ì†”ë£¨ì…˜"],
        "lg": ["lgì „ì", "lgí™”í•™", "lgì—ë„ˆì§€ì†”ë£¨ì…˜", "lgìœ í”ŒëŸ¬ìŠ¤", "ì—˜ì§€"],
        "í˜„ëŒ€ì°¨": ["í˜„ëŒ€ìë™ì°¨", "í˜„ëŒ€ì°¨", "í˜„ëŒ€ëª¨ë¹„ìŠ¤", "ì œë„¤ì‹œìŠ¤", "ì•„ì´ì˜¤ë‹‰"],
        "sk": ["skí…”ë ˆì½¤", "skí•˜ì´ë‹‰ìŠ¤", "skt", "skì´ë…¸ë² ì´ì…˜", "skë°”ì´ì˜¤íŒœ"]
    }
    
    # í‚¤ì›Œë“œ ì •ê·œí™” (ê´„í˜¸ ì œê±° ë“±)
    clean_keyword = keyword_lower.split('(')[0].strip()
    
    if clean_keyword in related_keywords:
        print(f"    ğŸ” ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡ í™•ì¸: {related_keywords[clean_keyword]}")
        for related_word in related_keywords[clean_keyword]:
            if related_word in title_lower or related_word in content_lower:
                score += 0.3  # ê´€ë ¨ í‚¤ì›Œë“œ ë°œê²¬ ì‹œ ì ìˆ˜ ì¶”ê°€
                print(f"    ğŸ’¡ ê´€ë ¨ í‚¤ì›Œë“œ ë°œê²¬: {related_word} (+0.3)")
                break
    
    # ë¶€ì •ì  í‚¤ì›Œë“œ (ë¬´ê´€í•œ ë‚´ìš©) ì²´í¬
    negative_keywords = [
        "ì½˜ì„œíŠ¸", "k-pop", "ì•„ì´ëŒ", "ë“œë¼ë§ˆ", "ì˜í™”", "ì—°ì˜ˆ", "ì˜ˆëŠ¥", "ìŠ¤í¬ì¸ ",
        "ì¶•êµ¬", "ì•¼êµ¬", "ì˜¬ë¦¼í”½", "ê²Œì„", "ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ", "ë¡¤", "ë°°í‹€ê·¸ë¼ìš´ë“œ",
        "ìš”ë¦¬", "ë ˆì‹œí”¼", "ë§›ì§‘", "ì—¬í–‰", "ê´€ê´‘", "íŒ¨ì…˜", "ë·°í‹°", "ì½”ìŠ¤ë©”í‹±"
    ]
    
    for neg_word in negative_keywords:
        if neg_word in title_lower or neg_word in content_lower:
            score -= 0.5  # ë¬´ê´€í•œ í‚¤ì›Œë“œ ë°œê²¬ ì‹œ ì ìˆ˜ ê°ì 
            print(f"    âš ï¸ ë¬´ê´€í•œ í‚¤ì›Œë“œ ë°œê²¬: {neg_word} (-0.5)")
            break
    
    print(f"    ğŸ“Š ìµœì¢… ì ìˆ˜: {max(0.0, score):.2f}")
    return max(0.0, score)  # ìŒìˆ˜ ì ìˆ˜ ë°©ì§€

def get_weekly_hot_keywords_from_aggregation(start_date, end_date):
    """DeepSearch APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ Azure AI Search ì—…ë¡œë“œ â†’ GPT-4o í‚¤ì›Œë“œ ì¶”ì¶œ â†’ Top 5 ì„ ì •"""
    
    try:
        print(f"ğŸ” ì˜¬ë°”ë¥¸ í”Œë¡œìš° ì‹œì‘: {start_date} ~ {end_date}")
        
        # 1ï¸âƒ£ DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘
        print("\n1ï¸âƒ£ DeepSearch API ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        articles = collect_it_news_from_deepsearch(start_date, end_date)
        
        if not articles:
            print("âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
            return [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "source": "ITê¸°ìˆ "},
                {"keyword": "ë°˜ë„ì²´", "count": 20, "source": "ITê¸°ìˆ "},
                {"keyword": "í´ë¼ìš°ë“œ", "count": 18, "source": "ITê¸°ìˆ "},
                {"keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15, "source": "ITê¸°ìˆ "},
                {"keyword": "ë¸”ë¡ì²´ì¸", "count": 12, "source": "ITê¸°ìˆ "}
            ]
        
        # 2ï¸âƒ£ Azure AI Searchì— ì—…ë¡œë“œ
        print(f"\n2ï¸âƒ£ Azure AI Search ì—…ë¡œë“œ ì¤‘ ({len(articles)}ê°œ ê¸°ì‚¬)...")
        upload_success = upload_articles_to_azure_search(articles)
        
        # 3ï¸âƒ£ Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        print(f"\n3ï¸âƒ£ GPT-4o í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = extract_keywords_with_gpt4o(articles)
        
        # 4ï¸âƒ£ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ë° Top 5 ì„ ì •
        print(f"\n4ï¸âƒ£ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì¤‘...")
        top_keywords = analyze_keyword_frequency(keywords)
        
        print(f"ğŸ¯ ìµœì¢… Top 5 í‚¤ì›Œë“œ (ì‹¤ì œ ë‰´ìŠ¤ ê¸°ë°˜):")
        for i, kw in enumerate(top_keywords[:5], 1):
            print(f"  {i}. {kw['keyword']} ({kw['count']}íšŒ)")
        
        return top_keywords[:5]
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [
            {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "source": "ITê¸°ìˆ "},
            {"keyword": "ë°˜ë„ì²´", "count": 20, "source": "ITê¸°ìˆ "},
            {"keyword": "í´ë¼ìš°ë“œ", "count": 18, "source": "ITê¸°ìˆ "},
            {"keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15, "source": "ITê¸°ìˆ "},
            {"keyword": "ë¸”ë¡ì²´ì¸", "count": 12, "source": "ITê¸°ìˆ "}
        ]

def collect_it_news_from_deepsearch(start_date, end_date):
    """DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘"""
    if not DEEPSEARCH_API_KEY:
        print("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    
    try:
        articles = []
        tech_keywords = [
            "ì¸ê³µì§€ëŠ¥", "AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ë°˜ë„ì²´", "ì¹©", "í”„ë¡œì„¸ì„œ", 
            "í´ë¼ìš°ë“œ", "ë°ì´í„°ì„¼í„°", "ì†Œí”„íŠ¸ì›¨ì–´", "5G", "6G", "ë¸”ë¡ì²´ì¸", 
            "ë©”íƒ€ë²„ìŠ¤", "VR", "AR", "ë¡œë´‡", "ìë™í™”", "IoT", "ë¹…ë°ì´í„°"
        ]
        
        print(f"ğŸ” IT/ê¸°ìˆ  í‚¤ì›Œë“œ {len(tech_keywords)}ê°œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        for keyword in tech_keywords:
            try:
                url = "https://api-v2.deepsearch.com/v1/articles"
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "q": keyword,
                    "limit": 10,
                    "start_date": start_date,
                    "end_date": end_date,
                    "sort": "published_at:desc"
                }
                
                response = requests.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                for item in data.get("data", []):
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    
                    if start_date <= article_date <= end_date:
                        articles.append({
                            "id": f"news_{len(articles)}_{int(time.time())}",
                            "title": item.get("title", ""),
                            "content": item.get("summary", "") or item.get("content", ""),
                            "date": article_date,
                            "section": "IT/ê¸°ìˆ ",
                            "keyword": keyword,
                            "url": item.get("content_url", "") or item.get("url", "")
                        })
                
                print(f"  âœ… '{keyword}': {len(data.get('data', []))}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                time.sleep(0.1)  # API ì œí•œ ë°©ì§€
                
            except Exception as e:
                print(f"  âŒ '{keyword}' ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_articles = []
        seen_titles = set()
        for article in articles:
            if article["title"] not in seen_titles:
                seen_titles.add(article["title"])
                unique_articles.append(article)
        
        print(f"âœ… ì´ {len(unique_articles)}ê°œ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles
        
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return []

def upload_articles_to_azure_search(articles):
    """Azure AI Searchì— ê¸°ì‚¬ ì—…ë¡œë“œ"""
    try:
        search_client = SearchClient(
            endpoint=str(AZURE_SEARCH_ENDPOINT),
            index_name=str(AZURE_SEARCH_INDEX),
            credential=AzureKeyCredential(str(AZURE_SEARCH_API_KEY))
        )
        
        # ë°°ì¹˜ ì—…ë¡œë“œ (50ê°œì”©)
        batch_size = 50
        total_uploaded = 0
        
        for i in range(0, len(articles), batch_size):
            batch = articles[i:i+batch_size]
            result = search_client.upload_documents(batch)
            
            success_count = len([r for r in result if r.succeeded])
            total_uploaded += success_count
            print(f"  ğŸ“¤ ë°°ì¹˜ {i//batch_size + 1}: {success_count}/{len(batch)}ê°œ ì—…ë¡œë“œ ì„±ê³µ")
        
        print(f"âœ… ì´ {total_uploaded}ê°œ ê¸°ì‚¬ Azure AI Search ì—…ë¡œë“œ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ Azure AI Search ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False

def extract_keywords_with_gpt4o(articles):
    """Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        # ê¸°ì‚¬ ë‚´ìš© í•©ì¹˜ê¸° (ìµœëŒ€ 50ê°œ ê¸°ì‚¬)
        articles_text = "\n".join([
            f"ì œëª©: {article['title']}\në‚´ìš©: {article['content'][:200]}..."
            for article in articles[:50]
        ])
        
        prompt = f"""
ë‹¤ìŒ IT/ê¸°ìˆ  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ê³  ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ë‚´ìš©:
{articles_text}

ìš”êµ¬ì‚¬í•­:
1. IT/ê¸°ìˆ  ë¶„ì•¼ í•µì‹¬ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ì¶”ì¶œ
2. êµ¬ì²´ì ì´ê³  ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë§Œ ì„ ì •
3. ê° í‚¤ì›Œë“œë§ˆë‹¤ ì˜ˆìƒ ë¹ˆë„ë„ í•¨ê»˜ ì œê³µ
4. í•œêµ­ì–´ë¡œ ì‘ë‹µ
5. ì‘ë‹µ í˜•ì‹: í‚¤ì›Œë“œ1:ë¹ˆë„1, í‚¤ì›Œë“œ2:ë¹ˆë„2, ... (ì½¤ë§ˆë¡œ êµ¬ë¶„)

í‚¤ì›Œë“œ:
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ IT/ê¸°ìˆ  ë‰´ìŠ¤ ì „ë¬¸ í‚¤ì›Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.3
        )
        
        keywords_text = response.choices[0].message.content.strip()
        
        # í‚¤ì›Œë“œ:ë¹ˆë„ íŒŒì‹±
        keywords = []
        for item in keywords_text.split(','):
            if ':' in item:
                keyword, freq = item.strip().split(':', 1)
                try:
                    count = int(freq.strip())
                    keywords.append({"keyword": keyword.strip(), "count": count})
                except:
                    keywords.append({"keyword": keyword.strip(), "count": 10})
        
        print(f"âœ… GPT-4oë¡œ {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        return keywords
        
    except Exception as e:
        print(f"âŒ GPT-4o í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

def analyze_keyword_frequency(keywords):
    """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ë° ì •ë ¬"""
    if not keywords:
        return []
    
    # ë¹ˆë„ ê¸°ì¤€ ì •ë ¬
    sorted_keywords = sorted(keywords, key=lambda x: x['count'], reverse=True)
    
    # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
    for kw in sorted_keywords:
        kw['source'] = 'ITê¸°ìˆ '
    
    return sorted_keywords
        
    except Exception as e:
        print(f"âŒ Error in get_weekly_hot_keywords_from_aggregation: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë‚ ì§œë³„ í‚¤ì›Œë“œ ë°˜í™˜
        return [
            {"keyword": "ì›”ìš”ì¼ì´ìŠˆ", "count": 15000, "source": "ë‚ ì§œë³„"},
            {"keyword": "í™”ìš”ì¼ì´ìŠˆ", "count": 14500, "source": "ë‚ ì§œë³„"},
            {"keyword": "ìˆ˜ìš”ì¼ì´ìŠˆ", "count": 14000, "source": "ë‚ ì§œë³„"},
            {"keyword": "ëª©ìš”ì¼ì´ìŠˆ", "count": 13500, "source": "ë‚ ì§œë³„"},
            {"keyword": "ê¸ˆìš”ì¼ì´ìŠˆ", "count": 13000, "source": "ë‚ ì§œë³„"}
        ]

def get_aggregation_data(groupby, date_from, date_to, keyword=None, page_size=10):
    """DeepSearch API aggregation í˜¸ì¶œ"""
    
    if not DEEPSEARCH_API_KEY:
        return None
    
    url = "https://api-v2.deepsearch.com/v1/articles/aggregation"
    params = {
        "groupby": groupby,
        "date_from": date_from,
        "date_to": date_to,
        "page_size": page_size,
        "api_key": DEEPSEARCH_API_KEY
    }
    
    if keyword:
        params["keyword"] = keyword
    
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        return data.get('data', [])
    except Exception as e:
        log_error(
            error=e,
            file_name="main.py",
            function_name="get_aggregation_data",
            context=f"Aggregation API í˜¸ì¶œ ì˜¤ë¥˜ - groupby: {groupby}, ê¸°ê°„: {date_from}~{date_to}",
            severity="MEDIUM"
        )
        return None

def get_current_week_news_from_deepsearch_global(query, start_date=None, end_date=None):
    if not DEEPSEARCH_API_KEY:
        print("DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    # í˜„ì¬ ì£¼ê°„ ë‚ ì§œ ì„¤ì • (2025ë…„ 7ì›” 3ì£¼ì°¨)
    if not start_date:
        start_date = "2025-07-14"  # 7ì›” 3ì£¼ì°¨ ì‹œì‘
    if not end_date:
        end_date = "2025-07-20"    # 7ì›” 3ì£¼ì°¨ ì¢…ë£Œ
    
    try:
        # DeepSearch API í˜¸ì¶œ
        url = "https://api-v2.deepsearch.com/v1/global-articles"
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "q": query,
            "limit": 20,  # ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ìˆ˜
            "start_date": start_date,
            "end_date": end_date,
            "sort": "published_at:desc"  # ìµœì‹ ìˆœ ì •ë ¬
        }
        
        response = requests.get(url, params=params)
        response.raise_for_status()
        
        data = response.json()
        articles = []
        
        for item in data.get("data", []):
            # ë‚ ì§œ í•„í„°ë§ (APIì—ì„œ ì´ë¯¸ í•„í„°ë§ë˜ì§€ë§Œ ì¶”ê°€ í™•ì¸)
            pub_date = item.get("published_at", "") or item.get("date", "")
            
            try:
                # ISO í˜•ì‹ ë‚ ì§œ íŒŒì‹±
                if "T" in pub_date:
                    article_date = pub_date.split("T")[0]  # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ
                else:
                    article_date = pub_date
                
                # ì§€ì •ëœ ì£¼ê°„ ë²”ìœ„ ë‚´ì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                if start_date <= article_date <= end_date:
                    articles.append({
                        "title": item.get("title", ""),
                        "description": item.get("summary", "") or (item.get("content", "") or "")[:200] + "...",
                        "link": item.get("url", "") or item.get("link", ""),
                        "pubDate": article_date
                    })
            except Exception as e:
                print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                # ë‚ ì§œ íŒŒì‹±ì´ ì‹¤íŒ¨í•´ë„ ê¸°ì‚¬ëŠ” í¬í•¨ì‹œí‚´
                articles.append({
                    "title": item.get("title", ""),
                    "description": item.get("summary", "") or (item.get("content", "") or "")[:200] + "...",
                    "link": item.get("url", "") or item.get("link", ""),
                    "pubDate": pub_date
                })
                continue
        
        return articles[:10]  # ìµœëŒ€ 10ê°œ ë°˜í™˜
        
    except Exception as e:
        log_error(
            error=e,
            file_name="main.py",
            function_name="get_current_week_news_from_deepsearch",
            context=f"DeepSearch API í˜¸ì¶œ ì˜¤ë¥˜ - ì¿¼ë¦¬: {query}, ê¸°ê°„: {start_date}~{end_date}",
            additional_info={
                "query": query,
                "start_date": start_date,
                "end_date": end_date,
                "has_api_key": bool(DEEPSEARCH_API_KEY)
            },
            severity="HIGH"
        )
        print(f"DeepSearch API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return []

# ì •ì  íŒŒì¼ ì„œë¹™ ì¶”ê°€
app.mount("/static", StaticFiles(directory="."), name="static")

@app.get("/")
async def read_index():
    return FileResponse('index.html')

@app.get("/weekly-summary")
def get_weekly_summary():
    """ì£¼ê°„ í‚¤ì›Œë“œ ìš”ì•½ ì¡°íšŒ"""
    try:
        # ì£¼ê°„ ìš”ì•½ ë°ì´í„° ê²€ìƒ‰ (ë” êµ¬ì²´ì ìœ¼ë¡œ)
        results = search_client.search(
            search_text="",  # ë¹ˆ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œ ëŒ€ìƒ
            filter="id eq 'weekly_summary_2025_week3'",  # ì •í™•í•œ IDë¡œ í•„í„°ë§
            top=1
        )
        
        for doc in results:
            return {
                "title": doc.get("title", ""),
                "content": doc.get("content", ""),
                "date": doc.get("date", "")
            }
                
        return {"title": "ì£¼ê°„ ìš”ì•½ ì—†ìŒ", "content": "ì•„ì§ ë¶„ì„ëœ ì£¼ê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "date": ""}
    except Exception as e:
        log_error(
            error=e,
            file_name="main.py",
            function_name="get_weekly_summary",
            context="ì£¼ê°„ ìš”ì•½ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            additional_info={"search_filter": "id eq 'weekly_summary_2025_week3'"},
            severity="HIGH"
        )
        return {"title": "ì˜¤ë¥˜", "content": f"ì£¼ê°„ ìš”ì•½ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "date": ""}

@app.get("/section-analysis/{section}")
def get_section_analysis(section: str):
    """í‚¤ì›Œë“œì— ëŒ€í•œ ì‚°ì—…ë³„ ì‹œê° ë¶„ì„"""
    
    try:
        # í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        weekly_results = search_client.search(search_text="weekly_summary", top=1)
        current_keywords = ["AI", "ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´"]  # ê¸°ë³¸ê°’
        
        for doc in weekly_results:
            content = doc.get("content", "")
            # contentì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            import re
            keyword_match = re.findall(r'\[(.*?)\]', content)
            if keyword_match and len(keyword_match) >= 3:
                current_keywords = keyword_match[:3]
            break
        
        # í‚¤ì›Œë“œë“¤ì„ ì¡°í•©í•œ ì»¨í…ìŠ¤íŠ¸
        keywords_text = ", ".join(current_keywords)
        
        # í•´ë‹¹ í‚¤ì›Œë“œ ê´€ë ¨ ê¸°ì‚¬ë“¤ ìˆ˜ì§‘
        results = search_client.search(
            search_text=keywords_text,
            top=10
        )
        
        # ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘
        articles_content = []
        for doc in results:
            content = doc.get("content", "")
            if content:
                articles_content.append(content)
        
        context = "\n".join(articles_content[:5])  # ìƒìœ„ 5ê°œ ê¸°ì‚¬ë§Œ ì‚¬ìš©
        
        if not context.strip():
            return {
                "section": section,
                "keywords": current_keywords,
                "analysis": f"í˜„ì¬ ì£¼ìš” í‚¤ì›Œë“œ({keywords_text})ì— ëŒ€í•œ {section} ì‹œê°ì˜ ë¶„ì„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.",
                "summary": "ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }
        
        # LLMì„ í†µí•œ í‚¤ì›Œë“œë³„ ì‚°ì—… ì‹œê° ë¶„ì„
        analysis = generate_keyword_section_analysis(section, current_keywords, context)
        
        return {
            "section": section,
            "keywords": current_keywords,
            "analysis": analysis,
            "summary": f"{section} ì‹œê°ì—ì„œ ë³¸ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ"
        }
        
    except Exception as e:
        return {
            "section": section, 
            "keywords": [],
            "analysis": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "summary": "ì˜¤ë¥˜ ë°œìƒ"
        }

def generate_keyword_section_analysis(section: str, keywords: list, context: str):
    """í‚¤ì›Œë“œì— ëŒ€í•œ ì‚°ì—…ë³„ ì‹œê° ë¶„ì„ ìƒì„±"""
    keywords_text = ", ".join(keywords)
    
    # ì„¹ì…˜ë³„ ê´€ì  ì •ì˜
    section_perspectives = {
        "ì‚¬íšŒ": "ì‚¬íšŒì  ì˜í–¥, ì¼ìë¦¬ ë³€í™”, êµìœ¡, ë””ì§€í„¸ ê²©ì°¨, ìœ¤ë¦¬ì  ì¸¡ë©´",
        "ê²½ì œ": "ê²½ì œì  íŒŒê¸‰íš¨ê³¼, ì‹œì¥ ê·œëª¨, íˆ¬ì ë™í–¥, ì‚°ì—… ì„±ì¥, ë¹„ìš© íš¨ìœ¨ì„±",
        "ITê³¼í•™": "ê¸°ìˆ ì  í˜ì‹ , ì—°êµ¬ê°œë°œ ë™í–¥, ê¸°ìˆ  í‘œì¤€, íŠ¹í—ˆ, ê¸°ìˆ ì  í•œê³„ì™€ ë°œì „ë°©í–¥",
        "ì„¸ê³„": "ê¸€ë¡œë²Œ ê²½ìŸë ¥, êµ­ê°€ê°„ ê¸°ìˆ  ê²©ì°¨, êµ­ì œ í˜‘ë ¥, í‘œì¤€í™”, ì§€ì •í•™ì  ì˜í–¥",
        "ìƒí™œë¬¸í™”": "ì¼ìƒìƒí™œ ë³€í™”, ì†Œë¹„ì í–‰ë™, ë¬¸í™”ì  ìˆ˜ìš©ì„±, ë¼ì´í”„ìŠ¤íƒ€ì¼ ê°œì„ "
    }
    
    perspective = section_perspectives.get(section, f"{section} ë¶„ì•¼ì˜ ì „ë¬¸ì  ê´€ì ")
    
    prompt = f"""
ì£¼ìš” í‚¤ì›Œë“œ: {keywords_text}

ìœ„ í‚¤ì›Œë“œë“¤ì— ëŒ€í•´ {section} ë¶„ì•¼ì˜ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.
ë¶„ì„ ê´€ì : {perspective}

ë‹¤ìŒ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”:
{context}

ë¶„ì„ í˜•ì‹:
1. {section} ê´€ì ì—ì„œ ë³¸ ì£¼ìš” í‚¤ì›Œë“œì˜ ì˜ë¯¸ (2-3ì¤„)
2. {section} ë¶„ì•¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (2-3ì¤„)
3. {section} ì¸¡ë©´ì—ì„œì˜ ì „ë§ê³¼ ê³¼ì œ (2-3ì¤„)

ì „ë¬¸ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
    
    try:
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ {section} ë¶„ì•¼ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í‚¤ì›Œë“œë“¤ì„ {section} ê´€ì ì—ì„œ ì‹¬ì¸µ ë¶„ì„í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@app.post("/chat")
def chat(query: dict):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ ê¸°ë°˜ ë™ì  ì±—ë´‡"""
    question = query.get("question", "")
    
    if not question:
        return {"answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}
    
    # 1. ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì‚°ì—… ë¶„ë¥˜
    keyword_info = extract_keyword_and_industry(question)
    
    # 2. í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    current_keywords = get_current_weekly_keywords()
    
    # 3. ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë™ì  ì‘ë‹µ
    if keyword_info["type"] == "industry_analysis":
        # ì‚°ì—…ë³„ ë¶„ì„ ìš”ì²­
        answer = generate_industry_based_answer(
            question, 
            keyword_info["keyword"], 
            keyword_info["industry"], 
            current_keywords
        )
    elif keyword_info["type"] == "keyword_trend":
        # í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ìš”ì²­
        answer = generate_keyword_trend_answer(question, keyword_info["keyword"])
    elif keyword_info["type"] == "comparison":
        # ë¹„êµ ë¶„ì„ ìš”ì²­
        answer = generate_comparison_answer(question, keyword_info["keywords"])
    else:
        # ì¼ë°˜ ì§ˆë¬¸ - ê¸°ì¡´ ë°©ì‹ + í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸
        answer = generate_contextual_answer(question, current_keywords)
    
    return {"answer": answer}

def extract_keyword_and_industry(question):
    """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œì™€ ì‚°ì—… ë¶„ë¥˜ ì¶”ì¶œ"""
    import re
    
    # ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
    industry_keywords = {
        "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ì¼ìë¦¬", "ë³µì§€", "ì •ì±…", "ì œë„", "ì‹œë¯¼", "ê³µê³µ"],
        "ê²½ì œ": ["ê²½ì œ", "ì‹œì¥", "íˆ¬ì", "ê¸ˆìœµ", "ì£¼ê°€", "ë¹„ìš©", "ìˆ˜ìµ", "ë§¤ì¶œ", "ê¸°ì—…"],
        "IT/ê³¼í•™": ["ê¸°ìˆ ", "ê°œë°œ", "í˜ì‹ ", "ì—°êµ¬", "ê³¼í•™", "IT", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´", "í”Œë«í¼"],
        "ìƒí™œ/ë¬¸í™”": ["ìƒí™œ", "ë¬¸í™”", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì†Œë¹„", "íŠ¸ë Œë“œ", "ì¼ìƒ", "ì—¬ê°€", "ì—”í„°í…Œì¸ë¨¼íŠ¸"],
        "ì„¸ê³„": ["ê¸€ë¡œë²Œ", "êµ­ì œ", "ì„¸ê³„", "í•´ì™¸", "ìˆ˜ì¶œ", "í˜‘ë ¥", "ê²½ìŸ", "í‘œì¤€"]
    }
    
    question_lower = question.lower()
    
    # ì‚°ì—… ë¶„ë¥˜ ì¶”ì¶œ
    detected_industry = None
    for industry, keywords in industry_keywords.items():
        if any(keyword in question_lower for keyword in keywords):
            detected_industry = industry
            break
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
    # í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œì™€ ë§¤ì¹˜ë˜ëŠ” ê²ƒ ì°¾ê¸°
    current_keywords = get_current_weekly_keywords()
    detected_keyword = None
    for keyword in current_keywords:
        if keyword in question or keyword.lower() in question_lower:
            detected_keyword = keyword
            break
    
    # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
    if detected_industry and detected_keyword:
        question_type = "industry_analysis"
    elif "vs" in question_lower or "ë¹„êµ" in question_lower or "ì°¨ì´" in question_lower:
        question_type = "comparison"
        # ë¹„êµ ëŒ€ìƒ í‚¤ì›Œë“œë“¤ ì¶”ì¶œ
        comparison_keywords = [kw for kw in current_keywords if kw.lower() in question_lower]
        return {
            "type": question_type,
            "keywords": comparison_keywords,
            "industry": detected_industry,
            "keyword": detected_keyword
        }
    elif detected_keyword:
        question_type = "keyword_trend"
    else:
        question_type = "general"
    
    return {
        "type": question_type,
        "keyword": detected_keyword,
        "industry": detected_industry or "ì‚¬íšŒ"  # ê¸°ë³¸ê°’
    }

def get_current_weekly_keywords():
    """í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # Azure Searchì—ì„œ ëª¨ë“  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        results = search_client.search(search_text="*", top=50)
        
        # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for doc in results:
            content = doc.get("content", "")
            # "í•µì‹¬ í‚¤ì›Œë“œ: " ë¶€ë¶„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if "í•µì‹¬ í‚¤ì›Œë“œ:" in content:
                keywords_text = content.split("í•µì‹¬ í‚¤ì›Œë“œ:")[1].split("\n")[0].strip()
                keywords = [k.strip() for k in keywords_text.split(",")]
                all_keywords.extend(keywords)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚° ë° Top 3 ì„ íƒ
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        top_keywords = [keyword for keyword, count in keyword_counts.most_common(3)]
        
        return top_keywords if top_keywords else ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"]
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"]

def generate_industry_based_answer(question, keyword, industry, current_keywords):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    try:
        # ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
        results = search_client.search(search_text=keyword, top=5)
        context = "\n".join([doc.get("content", "") for doc in results])
        
        # ì‚°ì—…ë³„ ê´€ì  ì •ì˜
        industry_context = {
            "ì‚¬íšŒ": "ì‚¬íšŒì  ì˜í–¥, ì •ì±…ì  ì¸¡ë©´, ì‹œë¯¼ ìƒí™œ ë³€í™”",
            "ê²½ì œ": "ê²½ì œì  íŒŒê¸‰íš¨ê³¼, ì‹œì¥ ë™í–¥, íˆ¬ì ê´€ì ",
            "IT/ê³¼í•™": "ê¸°ìˆ ì  í˜ì‹ , ì—°êµ¬ê°œë°œ ë™í–¥, ê¸°ìˆ ì  ê³¼ì œ",
            "ìƒí™œ/ë¬¸í™”": "ì¼ìƒìƒí™œ ë³€í™”, ë¬¸í™”ì  ìˆ˜ìš©ì„±, ì†Œë¹„ì í–‰ë™",
            "ì„¸ê³„": "ê¸€ë¡œë²Œ íŠ¸ë Œë“œ, êµ­ì œ ê²½ìŸ, í•´ì™¸ ë™í–¥"
        }
        
        context_desc = industry_context.get(industry, "ì „ë°˜ì ì¸ ê´€ì ")
        
        prompt = f"""
ì§ˆë¬¸: {question}
í‚¤ì›Œë“œ: {keyword}
ê´€ì : {industry} ({context_desc})
í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(current_keywords)}

ë‹¤ìŒ ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ {industry} ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{context}

ë‹µë³€ í˜•ì‹:
1. {industry} ê´€ì ì—ì„œ ë³¸ '{keyword}'ì˜ í˜„ì¬ ìƒí™©
2. ì£¼ìš” ë™í–¥ê³¼ ë³€í™”
3. ì „ë§ê³¼ ì‹œì‚¬ì 

êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ {industry} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {industry} ê´€ì ì—ì„œ í‚¤ì›Œë“œì— ëŒ€í•´ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. {industry} ê´€ì ì—ì„œì˜ '{keyword}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_keyword_trend_answer(question, keyword):
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ë‹µë³€ ìƒì„±"""
    try:
        # í‚¤ì›Œë“œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
        results = search_client.search(search_text=keyword, top=8)
        articles = []
        for doc in results:
            content = doc.get("content", "")
            date = doc.get("date", "")
            if content:
                articles.append(f"[{date}] {content}")
        
        context = "\n".join(articles)
        
        prompt = f"""
ì§ˆë¬¸: {question}
í‚¤ì›Œë“œ: {keyword}

ë‹¤ìŒ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{keyword}'ì˜ ìµœê·¼ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
{context}

ë¶„ì„ ë‚´ìš©:
1. ìµœê·¼ '{keyword}' ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ ë™í–¥
2. ì‹œê°„ì  ë³€í™”ì™€ ë°œì „ ë°©í–¥
3. í–¥í›„ ì „ë§ê³¼ ê´€ì‹¬ í¬ì¸íŠ¸

ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ íŠ¸ë Œë“œë¥¼ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ '{keyword}' ë¶„ì•¼ì˜ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œê°„ì  ë³€í™”ì™€ ë™í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{keyword}' íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_comparison_answer(question, keywords):
    """ë¹„êµ ë¶„ì„ ë‹µë³€ ìƒì„±"""
    try:
        # ê° í‚¤ì›Œë“œë³„ë¡œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
        comparison_data = {}
        for keyword in keywords:
            results = search_client.search(search_text=keyword, top=3)
            articles = [doc.get("content", "") for doc in results]
            comparison_data[keyword] = "\n".join(articles)
        
        context = ""
        for keyword, content in comparison_data.items():
            context += f"\n=== {keyword} ê´€ë ¨ ë‰´ìŠ¤ ===\n{content}\n"
        
        prompt = f"""
ì§ˆë¬¸: {question}
ë¹„êµ ëŒ€ìƒ: {', '.join(keywords)}

ë‹¤ìŒ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‚¤ì›Œë“œë“¤ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”:
{context}

ë¹„êµ ë¶„ì„ ë‚´ìš©:
1. ê° í‚¤ì›Œë“œì˜ í˜„ì¬ ìƒí™©ê³¼ íŠ¹ì§•
2. ê³µí†µì ê³¼ ì°¨ì´ì 
3. ìƒí˜¸ ê´€ê³„ì™€ ì˜í–¥
4. ê°ê°ì˜ ì „ë§ê³¼ ì¤‘ìš”ì„±

ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ë¹„êµí•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=600
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_contextual_answer(question, current_keywords):
    """í˜„ì¬ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¼ë°˜ ë‹µë³€ ìƒì„±"""
    try:
        # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê¸°ì‚¬ ê²€ìƒ‰
        results = search_client.search(search_text=question, top=5)
        context = "\n".join([doc.get("content", "") for doc in results])
        
        # í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        keywords_context = f"í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(current_keywords)}"
        
        prompt = f"""
ì§ˆë¬¸: {question}
{keywords_context}

ë‹¤ìŒ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
{context}

ë‹µë³€ ì‹œ ê³ ë ¤ì‚¬í•­:
1. í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œì™€ì˜ ì—°ê´€ì„± ì–¸ê¸‰
2. êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ë°ì´í„° í™œìš©
3. ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ì„¤ëª…
4. ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ

ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        prompt = f"""
ì§ˆë¬¸: {question}
{keywords_context}

ë‹¤ìŒ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
{context}

í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œë“¤ê³¼ ì—°ê´€ì§€ì–´ ë‹µë³€í•˜ë˜, ì§ˆë¬¸ì˜ ë§¥ë½ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ({', '.join(current_keywords)})ë¥¼ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def get_embedding(text):
    # Azure OpenAI text-embedding-ada-002ë¡œ ì„ë² ë”© ìƒì„±
    try:
        response = openai_client.embeddings.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response.data[0].embedding
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")

def generate_answer(question, context):
    # Azure OpenAI GPT-4oë¡œ ë‹µë³€ ìƒì„±
    try:
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”."},
                {"role": "user", "content": f"ì»¨í…ìŠ¤íŠ¸: {context}\n\nì§ˆë¬¸: {question}"}
            ]
        )
        return completion.choices[0].message.content
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")

# ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸ë“¤ ì¶”ê°€

@app.get("/weekly-keywords")
def get_weekly_keywords():
    """ì£¼ê°„ Top 3 í‚¤ì›Œë“œ ë°˜í™˜"""
    try:
        # Azure Searchì—ì„œ ëª¨ë“  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        results = search_client.search(search_text="*", top=100)
        
        # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for doc in results:
            content = doc.get("content", "")
            # "í•µì‹¬ í‚¤ì›Œë“œ: " ë¶€ë¶„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if "í•µì‹¬ í‚¤ì›Œë“œ:" in content:
                keywords_text = content.split("í•µì‹¬ í‚¤ì›Œë“œ:")[1].strip()
                keywords = [k.strip() for k in keywords_text.split(",")]
                all_keywords.extend(keywords)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚° ë° Top 3 ì„ íƒ
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        top_keywords = [keyword for keyword, count in keyword_counts.most_common(3)]
        
        response_data = {
            "keywords": top_keywords,
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.11~07.17) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        # ì˜¤ë¥˜ ì‹œ ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
        response_data = {
            "keywords": ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"],
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.11~07.17) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")

@app.get("/weekly-keywords-by-date")
def get_weekly_keywords_by_date(start_date: str, end_date: str):
    """íŠ¹ì • ë‚ ì§œ ë²”ìœ„ì˜ ì£¼ê°„ í‚¤ì›Œë“œ ë°˜í™˜ (ì‹¤ì œ aggregation ë°ì´í„° ì‚¬ìš©)"""
    try:
        # DeepSearch aggregation APIë¡œ ì‹¤ì œ í•« í‚¤ì›Œë“œ ì¶”ì¶œ
        hot_keywords = get_weekly_hot_keywords_from_aggregation(start_date, end_date)
        
        if hot_keywords:
            # í‚¤ì›Œë“œì™€ ì¹´ìš´íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜
            formatted_keywords = []
            for item in hot_keywords:
                if isinstance(item, dict):
                    keyword_with_count = f"{item['keyword']} ({item['count']:,}íšŒ)"
                    formatted_keywords.append(keyword_with_count)
                else:
                    # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜ì„±ì„ ìœ„í•´
                    formatted_keywords.append(str(item))
            
            return JSONResponse(content={
                "keywords": formatted_keywords,
                "week_info": f"{start_date} ~ {end_date} ì£¼ê°„ ë¶„ì„ (ì‹¤ì œ ë°ì´í„°)",
                "article_count": sum(item['count'] if isinstance(item, dict) else 0 for item in hot_keywords)
            }, media_type="application/json; charset=utf-8")
        else:
            # Aggregation ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            queries = ["AI", "ì¸ê³µì§€ëŠ¥", "ê¸°ìˆ ", "ê²½ì œ", "ì‚¬íšŒ", "ì •ì¹˜", "IT", "ìŠ¤íƒ€íŠ¸ì—…", "íˆ¬ì"]
            all_articles = []
            
            for query in queries[:3]:  # API í˜¸ì¶œ ì œí•œìœ¼ë¡œ 3ê°œë§Œ ì‚¬ìš©
                articles = get_current_week_news_from_deepsearch(query, start_date, end_date)
                all_articles.extend(articles)
                
                # API í˜¸ì¶œ ê°„ ë”œë ˆì´
                import time
                time.sleep(0.1)
            
            if not all_articles:
                # ëª¨ë“  API ì‹¤íŒ¨ì‹œ í•´ë‹¹ ì£¼ê°„ì˜ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜
                week_keywords = get_sample_keywords_by_date(start_date, end_date)
            return JSONResponse(content={
                "keywords": week_keywords,
                "week_info": f"{start_date} ~ {end_date} ì£¼ê°„ ë¶„ì„",
                "article_count": 0
            }, media_type="application/json; charset=utf-8")
        
        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords_from_articles(all_articles)
        
        response_data = {
            "keywords": keywords[:3],  # Top 3 í‚¤ì›Œë“œ
            "week_info": f"{start_date} ~ {end_date} ì£¼ê°„ ë¶„ì„",
            "article_count": len(all_articles)
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
        
    except Exception as e:
        print(f"ë‚ ì§œë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ í•´ë‹¹ ì£¼ê°„ì˜ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜
        week_keywords = get_sample_keywords_by_date(start_date, end_date)
        return JSONResponse(content={
            "keywords": week_keywords,
            "week_info": f"{start_date} ~ {end_date} ì£¼ê°„ ë¶„ì„",
            "article_count": 0
        }, media_type="application/json; charset=utf-8")

def get_sample_keywords_by_date(start_date: str, end_date: str):
    """ë‚ ì§œì— ë”°ë¥¸ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜"""
    if "07-01" in start_date:  # 7ì›” 1ì£¼ì°¨
        return ["ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ì¶©ì „ì¸í”„ë¼"]
    elif "07-06" in start_date:  # 7ì›” 2ì£¼ì°¨  
        return ["ë©”íƒ€ë²„ìŠ¤", "VR", "ê°€ìƒí˜„ì‹¤"]
    elif "07-14" in start_date:  # 7ì›” 3ì£¼ì°¨
        return ["ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›", "AI Youth Festa 2025", "ì¸ê³µì§€ëŠ¥"]
    else:
        return ["ê¸°ìˆ ", "í˜ì‹ ", "ë””ì§€í„¸"]

def extract_keywords_from_articles(articles):
    """ê¸°ì‚¬ë“¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        # ëª¨ë“  ê¸°ì‚¬ì˜ ì œëª©ê³¼ ì„¤ëª…ì„ í•©ì³ì„œ í…ìŠ¤íŠ¸ ìƒì„±
        all_text = ""
        for article in articles:
            all_text += f"{article.get('title', '')} {article.get('description', '')} "
        
        if not all_text.strip():
            return ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
        
        # OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê³  í•µì‹¬ì ì¸ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í‚¤ì›Œë“œëŠ” ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n\n{all_text[:2000]}"  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                }
            ],
            max_tokens=100,
            temperature=0.3
        )
        
        keywords_text = response.choices[0].message.content
        if keywords_text:
            keywords_text = keywords_text.strip()
            keywords = [k.strip() for k in keywords_text.split(",")]
        else:
            keywords = ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
        
        return keywords[:3] if len(keywords) >= 3 else keywords + ["ê¸°ìˆ ", "ë‰´ìŠ¤", "íŠ¸ë Œë“œ"][:3-len(keywords)]
        
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ["AI", "ê¸°ìˆ ", "í˜ì‹ "]

@app.post("/industry-analysis")
def get_industry_analysis(request: dict):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ (ê¸°ì¡´ + ì •ë°˜ëŒ€ ê´€ì )"""
    industry = request.get("industry", "")
    keyword = request.get("keyword", "")
    
    if not industry or not keyword:
        raise HTTPException(status_code=400, detail="ì‚°ì—…ê³¼ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
    
    # ê¸°ì¡´ ë¶„ì„ í”„ë¡¬í”„íŠ¸
    industry_prompts = {
        "ì‚¬íšŒ": f"'{keyword}'ì— ëŒ€í•œ ì‚¬íšŒì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì‚¬íšŒ êµ¬ì¡°, ì‹œë¯¼ ìƒí™œ, ì‚¬íšŒ ë¬¸ì œ í•´ê²° ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ê²½ì œ": f"'{keyword}'ì— ëŒ€í•œ ê²½ì œì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì‹œì¥ ì˜í–¥, íˆ¬ì ì „ë§, ì‚°ì—… íŒŒê¸‰íš¨ê³¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "IT/ê³¼í•™": f"'{keyword}'ì— ëŒ€í•œ IT/ê³¼í•™ ê¸°ìˆ ì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê¸°ìˆ  ë°œì „, í˜ì‹  ë™í–¥, ê¸°ìˆ ì  ê³¼ì œ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ìƒí™œ/ë¬¸í™”": f"'{keyword}'ì— ëŒ€í•œ ìƒí™œ/ë¬¸í™”ì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì¼ìƒ ìƒí™œ ë³€í™”, ë¬¸í™”ì  ì˜í–¥, ë¼ì´í”„ìŠ¤íƒ€ì¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì„¸ê³„": f"'{keyword}'ì— ëŒ€í•œ ê¸€ë¡œë²Œ/êµ­ì œì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. êµ­ì œ ë™í–¥, ê¸€ë¡œë²Œ ê²½ìŸ, ì™¸êµì  ì˜í–¥ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    }
    
    # ì •ë°˜ëŒ€ ê´€ì  í”„ë¡¬í”„íŠ¸
    counter_prompts = {
        "ì‚¬íšŒ": f"'{keyword}'ì— ëŒ€í•œ ë¹„íŒì /íšŒì˜ì  ì‚¬íšŒ ê´€ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ì‚¬íšŒì  ìš°ë ¤, ë¶€ì‘ìš©, ê²©ì°¨ ì‹¬í™” ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ê²½ì œ": f"'{keyword}'ì— ëŒ€í•œ ê²½ì œì  ë¦¬ìŠ¤í¬ì™€ ë¶€ì •ì  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ì‹œì¥ ë¶ˆì•ˆì •ì„±, íˆ¬ì ìœ„í—˜, ê²½ì œì  ë¶€ì‘ìš© ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "IT/ê³¼í•™": f"'{keyword}'ì— ëŒ€í•œ ê¸°ìˆ ì  í•œê³„ì™€ ë¬¸ì œì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ê¸°ìˆ ì  ìœ„í—˜, ìœ¤ë¦¬ì  ë¬¸ì œ, ë°œì „ ì¥ì• ë¬¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ìƒí™œ/ë¬¸í™”": f"'{keyword}'ì— ëŒ€í•œ ë¬¸í™”ì  ì €í•­ê³¼ ìƒí™œìƒì˜ ë¬¸ì œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ì „í†µ ë¬¸í™” ì¶©ëŒ, ìƒí™œ ë¶ˆí¸, ë¬¸í™”ì  ë¶€ì‘ìš© ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì„¸ê³„": f"'{keyword}'ì— ëŒ€í•œ êµ­ì œì  ê°ˆë“±ê³¼ ë¶€ì •ì  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. êµ­ê°€ê°„ ë¶„ìŸ, ê¸€ë¡œë²Œ ë¶ˆí‰ë“±, êµ­ì œì  ìš°ë ¤ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    }
    
    main_prompt = industry_prompts.get(industry, f"'{keyword}'ì— ëŒ€í•œ {industry} ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    counter_prompt = counter_prompts.get(industry, f"'{keyword}'ì— ëŒ€í•œ {industry} ê´€ì ì—ì„œì˜ ë°˜ëŒ€ ì˜ê²¬ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    
    try:
        import openai
        openai.api_type = "azure"
        openai.api_key = AZURE_OPENAI_API_KEY
        openai.azure_endpoint = AZURE_OPENAI_ENDPOINT
        openai.api_version = "2024-02-15-preview"
        
        # ê¸°ì¡´ ë¶„ì„
        main_completion = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"{industry} ë¶„ì•¼ ì „ë¬¸ê°€ë¡œì„œ í‚¤ì›Œë“œì— ëŒ€í•œ ê¸ì •ì  ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": main_prompt}
            ]
        )
        
        # ì •ë°˜ëŒ€ ê´€ì  ë¶„ì„
        counter_completion = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"{industry} ë¶„ì•¼ì˜ ë¹„íŒì  ì‹œê°ì„ ê°€ì§„ ì „ë¬¸ê°€ë¡œì„œ ë°˜ëŒ€ ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤."},
                {"role": "user", "content": counter_prompt}
            ]
        )
        
        return {
            "analysis": main_completion.choices[0].message.content,
            "counter_analysis": counter_completion.choices[0].message.content
        }
    except Exception as e:
        return {
            "analysis": f"ë¶„ì„ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "counter_analysis": "ë°˜ëŒ€ ì˜ê²¬ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

@app.get("/keyword-articles")
def get_keyword_articles(keyword: str, start_date: Optional[str] = None, end_date: Optional[str] = None):
    """í‚¤ì›Œë“œ ê´€ë ¨ ê¸°ì‚¬ Top 5 ì¡°íšŒ (DeepSearch API ì‚¬ìš©)"""
    try:
        # í‚¤ì›Œë“œì—ì„œ "(níšŒ)" í˜•íƒœì˜ í…ìŠ¤íŠ¸ ì œê±°
        import re
        clean_keyword = re.sub(r'\s*\(\d+[,\d]*íšŒ\)', '', keyword).strip()
        
        print(f"ğŸ” í‚¤ì›Œë“œ ê¸°ì‚¬ ì¡°íšŒ - ì›ë³¸: {keyword}, ì •ë¦¬ëœ í‚¤ì›Œë“œ: {clean_keyword}")
        print(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}")
        
        # ë‚ ì§œ ë²”ìœ„ê°€ ì œê³µë˜ë©´ í•´ë‹¹ ë‚ ì§œ ë²”ìœ„ë¡œ ê²€ìƒ‰
        if start_date and end_date:
            deepsearch_articles = get_current_week_news_from_deepsearch(clean_keyword, start_date, end_date)
        else:
            # ê¸°ë³¸ê°’: í˜„ì¬ ì£¼ì°¨ (3ì£¼ì°¨)
            deepsearch_articles = get_current_week_news_from_deepsearch(clean_keyword, "2025-07-14", "2025-07-18")
        
        print(f"ğŸ“° DeepSearch ê²€ìƒ‰ ê²°ê³¼: {len(deepsearch_articles)}ê°œ ê¸°ì‚¬")
        
        if deepsearch_articles:
            # DeepSearch API ê²°ê³¼ë¥¼ ì‚¬ìš©
            articles = []
            for article in deepsearch_articles[:5]:  # Top 5ë§Œ ì„ íƒ
                print(f"  - ì œëª©: {article['title'][:50]}...")
                print(f"    URL: {article['link']}")
                articles.append({
                    "title": article["title"],
                    "summary": article["description"],
                    "date": article["pubDate"],
                    "url": article["link"]
                })
            
            print(f"âœ… DeepSearch APIë¡œ {len(articles)}ê°œ ê¸°ì‚¬ ë°˜í™˜")
            return {"articles": articles}
        
        # DeepSearch API ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Azure Search ì‚¬ìš©
        print("ğŸ” DeepSearch ê²°ê³¼ê°€ ì—†ì–´ Azure Search ì‚¬ìš©")
        results = search_client.search(
            search_text=clean_keyword,
            top=10,
            select=["title", "content", "date"]
        )
        
        articles = []
        for doc in results:
            if len(articles) >= 5:
                break
                
            title = doc.get("title", "")
            content = doc.get("content", "")
            date = doc.get("date", "")
            
            if title and content:
                # ê°„ë‹¨í•œ ìš”ì•½ (ì²˜ìŒ 200ì)
                summary = content[:200] + "..." if len(content) > 200 else content
                
                articles.append({
                    "title": title,
                    "summary": summary,
                    "date": date,
                    "url": "#"  # Azure Searchì—ëŠ” URLì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’
                })
        
        print(f"ğŸ“‹ Azure Search ê²°ê³¼: {len(articles)}ê°œ ê¸°ì‚¬")
        
        if not articles:
            print("âŒ ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ë” ì¹œí™”ì ì¸ ë©”ì‹œì§€ì™€ ëŒ€ì•ˆ ì œì•ˆ
            suggestions = []
            
            # ê¸°ë³¸ í‚¤ì›Œë“œ ì œì•ˆ
            if clean_keyword.lower() in ["db", "ë””ë¹„"]:
                suggestions = ["DBê¸ˆìœµíˆ¬ì", "DBì†í•´ë³´í—˜", "DBê·¸ë£¹", "ì‚¼ì„±ì „ì", "ë„¤ì´ë²„"]
            elif clean_keyword.lower() in ["ai", "ì¸ê³µì§€ëŠ¥"]:
                suggestions = ["ChatGPT", "ì‚¼ì„±ì „ì", "ë„¤ì´ë²„", "SKT", "LGì „ì"]
            else:
                suggestions = ["ì‚¼ì„±ì „ì", "ë„¤ì´ë²„", "AI", "ë°˜ë„ì²´", "5G"]
            
            return {
                "articles": [], 
                "message": f"'{clean_keyword}' í‚¤ì›Œë“œ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "suggestions": {
                    "reason": "í•´ë‹¹ ê¸°ê°„(2025-07-14 ~ 2025-07-18)ì— ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ êµ¬ì²´ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "alternatives": suggestions,
                    "tip": "ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë‚˜ ê´€ë ¨ ê¸°ì—…ëª…ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
                }
            }
        
        return {"articles": articles}
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ê¸°ì‚¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {"error": f"ê¸°ì‚¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

@app.post("/keyword-analysis")
def analyze_keyword_dynamically(request: dict):
    """ë™ì  í‚¤ì›Œë“œ ë¶„ì„ - í´ë¦­ëœ í‚¤ì›Œë“œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„"""
    keyword = request.get("keyword", "")
    
    if not keyword:
        return {"error": "í‚¤ì›Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."}
    
    try:
        # 1. í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
        search_results = search_client.search(
            search_text=keyword,
            top=10,
            select=["title", "content", "date"]  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í•„ë“œë§Œ ì„ íƒ
        )
        
        # 2. ê²€ìƒ‰ëœ ë‰´ìŠ¤ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
        context_articles = []
        for doc in search_results:
            if len(context_articles) >= 5:
                break
            context_articles.append({
                "title": doc.get("title", ""),
                "content": doc.get("content", ""),
                "date": doc.get("date", ""),
                "section": "AI/ê¸°ìˆ "  # ê¸°ë³¸ê°’ ì„¤ì •
            })
        
        # 3. ë‹¤ê°ë„ ë¶„ì„ ìƒì„±
        perspectives = [
            "ì‚¬íšŒì  ì˜í–¥",
            "ê²½ì œì  ì¸¡ë©´", 
            "ê¸°ìˆ ì  í˜ì‹ ",
            "ë¯¸ë˜ ì „ë§",
            "ì£¼ìš” ì´ìŠˆ"
        ]
        
        analyses = {}
        
        for perspective in perspectives:
            analysis_text = generate_perspective_analysis(keyword, perspective, context_articles)
            analyses[perspective] = analysis_text
        
        # 4. í‚¤ì›Œë“œ íŠ¸ë Œë“œ ìš”ì•½
        trend_summary = generate_keyword_trend_summary(keyword, context_articles)
        
        return {
            "keyword": keyword,
            "trend_summary": trend_summary,
            "perspectives": analyses,
            "related_articles": context_articles
        }
        
    except Exception as e:
        log_error(
            error=e,
            file_name="main.py",
            function_name="analyze_keyword_dynamically",
            context=f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - í‚¤ì›Œë“œ: {keyword}",
            additional_info={
                "keyword": keyword,
                "search_attempted": True
            },
            severity="HIGH"
        )
        return {"error": f"ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}

def generate_perspective_analysis(keyword, perspective, articles):
    """íŠ¹ì • ê´€ì ì—ì„œì˜ í‚¤ì›Œë“œ ë¶„ì„ ìƒì„±"""
    try:
        # ê´€ë ¨ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        context = "\n".join([
            f"ì œëª©: {article['title']}\në‚´ìš©: {article['content'][:200]}..."
            for article in articles[:3]
        ])
        
        prompt = f"""
ë‹¤ìŒ í‚¤ì›Œë“œì— ëŒ€í•´ {perspective} ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

í‚¤ì›Œë“œ: {keyword}

ê´€ë ¨ ë‰´ìŠ¤:
{context}

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
- {perspective} ê´€ì ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œì˜ ì˜ë¯¸ì™€ ì¤‘ìš”ì„±
- í˜„ì¬ íŠ¸ë Œë“œì™€ í–¥í›„ ì „ë§
- í•µì‹¬ í¬ì¸íŠ¸ 3ê°€ì§€
- ê°„ê²°í•˜ê³  ëª…í™•í•œ ì„¤ëª… (200ì ì´ë‚´)

ë¶„ì„ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ íŠ¹ì • ê´€ì ì—ì„œ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ê³ , í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=300
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_keyword_trend_summary(keyword, articles):
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ìš”ì•½ ìƒì„±"""
    try:
        context = "\n".join([
            f"ì œëª©: {article['title']}\në‚´ìš©: {article['content'][:150]}..."
            for article in articles[:5]
        ])
        
        prompt = f"""
ë‹¤ìŒ í‚¤ì›Œë“œì˜ í˜„ì¬ íŠ¸ë Œë“œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

í‚¤ì›Œë“œ: {keyword}

ê´€ë ¨ ë‰´ìŠ¤:
{context}

ìš”ì•½ ìš”êµ¬ì‚¬í•­:
- í˜„ì¬ ì´ìŠˆì˜ í•µì‹¬ ë‚´ìš©
- ì£¼ìš” ì´í•´ê´€ê³„ìë“¤
- ì‚¬íšŒì /ê²½ì œì  ì˜í–¥
- í•œ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½

150ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ ì´ìŠˆë¥¼ í•µì‹¬ë§Œ ê°„ì¶”ë ¤ ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=200
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ íŠ¸ë Œë“œ ë¶„ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if not articles:
            # ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
            articles = [
                {
                    "title": f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ 1",
                    "summary": f"{keyword}ì— ëŒ€í•œ ìµœì‹  ë™í–¥ê³¼ ê´€ë ¨ëœ ì£¼ìš” ë‚´ìš©ì„ ë‹¤ë£¬ ê¸°ì‚¬ì…ë‹ˆë‹¤.",
                    "date": "2025-07-17",
                    "url": "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=001&aid=0014567890"
                },
                {
                    "title": f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ 2", 
                    "summary": f"{keyword} ë¶„ì•¼ì˜ ìƒˆë¡œìš´ ë°œì „ê³¼ ì „ë§ì— ëŒ€í•´ ë¶„ì„í•œ ê¸°ì‚¬ì…ë‹ˆë‹¤.",
                    "date": "2025-07-16",
                    "url": "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=001&aid=0014567891"
                },
                {
                    "title": f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ 3",
                    "summary": f"{keyword} ì‚°ì—…ì˜ ë¯¸ë˜ ì „ë§ê³¼ ê¸°ìˆ  í˜ì‹  ë™í–¥ì„ ë¶„ì„í•œ ê¸°ì‚¬ì…ë‹ˆë‹¤.",
                    "date": "2025-07-15",
                    "url": "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=001&aid=0014567892"
                },
                {
                    "title": f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ 4",
                    "summary": f"{keyword} ë¶„ì•¼ì˜ ìµœì‹  ì—°êµ¬ ê²°ê³¼ì™€ ì‹œì¥ ë™í–¥ì„ ë‹¤ë£¬ ê¸°ì‚¬ì…ë‹ˆë‹¤.",
                    "date": "2025-07-14",
                    "url": "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=001&aid=0014567893"
                },
                {
                    "title": f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ 5",
                    "summary": f"{keyword} ê¸°ìˆ ì˜ ì‹¤ìš©í™”ì™€ ìƒìš©í™” ê³„íšì— ëŒ€í•œ ê¸°ì‚¬ì…ë‹ˆë‹¤.",
                    "date": "2025-07-13",
                    "url": "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=001&aid=0014567894"
                }
            ]
        
        return {"articles": articles}
    except Exception as e:
        log_error(
            error=e,
            file_name="main.py",
            function_name="get_keyword_articles",
            context=f"í‚¤ì›Œë“œ ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜ - í‚¤ì›Œë“œ: {keyword}",
            additional_info={"keyword": keyword},
            severity="MEDIUM"
        )
        print(f"í‚¤ì›Œë“œ ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            "articles": [
                {
                    "title": "ê²€ìƒ‰ ì˜¤ë¥˜",
                    "summary": f"'{keyword}' ê´€ë ¨ ê¸°ì‚¬ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "date": "ì˜¤ë¥˜",
                    "url": ""
                }
            ]
        }

def calculate_relevance_score(title, content, keyword):
    """í‚¤ì›Œë“œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
    score = 0
    keyword_lower = keyword.lower()
    title_lower = title.lower()
    content_lower = content.lower()
    
    # ì œëª©ì— í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
    if keyword_lower in title_lower:
        score += 3
    
    # ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜
    score += content_lower.count(keyword_lower)
    
    return score

def generate_article_summary(title, content, keyword):
    """ê¸°ì‚¬ ìš”ì•½ ìƒì„± (í‚¤ì›Œë“œ ì¤‘ì‹¬)"""
    try:
        prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ë¥¼ '{keyword}' í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {content[:500]}

ìš”ì•½ì‹œ ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. {keyword}ì™€ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©
2. ì£¼ìš” ë™í–¥ì´ë‚˜ ë³€í™”
3. ì˜í–¥ì´ë‚˜ ì „ë§

ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200
        )
        
        return completion.choices[0].message.content
    except Exception as e:
        print(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
        return content[:100] + "..."

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
