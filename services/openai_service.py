import logging
import re
import asyncio
from typing import List, Dict, Any, Optional
from fastapi import HTTPException
from core.config import openai_client, openai_keyword_explainer_client, ncs_search_client, AZURE_OPENAI_DEPLOYMENT, AZURE_OPENAI_DEPLOYMENT_NCS, AZURE_OPENAI_KEYWORD_EXPLAINER_DEPLOYMENT

logger = logging.getLogger(__name__)

async def search_ncs_documents(query, top_k=3):
    """Azure AI Searchì—ì„œ NCS ì§ë¬´ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³  ê´€ë ¨ ë¬¸ì„œ ë°˜í™˜"""
    if not ncs_search_client:
        logger.warning("NCS Search Clientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìƒ˜í”Œ NCS ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return [
            {"title": "ìƒ˜í”Œ NCS ì§ë¬´: ì¸ê³µì§€ëŠ¥ ê°œë°œì", "content": "ì¸ê³µì§€ëŠ¥ ê°œë°œìëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„, ë°ì´í„° ë¶„ì„, AI ì‹œìŠ¤í…œ êµ¬ì¶• ë“±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìš”êµ¬ ì—­ëŸ‰ìœ¼ë¡œëŠ” íŒŒì´ì¬, ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ì´í•´, ë°ì´í„° ê³¼í•™ ì§€ì‹ ë“±ì´ ìˆìŠµë‹ˆë‹¤.", "source": "NCS ìƒ˜í”Œ"},
            {"title": "ìƒ˜í”Œ NCS ì§ë¬´: ë¹…ë°ì´í„° ë¶„ì„ê°€", "content": "ë¹…ë°ì´í„° ë¶„ì„ê°€ëŠ” ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì €ì¥, ì²˜ë¦¬, ë¶„ì„í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤. í†µê³„í•™, í”„ë¡œê·¸ë˜ë°, ë°ì´í„°ë² ì´ìŠ¤ ì§€ì‹ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.", "source": "NCS ìƒ˜í”Œ"}
        ]

    try:
        logger.info(f"ğŸ” Azure AI Searchì—ì„œ NCS ë°ì´í„° ê²€ìƒ‰ ì¤‘: '{query}'")
        search_results = await asyncio.to_thread(
            ncs_search_client.search,
            search_text=query,
            query_type="semantic",
            semantic_configuration_name="default",
            search_fields=["content"],
            top=top_k
        )

        docs = []
        for result in search_results:
            docs.append(result.get("content", result.get("text", result.get("description", "ë‚´ìš© ì—†ìŒ"))))

        return "\n\n---\n\n".join(docs)
    except Exception as e:
        logger.error(f"âŒ Azure AI Search NCS ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return ""

async def get_job_industry_summary(query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì— ê¸°ë°˜í•˜ì—¬ NCS ì§ë¬´/ì‚°ì—… ì •ë³´ ìš”ì•½ (AI í™œìš©)"""
    logger.info(f"ğŸ§  get_job_industry_summary í˜¸ì¶œ - ì¿¼ë¦¬: {query}")
    context_text = await search_ncs_documents(query)

    if not context_text:
        logger.warning("â— ê´€ë ¨ëœ ì§ë¬´/ì‚°ì—… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ GPT ë‹µë³€ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        system_msg = "ë„ˆëŠ” ì§ë¬´/ì‚°ì—… ê´€ë ¨ ì „ë¬¸ê°€ AIì•¼."
        user_prompt = f"'{query}'ì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ìš”ì•½í•´ì¤˜."
        try:
            response = openai_client.chat.completions.create(
                model=AZURE_OPENAI_DEPLOYMENT_NCS,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.4,
                max_tokens=1500
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ê´€ë ¨ëœ ì§ë¬´/ì‚°ì—… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê³ , ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    system_msg = "ë„ˆëŠ” ì§ë¬´/ì‚°ì—… ê´€ë ¨ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” AI ì§ë¬´ ë¶„ì„ê°€ì•¼. NCS ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´."
    user_prompt = f"""
    ë„ˆëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ë¬´/ì‚°ì—… í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
    Azure Searchì— ì €ì¥ëœ NCS ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ìš”ì•½í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.

    - ì•„ë˜ ê²°ê³¼ëŠ” Azure Searchë¥¼ í†µí•´ ê²€ìƒ‰ëœ ë¬¸ì„œì´ë‹¤. ë°˜ë“œì‹œ ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •ë¦¬í•  ê²ƒ.
    - ê²°ê³¼ê°€ ì—†ìœ¼ë©´ â€œì—†ë‹¤â€ê³  í•˜ì§€ ë§ê³ , ê°€ëŠ¥í•œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ìœ ì¶”í•˜ë¼.

    [ì‚¬ìš©ì ì…ë ¥]
    {query}

    [Azure Search ê²€ìƒ‰ ê²°ê³¼]
    {context_text}

    [ìš”ì•½ í˜•ì‹]
    ---
    ğŸ”§ ì§ë¬´ ê°œìš”  
    ğŸ“š ìš”êµ¬ ì§€ì‹  
    ğŸ›  ìš”êµ¬ ê¸°ìˆ   
    ğŸ¤ ìš”êµ¬ íƒœë„
    """

    try:
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT_NCS,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.4,
            max_tokens=1500
        )
        summary = response.choices[0].message.content.strip()
        logger.info(f"âœ… NCS ìš”ì•½ ì„±ê³µ. ê¸¸ì´: {len(summary)}")
        return summary
    except Exception as e:
        logger.error(f"âŒ NCS ì§ë¬´ ìš”ì•½ ì˜¤ë¥˜: {e}", exc_info=True)
        return "NCS ì§ë¬´ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. API ì—°ê²° ë˜ëŠ” ëª¨ë¸ ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”."

async def extract_keywords_with_gpt(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """GPTë¥¼ ì‚¬ìš©í•´ ê¸°ì‚¬ë“¤ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , ê° í‚¤ì›Œë“œ ì„ ì • ì´ìœ ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not articles:
        logger.warning("âŒ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []

    try:
        top_articles = articles[:10]
        titles_text = "\n".join([f"- {article['title']}" for article in top_articles])

        prompt = f"""ë‹¤ìŒ ITê¸°ìˆ  ë‰´ìŠ¤ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•˜ê³ , ê° í‚¤ì›Œë“œê°€ **í˜„ì¬ ë‰´ìŠ¤ì—ì„œ ì£¼ëª©ë°›ëŠ” êµ¬ì²´ì ì¸ ë°°ê²½ì´ë‚˜ ë™í–¥ì„ í¬í•¨í•˜ì—¬ ì„ ì • ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…**í•˜ì„¸ìš”.

ë‰´ìŠ¤ ì œëª©:
{titles_text}

ì¤‘ìš”: ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€.
í˜•ì‹:
í‚¤ì›Œë“œ1: ì„ ì • ì´ìœ 1
í‚¤ì›Œë“œ2: ì„ ì • ì´ìœ 2
í‚¤ì›Œë“œ3: ì„ ì • ì´ìœ 3
í‚¤ì›Œë“œ4: ì„ ì • ì´ìœ 4
í‚¤ì›Œë“œ5: ì„ ì • ì´ìœ 5
"""
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ITê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€. ê° í‚¤ì›Œë“œë¥¼ ì„ ì •í•œ í•µì‹¬ ì´ìœ ë¥¼ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë” ì‚¬ìš© ê¸ˆì§€."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.2
        )

        keywords_with_reasons_text = response.choices[0].message.content or ""
        logger.info(f"ğŸš€ GPT í‚¤ì›Œë“œ ë° ì´ìœ  ì¶”ì¶œ ì™„ë£Œ: \n{keywords_with_reasons_text}")

        keywords = []
        
        processed_text = re.sub(
            r'(í‚¤ì›Œë“œ(\d+):\s*(.+?))\n\s*(ì„ ì • ì´ìœ \2:\s*(.+?))',
            r'\1 \4',
            keywords_with_reasons_text,
            flags=re.IGNORECASE | re.DOTALL
        )

        lines = processed_text.strip().split('\n')
        filtered_lines = [line.strip() for line in lines if line.strip()]

        for i, line in enumerate(filtered_lines):
            keyword_raw = ""
            reason = "ì´ìœ  ì—†ìŒ"

            if ':' in line:
                first_colon_idx = line.find(':')
                keyword_part = line[:first_colon_idx].strip()
                rest_of_line = line[first_colon_idx+1:].strip()

                reason_match = re.search(r'ì„ ì • ì´ìœ \d+:\s*(.+)', rest_of_line, re.IGNORECASE)
                if reason_match:
                    reason = reason_match.group(1).strip()
                    keyword_raw = re.sub(r'ì„ ì • ì´ìœ \d+:\s*.+', '', rest_of_line, re.IGNORECASE).strip()
                else:
                    reason = rest_of_line
                    keyword_raw = keyword_part
            else:
                keyword_raw = line.strip()
                reason = "GPTê°€ ì„ ì • ì´ìœ ë¥¼ ì œê³µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            keyword_final = re.sub(r'^(í‚¤ì›Œë“œ\d+|Keyword\d+|\d+\.)\s*', '', keyword_raw, flags=re.IGNORECASE).strip()
            keyword_final = re.sub(r'^[^\w\s]*', '', keyword_final).strip()
            keyword_final = re.sub(r'[^\w\s\-/ê°€-í£]', '', keyword_final).strip()
            keyword_final = re.sub(r'[:.]$', '', keyword_final).strip()

            if keyword_final and 2 <= len(keyword_final) <= 30:
                keywords.append({
                    "keyword": keyword_final,
                    "reason": reason,
                    "count": 30 - (len(keywords) * 5),
                    "rank": len(keywords) + 1
                })
                if len(keywords) >= 5:
                    break
            else:
                logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œ íŒŒì‹±ë¨: ì›ë³¸:'{line}', í›„ë³´:'{keyword_raw}', ìµœì¢…:'{keyword_final}' (ê¸¸ì´/ì¡°ê±´ ë¶ˆì¶©ì¡±)")

        if len(keywords) < 3:
            logger.warning("âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¶€ì¡±, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 25, "rank": 1},
                {"keyword": "ë°˜ë„ì²´", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 20, "rank": 2},
                {"keyword": "í´ë¼ìš°ë“œ", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 15, "rank": 3},
                {"keyword": "ë¹…ë°ì´í„°", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 10, "rank": 4},
                {"keyword": "ë¡œë´‡", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 5, "rank": 5}
            ]
        
        return keywords

    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
        return [
            {"keyword": "ì¸ê³µì§€ëŠ¥", "reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ìƒ˜í”Œ ë°ì´í„°", "count": 25, "rank": 1},
            {"keyword": "ë°˜ë„ì²´", "reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ìƒ˜í”Œ ë°ì´í„°", "count": 20, "rank": 2},
            {"keyword": "í´ë¼ìš°ë“œ", "reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ìƒ˜í”Œ ë°ì´í„°", "count": 15, "rank": 3}
        ]

async def extract_global_keywords_with_gpt(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """GPTë¥¼ ì‚¬ìš©í•´ í•´ì™¸ ê¸°ì‚¬ë“¤ì—ì„œ ì˜ì–´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , ê° í‚¤ì›Œë“œ ì„ ì • ì´ìœ ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not articles:
        logger.warning("âŒ ë¶„ì„í•  í•´ì™¸ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    try:
        top_articles = articles[:10]
        titles_text = "\n".join([f"- {article['title']}" for article in top_articles])

        prompt = f"""Extract 5 key English tech keywords from these global news titles:
For each keyword, provide a **detailed reason explaining its current prominence or relevant trend in the news.**

News Titles:
{titles_text}

Requirements:
- Only English words
- Tech/Technology focused
- No Korean words
- No markdown headers (#)
- Plain text only
Format:
Keyword1: Reason1
Keyword2: Reason2
Keyword3: Reason3
Keyword4: Reason4
Keyword5: Reason5
"""
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "You are an expert at extracting English tech keywords from global news. Provide concise reasons for each keyword. Use plain text only, no markdown headers."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.2
        )
        
        keywords_with_reasons_text = response.choices[0].message.content or ""
        logger.info(f"ğŸŒ í•´ì™¸ GPT í‚¤ì›Œë“œ ë° ì´ìœ  ì¶”ì¶œ ì™„ë£Œ: \n{keywords_with_reasons_text}")
        
        keywords = []
        lines = keywords_with_reasons_text.strip().split('\n')
        
        filtered_lines = [line.strip() for line in lines if line.strip()]

        for i, line in enumerate(filtered_lines):
            keyword_raw = ""
            reason = "Reason not provided by AI."
            
            if ':' in line:
                parts = line.split(':', 1)
                keyword_raw = parts[0].strip()
                reason = parts[1].strip()
            else:
                keyword_raw = line.strip()
                reason = "Reason not provided by AI."

            keyword_final = re.sub(r'^(Keyword\d+|\d+\.)\s*:\s*', '', keyword_raw, flags=re.IGNORECASE).strip()
            keyword_final = re.sub(r'^[^\w\s]*', '', keyword_final).strip()
            keyword_final = re.sub(r'[^a-zA-Z0-9\s\-/]', '', keyword_final).strip()
            keyword_final = re.sub(r'[:.]$', '', keyword_final).strip()

            if not keyword_final:
                logger.warning(f"âš ï¸ Global keyword parsing resulted in empty string: Original:'{line}', Candidate:'{keyword_raw}', Final:'{keyword_final}'")
                continue 

            if keyword_final and 2 <= len(keyword_final) <= 30:
                keywords.append({
                    "keyword": keyword_final,
                    "reason": reason,
                    "count": 30 - (len(keywords) * 5),
                    "rank": len(keywords) + 1
                })
                if len(keywords) >= 5:
                    break
            else:
                logger.warning(f"âš ï¸ Invalid global keyword parsed: Original:'{line}', Candidate:'{keyword_raw}', Final:'{keyword_final}' (Length/Condition failed)")
        
        if len(keywords) < 3:
            logger.warning("âš ï¸ Global keyword extraction failed or insufficient, using default keywords.")
            keywords = [
                {"keyword": "AI Technology", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 25, "rank": 1},
                {"keyword": "Innovation", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 20, "rank": 2},
                {"keyword": "Digital Transformation", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 15, "rank": 3},
                {"keyword": "Machine Learning", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 10, "rank": 4},
                {"keyword": "Cloud Computing", "reason": "GPT ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê¸°ë³¸ í‚¤ì›Œë“œ", "count": 5, "rank": 5}
            ]

        return keywords

    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
        return [
            {"keyword": "AI Tech", "reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ìƒ˜í”Œ ë°ì´í„°", "count": 25, "rank": 1},
            {"keyword": "Global Innovation", "reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ìƒ˜í”Œ ë°ì´í„°", "count": 20, "rank": 2},
            {"keyword": "Digital Future", "reason": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ìƒ˜í”Œ ë°ì´í„°", "count": 15, "rank": 3}
        ]

async def extract_keywords_with_gpt4o(articles):
    """Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    try:
        articles_text = "\n".join([
            f"ì œëª©: {article['title']}\në‚´ìš©: {article['content'][:200]}..."
            for article in articles[:50]
        ])
        
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ê³  ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ê¸°ì‚¬ ë‚´ìš©:
{articles_text}

ìš”êµ¬ì‚¬í•­:
1. ê¸°ì—…ëª…ì€ ì œì™¸í•˜ê³  ê¸°ìˆ /ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
2. ê¸°ì‚¬ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
3. ì‘ë‹µ í˜•ì‹: í‚¤ì›Œë“œ1:ë¹ˆë„1, í‚¤ì›Œë“œ2:ë¹ˆë„2 (ì½¤ë§ˆ êµ¬ë¶„)
4. ë¹ˆë„ëŠ” 5-25 ë²”ìœ„
5. ìµœì†Œ 5ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ

ì£¼ìš” í‚¤ì›Œë“œ:
"""
        
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.2
        )
        
        keywords_text = response.choices[0].message.content or ""
        logger.info(f"GPT-4o ì‘ë‹µ: {keywords_text}")
        
        keywords = []
        for item in keywords_text.split(','):
            if ':' in item:
                parts = item.strip().split(':', 1)
                keyword = parts[0].strip()
                try:
                    count = int(parts[1].strip())
                    keywords.append({"keyword": keyword, "count": count})
                except:
                    keywords.append({"keyword": keyword, "count": 10})
        
        if not keywords:
            logger.warning("âš ï¸ GPT-4oì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "IT", "count": 20},
                {"keyword": "ê¸°ìˆ ", "count": 18},
                {"keyword": "ë””ì§€í„¸", "count": 15},
                {"keyword": "ì •ë³´", "count": 12},
                {"keyword": "ì‹œìŠ¤í…œ", "count": 10}
            ]
        
        keywords.sort(key=lambda x: x['count'], reverse=True)
        
        logger.info(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        return keywords
        
    except Exception as e:
        logger.error(f"âŒ GPT-4o ì˜¤ë¥˜: {e}")
        return []

def extract_keyword_and_industry(question: str) -> Dict[str, Any]:
    """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œì™€ ì‚°ì—… ë¶„ë¥˜, ê·¸ë¦¬ê³  ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì„ ì • ì´ìœ ë¥¼ ì¶”ì¶œ"""
    
    industry_keywords = {
        "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ì¼ìë¦¬", "ë³µì§€", "ì •ì±…", "ì œë„", "ì‹œë¯¼", "ê³µê³µ"],
        "ê²½ì œ": ["ê²½ì œ", "ì‹œì¥", "íˆ¬ì", "ê¸ˆìœµ", "ì£¼ê°€", "ë¹„ìš©", "ìˆ˜ìµ", "ë§¤ì¶œ", "ê¸°ì—…"],
        "IT/ê³¼í•™": ["ê¸°ìˆ ", "ê°œë°œ", "í˜ì‹ ", "ì—°êµ¬", "ê³¼í•™", "IT", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´", "í”Œë«í¼", "ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "í´ë¼ìš°ë“œ", "ë©”íƒ€ë²„ìŠ¤", "ë¸”ë¡ì²´ì¸", "AI", "ChatGPT", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ë¡œë´‡", "ë°ì´í„° ê³¼í•™"],
        "ìƒí™œ/ë¬¸í™”": ["ìƒí™œ", "ë¬¸í™”", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì†Œë¹„", "íŠ¸ë Œë“œ", "ì¼ìƒ", "ì—¬ê°€", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "ì˜í™”", "ìŒì•…", "ê²Œì„"],
        "ì„¸ê³„": ["ê¸€ë¡œë²Œ", "êµ­ì œ", "ì„¸ê³„", "í•´ì™¸", "ìˆ˜ì¶œ", "í˜‘ë ¥", "ê²½ìŸ", "í‘œì¤€"]
    }
    
    question_lower = question.lower()
    
    detected_industry = None
    for industry, keywords in industry_keywords.items():
        if any(keyword in question_lower for keyword in keywords):
            detected_industry = industry
            break
    
    current_keywords = get_current_weekly_keywords()
    detected_keyword = None
    
    detected_keyword_reason = None

    for keyword in current_keywords:
        if keyword in question:
            detected_keyword = keyword
            match_reason = re.search(f"(ìµœê·¼|ìƒˆë¡œìš´|ë°œì „|ì¦ê°€|í•˜ë½|ì¸í•´|ë”°ë¼|ê´€ë ¨í•˜ì—¬|ëŒ€í•´)\s*(?:{re.escape(keyword)})\s*(.*)", question)
            if match_reason:
                context_word = match_reason.group(1).strip()
                trailing_text = match_reason.group(2).strip()
                if trailing_text:
                    detected_keyword_reason = f"{context_word} {trailing_text}"[:50].strip()
                else:
                    detected_keyword_reason = f"{context_word} ì–¸ê¸‰"
            else:
                detected_keyword_reason = "ì§ˆë¬¸ ë‚´ ëª…ì‹œëœ ì´ìœ  ì—†ìŒ"
            break
        elif keyword.lower() in question_lower:
            detected_keyword = keyword
            detected_keyword_reason = "ì§ˆë¬¸ ë‚´ ê°„ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰ë¨"
            break

    question_type = "general"

    comparison_keywords = []
    if "vs" in question_lower or "ë¹„êµ" in question_lower or "ì°¨ì´" in question_lower:
        question_type = "comparison"
        comparison_keywords = [kw for kw in current_keywords if kw.lower() in question_lower]
        if not comparison_keywords and detected_keyword:
            comparison_keywords = [detected_keyword]
        
        if not comparison_keywords:
            match = re.search(r'(.+?)\s*(?:ì™€|ì™€ë„)\s*(.+?)\s*ë¹„êµ', question_lower)
            if match:
                comparison_keywords = [match.group(1).strip(), match.group(2).strip()]
            else:
                comparison_keywords = current_keywords[:2]

    elif detected_industry and detected_keyword:
        question_type = "industry_analysis"
    elif detected_keyword:
        question_type = "keyword_trend"

    return {
        "type": question_type,
        "keyword": detected_keyword,
        "keywords": comparison_keywords,
        "industry": detected_industry or "ì‚¬íšŒ",
        "reason": detected_keyword_reason
    }

def get_current_weekly_keywords():
    """í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        return ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ìˆ í˜ì‹ "]
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"]

async def generate_industry_based_answer(question, keyword, industry, current_keywords, reason: Optional[str] = None, ncs_summary_text: Optional[str] = None):
    """
    ì‚°ì—…ë³„/ì§ë¬´ë³„ ê´€ì  ë¶„ì„ ê¸°ë°˜ ë‹µë³€ ìƒì„± (ê¸ì •ì  ì‹œê°, ì¹´í…Œê³ ë¦¬ë³„)
    """
    try:
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€: 'ê¸€ë¡œë²Œ Top ì»¨ì„¤íŒ… íŒì˜ ìˆ˜ì„ ì„±ì¥ ì „ëµê°€' í˜ë¥´ì†Œë‚˜ ì„¤ì •
        system_message_content = (
            "ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ Top ì»¨ì„¤íŒ… íŒì˜ ìˆ˜ì„ ì„±ì¥ ì „ëµê°€(Senior Growth Strategist)ì…ë‹ˆë‹¤. "
            "ë‹¹ì‹ ì˜ ë¶„ì„ì€ í•­ìƒ ìµœì‹  ì‹œì¥ ë°ì´í„°ì— ê¸°ë°˜í•˜ë©°, ìµœëŒ€ì˜ ì„±ì¥ ê¸°íšŒë¥¼ ë°œêµ´í•˜ê³  ìµœì ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. "
            "ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í—¤ë”(#)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ìƒì„±í•´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ë¶€ê°€ì ì¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤."
        )

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: 'ê³¼ì—… ëª©í‘œ', 'ë¶„ì„ í”„ë ˆì„ì›Œí¬', 'ë‹µë³€ í˜•ì‹'ì„ í¬í•¨
        keyword_str = keyword # ë‹¨ì¼ í‚¤ì›Œë“œ

        reason_text = ""
        if reason:
            reason_text = f"ì´ ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œì¸ '{keyword_str}'ì€(ëŠ”) '{reason}'ì´ë¼ëŠ” êµ¬ì²´ì ì¸ ì´ìœ ë¡œ í˜„ì¬ ê°€ì¥ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë§¥ë½ì„ ê¸ì •ì  ê¸°íšŒ ë¶„ì„ì— ê¹Šì´ í™œìš©í•´ì£¼ì„¸ìš”."

        # NCS ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ê³¼ì—… ëª©í‘œì— ì§ì ‘ ì‚½ì…
        ncs_specific_context = ""
        if ncs_summary_text:
            # NCS ìš”ì•½ì´ ìˆë‹¤ë©´, ì´ ì •ë³´ë¥¼ í´ë¼ì´ì–¸íŠ¸ì˜ ì§ë¬´/ì‚°ì—… ë°°ê²½ìœ¼ë¡œ ì œì‹œ
            ncs_specific_context = f"""
            **í´ë¼ì´ì–¸íŠ¸ì˜ ì§ë¬´/ì‚°ì—… ë°°ê²½:**
            {ncs_summary_text}
            ì´ëŸ¬í•œ ë°°ê²½ì„ ê°€ì§„ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ '{keyword_str}' íŠ¸ë Œë“œì— ë‚´ì¬ëœ,
            """
        else:
            # NCS ìš”ì•½ì´ ì—†ë‹¤ë©´, ê¸°ì¡´ industry ì¸ìë¥¼ í™œìš© (ì—†ì„ ê²½ìš° 'ì‚°ì—… ì „ë°˜'ìœ¼ë¡œ ëŒ€ì²´)
            ncs_specific_context = f"'{industry if industry else 'ì‚°ì—… ì „ë°˜'}' ê´€ì ì—ì„œ '{keyword_str}' íŠ¸ë Œë“œì— ë‚´ì¬ëœ,"
        
        prompt = f"""
### ê³¼ì—… ëª©í‘œ
{ncs_specific_context} **ì ì¬ì ì´ê³  ê°„ê³¼í•˜ê¸° ì‰¬ìš´ ê¸ì •ì  ê¸°íšŒ ìš”ì†Œë¥¼ ë‹¤ê°ì ìœ¼ë¡œ ë¶„ì„**í•˜ì—¬ ë³´ê³ í•´ì£¼ì‹­ì‹œì˜¤. ë¶„ì„ì€ ë°˜ë“œì‹œ **ê°ê´€ì ì´ê³  í˜„ì‹¤ì ì¸ ìµœì ì˜ ì‹œë‚˜ë¦¬ì˜¤**ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

{reason_text}

### ë¶„ì„ í”„ë ˆì„ì›Œí¬ ë° í•„ìˆ˜ í¬í•¨ ìš”ì†Œ (Analysis Framework)
ì•„ë˜ ì œì‹œëœ 4ê°€ì§€ ê¸ì •ì  ê¸°íšŒ ì¹´í…Œê³ ë¦¬ë³„ë¡œ í•µì‹¬ ë‚´ìš©, ë°œìƒ ê°€ëŠ¥í•œ ê¸ì •ì  ì‹œë‚˜ë¦¬ì˜¤, ê·¸ë¦¬ê³  ì„±ê³¼ ì§€í‘œë¥¼ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. ê° í•­ëª©ë‹¹ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•˜ë©°, ì „ì²´ ë‹µë³€ì€ ìµœëŒ€ 400ì(ê³µë°± í¬í•¨)ë¥¼ ë„˜ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”.

1.  **ê¸°ìˆ ì /í˜ì‹ ì  ê¸°íšŒ (Technological/Innovation Opportunity):** ìƒˆë¡œìš´ ê¸°ìˆ  ë„ì…, ìƒì‚°ì„± í–¥ìƒ, ì—°êµ¬ ê°œë°œ ê°€ì†í™”, í˜ì‹ ì ì¸ ì†”ë£¨ì…˜ ì°½ì¶œ.
2.  **ì‹œì¥/ê²½ì œì  ê¸°íšŒ (Market/Economic Opportunity):** ì‹ ê·œ ì‹œì¥ ì°½ì¶œ, ìˆ˜ìµ ì¦ëŒ€, ë¹„ìš© ì ˆê°, íˆ¬ì ìœ ì¹˜, ê²½ìŸ ìš°ìœ„ í™•ë³´.
3.  **ì‚¬íšŒì /ê°€ì¹˜ì  ê¸°íšŒ (Social/Value Opportunity):** ì‚¬íšŒ ë¬¸ì œ í•´ê²° ê¸°ì—¬, ì‚¶ì˜ ì§ˆ í–¥ìƒ, ìœ¤ë¦¬ì  ê°€ì¹˜ ì¦ì§„, ê¸ì •ì  ë¸Œëœë“œ ì´ë¯¸ì§€ êµ¬ì¶•.
4.  **ì •ì±…ì /ê·œì œì  ê¸°íšŒ (Policy/Regulatory Opportunity):** ì •ë¶€ ì§€ì› ë° ì •ì±…ì  ìš°ëŒ€, ê·œì œ ì™„í™”, ì‚°ì—… í‘œì¤€ ì„ ë„, êµ­ì œ í˜‘ë ¥ ì¦ëŒ€.

### ë‹µë³€ í˜•ì‹
Â· **ê¸°ìˆ ì /í˜ì‹ ì  ê¸°íšŒ:** í•µì‹¬ ê¸°íšŒ ìš”ì•½. ê¸ì •ì  ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ì„±ê³¼ ì§€í‘œ: [ì§€í‘œ].
Â· **ì‹œì¥/ê²½ì œì  ê¸°íšŒ:** í•µì‹¬ ê¸°íšŒ ìš”ì•½. ê¸ì •ì  ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ì„±ê³¼ ì§€í‘œ: [ì§€í‘œ].
Â· **ì‚¬íšŒì /ê°€ì¹˜ì  ê¸°íšŒ:** í•µì‹¬ ê¸°íšŒ ìš”ì•½. ê¸ì •ì  ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ì„±ê³¼ ì§€í‘œ: [ì§€í‘œ].
Â· **ì •ì±…ì /ê·œì œì  ê¸°íšŒ:** í•µì‹¬ ê¸°íšŒ ìš”ì•½. ê¸ì •ì  ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ì„±ê³¼ ì§€í‘œ: [ì§€í‘œ].
Â· **'ì§ë¬´ë³„ ê¸°íšŒ í™œìš© ì „ëµ:** [2~3ë¬¸ì¥ìœ¼ë¡œ ì§ë¬´ ë‹´ë‹¹ìë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆ ì œì‹œ. ì´ ì „ëµì€ í•´ë‹¹ ì§ë¬´ì˜ ì „ë¬¸ì„±ê³¼ ì—°ê´€ì„±ì„ ëª…í™•íˆ ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤.]
**ìµœì¢… ë¶„ì„ ìš”ì•½:** [2~3ë¬¸ì¥ìœ¼ë¡œ ì „ì²´ ê¸ì •ì  ë¶„ì„ì˜ í•µì‹¬ ìš”ì•½ ë° ê¸°íšŒ í™œìš© ë°©ì•ˆ í¬í•¨].
"""

        completion = await asyncio.to_thread( # await asyncio.to_thread ì¶”ê°€
            openai_client.chat.completions.create,
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": system_message_content},
                {"role": "user", "content": prompt}
            ],
            max_tokens=600, # ë„‰ë„‰í•˜ê²Œ ì„¤ì • (í•œê¸€ 1ì = ì•½ 2~3í† í°)
            temperature=0.3
        )

        response_content = completion.choices[0].message.content
        logger.info(f"âœ… ê¸ì •ì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì„±ê³µ. ê¸¸ì´: {len(response_content)}")
        return response_content

    except Exception as e:
        logger.error(f"âŒ ê¸ì •ì  ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ê¸ì •ì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


async def generate_keyword_trend_answer(question, keyword):
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ë‹µë³€ ìƒì„±"""
    try:
        prompt = f"""
ì§ˆë¬¸: {question}
í‚¤ì›Œë“œ: {keyword}

'{keyword}'ì˜ ìµœê·¼ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ ë‚´ìš©:
Â· ìµœê·¼ '{keyword}' ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ ë™í–¥
Â· ì‹œê°„ì  ë³€í™”ì™€ ë°œì „ ë°©í–¥
Â· í–¥í›„ ì „ë§ê³¼ ê´€ì‹¬ í¬ì¸íŠ¸

ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ íŠ¸ë Œë“œë¥¼ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ '{keyword}' ë¶„ì•¼ì˜ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ë™í–¥ê³¼ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=16384
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{keyword}' íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

async def generate_comparison_answer(question, keywords, perspective_role: Optional[str] = None, reason: Optional[str] = None):
    """
    ì£¼ì–´ì§„ í‚¤ì›Œë“œë“¤ì„ íŠ¹ì • ì§ë¬´/ì‚°ì—… ê´€ì ì—ì„œ ë¹„íŒì /íšŒì˜ì ì¸ ì‹œê°ìœ¼ë¡œ ë¶„ì„í•˜ì—¬
    í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ë¦¬ìŠ¤í¬ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (ê¸ì •ì  ë¶„ì„ ê²°ê³¼ í™œìš©)
    """
    try:
        # 1. ê¸ì •ì  ë¶„ì„ ê²°ê³¼ ë°›ì•„ì˜¤ê¸°
        positive_analysis_context = ""
        
        # NCS ì§ë¬´ ìš”ì•½ ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”
        ncs_job_summary_for_positive_analysis = None 

        if keywords: # í‚¤ì›Œë“œê°€ ìˆì„ ë•Œë§Œ ê¸ì •ì  ë¶„ì„ í˜¸ì¶œ
            # ì—¬ê¸°ì—ì„œ get_job_industry_summaryë¥¼ í˜¸ì¶œí•˜ì—¬ NCS ì§ë¬´/ì‚°ì—… ìš”ì•½ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            # ì§ë¬´ëª…ì¸ perspective_roleì„ queryë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            try:
                ncs_job_summary_for_positive_analysis = await get_job_industry_summary(perspective_role)
                logger.info(f"NCS Job Summary fetched for positive analysis: {ncs_job_summary_for_positive_analysis[:100]}...")
            except Exception as e:
                logger.error(f"Error fetching NCS job summary for positive analysis: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬
                ncs_job_summary_for_positive_analysis = "NCS ì§ë¬´ ì •ë³´ ìš”ì•½ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            positive_analysis_context = await generate_industry_based_answer(
                question=question,
                keyword=keywords[0],
                industry=perspective_role, # industryëŠ” ê¸°ì¡´ëŒ€ë¡œ perspective_role ì „ë‹¬
                current_keywords=[],
                reason=reason,
                ncs_summary_text=ncs_job_summary_for_positive_analysis # ì—¬ê¸°ì— NCS ìš”ì•½ ê²°ê³¼ ì „ë‹¬
            )


        # ì‹œìŠ¤í…œ ë©”ì‹œì§€: 'ê¸€ë¡œë²Œ Top ì»¨ì„¤íŒ… íŒì˜ ìˆ˜ì„ ë¦¬ìŠ¤í¬ ë¶„ì„ê°€' í˜ë¥´ì†Œë‚˜ ì„¤ì •
        system_message_content = (
            "ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ Top ì»¨ì„¤íŒ… íŒì˜ ìˆ˜ì„ ë¦¬ìŠ¤í¬ ë¶„ì„ê°€(Senior Risk Analyst)ì…ë‹ˆë‹¤. "
            "ë‹¹ì‹ ì˜ ë¶„ì„ì€ í•­ìƒ ìµœì‹  ë°ì´í„°ì— ê¸°ë°˜í•˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ ê°„ê³¼ë˜ê±°ë‚˜ ë‚™ê´€ì ìœ¼ë¡œë§Œ í•´ì„ë˜ëŠ” ë©´ ì´ë©´ì— ìˆ¨ê²¨ì§„ ë¦¬ìŠ¤í¬ë¥¼ ë°œêµ´í•˜ê³ , "
            "ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ê¹Œì§€ ëƒ‰ì² í•˜ê²Œ ê³ ë ¤í•©ë‹ˆë‹¤. "
            "ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í—¤ë”(#)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ìƒì„±í•´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ë¶€ê°€ì ì¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤."
        )

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: 'ê³¼ì—… ëª©í‘œ', 'ë¶„ì„ í”„ë ˆì„ì›Œí¬', 'ë‹µë³€ í˜•ì‹'ì„ í¬í•¨
        keyword_str = ", ".join(keywords)

        reason_text = ""
        if reason and keywords:
            reason_text = f"ì´ ë¶„ì„ ëŒ€ìƒ í‚¤ì›Œë“œ ì¤‘ '{keywords[0]}'ì€(ëŠ”) '{reason}'ì´ë¼ëŠ” êµ¬ì²´ì ì¸ ì´ìœ ë¡œ í˜„ì¬ ê°€ì¥ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë§¥ë½ì„ ë¹„íŒì  ë¦¬ìŠ¤í¬ ë¶„ì„ì— ê¹Šì´ í™œìš©í•´ì£¼ì„¸ìš”."

        # NCS ìš”ì•½ ì •ë³´ë¥¼ ê³¼ì—… ëª©í‘œì— ì§ì ‘ ì‚½ì…
        ncs_specific_client_context = ""
        if ncs_job_summary_for_positive_analysis:
            # NCS ìš”ì•½ì´ ìˆë‹¤ë©´, ì´ ì •ë³´ë¥¼ í´ë¼ì´ì–¸íŠ¸ì˜ ì§ë¬´/ì‚°ì—… ë°°ê²½ìœ¼ë¡œ ì œì‹œ
            ncs_specific_client_context = f"""
            **í´ë¼ì´ì–¸íŠ¸ì˜ ì§ë¬´/ì‚°ì—… ë°°ê²½:**
            {ncs_job_summary_for_positive_analysis}
            ì´ëŸ¬í•œ ë°°ê²½ì„ ê°€ì§„ í´ë¼ì´ì–¸íŠ¸ê°€
            """
        else:
            # NCS ìš”ì•½ì´ ì—†ë‹¤ë©´, ê¸°ì¡´ëŒ€ë¡œ 'ì‚°ì—… ì „ë°˜' í´ë¼ì´ì–¸íŠ¸ë¡œ ëŒ€ì²´
            ncs_specific_client_context = f"'{perspective_role if perspective_role else 'ì‚°ì—… ì „ë°˜'}' ì‚°ì—…ì˜ í´ë¼ì´ì–¸íŠ¸ê°€"


        prompt = f"""
### ê³¼ì—… ëª©í‘œ
{ncs_specific_client_context} '{keyword_str}' íŠ¸ë Œë“œì— ë‚´ì¬ëœ, **ê°„ê³¼í•˜ê¸° ì‰¬ìš´ ë¹„íŒì /íšŒì˜ì  ê´€ì ì˜ ì ì¬ì  ë¦¬ìŠ¤í¬**ë¥¼ ë‹¤ê°ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë³´ê³ í•´ì£¼ì‹­ì‹œì˜¤. ë¶„ì„ì€ ë°˜ë“œì‹œ **ê°ê´€ì ì´ê³  í˜„ì‹¤ì ì¸ ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤**ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

{reason_text}

--- ì°¸ê³ ìš© ê¸ì •ì  ë¶„ì„ ë³´ê³ ì„œ (ì´ ë‚´ìš©ì— ëŒ€í•œ ë¹„íŒì  ê´€ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”) ---
{positive_analysis_context}
---

### ë¶„ì„ ì§€ì¹¨ (í•„ìˆ˜ í¬í•¨ ìš”ì†Œ)
ìœ„ì—ì„œ ì œê³µëœ ê¸ì •ì  ë¶„ì„ ë‚´ìš©ì„ ì¶©ë¶„íˆ ì°¸ê³ í•˜ì—¬, ê·¸ ë‚´ìš©ì— ëŒ€í•œ ë°˜ëŒ€ ì˜ê²¬, ì ì¬ì  ë¬¸ì œì , í•œê³„ì , ë˜ëŠ” ë¶€ì •ì ì¸ ì˜í–¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
ì•„ë˜ ì œì‹œëœ 4ê°€ì§€ ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ í•µì‹¬ ë‚´ìš©, ë°œìƒ ê°€ëŠ¥í•œ ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤, ê·¸ë¦¬ê³  ëª¨ë‹ˆí„°ë§ ì§€í‘œë¥¼ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. ê° í•­ëª©ë‹¹ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•˜ë©°, ì „ì²´ ë‹µë³€ì€ ìµœëŒ€ 400ì(ê³µë°± í¬í•¨)ë¥¼ ë„˜ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”.

1.  **ê¸°ìˆ ì /ìš´ì˜ì  ë¦¬ìŠ¤í¬ (Technological/Operational Risk):** ê¸°ìˆ  í•œê³„, í™•ì¥ì„± ë¬¸ì œ, êµ¬í˜„ì˜ ë³µì¡ì„±, ì‹œìŠ¤í…œ ì˜¤ë¥˜ ê°€ëŠ¥ì„±, ì˜ˆìƒì¹˜ ëª»í•œ ìš´ì˜ ë¬¸ì œ.
2.  **ì‹œì¥/ê²½ì œì  ë¦¬ìŠ¤í¬ (Market/Economic Risk):** ì‹œì¥ ê³¼ì—´ë¡œ ì¸í•œ ë²„ë¸”, ìˆ˜ìµ ëª¨ë¸ì˜ ì·¨ì•½ì„± ë° ì§€ì† ë¶ˆê°€ëŠ¥ì„±, ì˜ˆìƒ ë°–ì˜ ë†’ì€ ë¹„ìš© ë˜ëŠ” íˆ¬ì íšŒìˆ˜ ì§€ì—°, ê²½ìŸ ì‹¬í™”.
3.  **ì‚¬íšŒì /ìœ¤ë¦¬ì  ë¦¬ìŠ¤í¬ (Social/Ethical Risk):** ì‚¬íšŒì  ë¶ˆí‰ë“± ì‹¬í™”, ì¼ìë¦¬ ê°ì†Œ ë° ì‚¬íšŒì  í˜¼ë€, ê°œì¸ì •ë³´ ì¹¨í•´ ë° ë°ì´í„° ì˜¤ìš©, ìœ¤ë¦¬ì  ë”œë ˆë§ˆ (ì˜ˆ: ì±…ì„ ì†Œì¬), ë¶€ì •ì ì¸ ë¸Œëœë“œ ì´ë¯¸ì§€ í˜•ì„±.
4.  **ê·œì œ/ë²•ë¥ ì  ë¦¬ìŠ¤í¬ (Regulatory/Legal Risk):** ì˜ˆìƒì¹˜ ëª»í•œ ì •ë¶€ ê·œì œ ë„ì… ë˜ëŠ” ê°•í™”, ê¸°ì¡´ ë²•ê·œì™€ì˜ ì¶©ëŒ, ì§€ì  ì¬ì‚°ê¶Œ ë¶„ìŸ ê°€ëŠ¥ì„±, êµ­ì œì  ê·œì œ ì¥ë²½.

### ë‹µë³€ í˜•ì‹
Â· **ê¸°ìˆ ì /ìš´ì˜ì  ë¦¬ìŠ¤í¬:** í•µì‹¬ ë¦¬ìŠ¤í¬ ìš”ì•½. ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ëª¨ë‹ˆí„°ë§ ì§€í‘œ: [ì§€í‘œ].
Â· **ì‹œì¥/ê²½ì œì  ë¦¬ìŠ¤í¬:** í•µì‹¬ ë¦¬ìŠ¤í¬ ìš”ì•½. ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ëª¨ë‹ˆí„°ë§ ì§€í‘œ: [ì§€í‘œ].
Â· **ì‚¬íšŒì /ìœ¤ë¦¬ì  ë¦¬ìŠ¤í¬:** í•µì‹¬ ë¦¬ìŠ¤í¬ ìš”ì•½. ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ëª¨ë‹ˆí„°ë§ ì§€í‘œ: [ì§€í‘œ].
Â· **ê·œì œ/ë²•ë¥ ì  ë¦¬ìŠ¤í¬:** í•µì‹¬ ë¦¬ìŠ¤í¬ ìš”ì•½. ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ: [ì˜ˆì‹œ ë‚´ìš©]. ëª¨ë‹ˆí„°ë§ ì§€í‘œ: [ì§€í‘œ].
Â· **'ì§ë¬´ë³„ ê¸°íšŒ í™œìš© ì „ëµ:** [2~3ë¬¸ì¥ìœ¼ë¡œ ì§ë¬´ ë‹´ë‹¹ìë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆ ì œì‹œ. ì´ ì „ëµì€ í•´ë‹¹ ì§ë¬´ì˜ ì „ë¬¸ì„±ê³¼ ì—°ê´€ì„±ì„ ëª…í™•íˆ ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤.]

**ìµœì¢… ë¶„ì„ ìš”ì•½:** [2~3ë¬¸ì¥ìœ¼ë¡œ ì „ì²´ ë¦¬ìŠ¤í¬ ë¶„ì„ì˜ í•µì‹¬ ìš”ì•½ ë° ê¶Œê³  ì‚¬í•­ í¬í•¨].
"""

        completion = await asyncio.to_thread(
            openai_client.chat.completions.create,
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": system_message_content},
                {"role": "user", "content": prompt}
            ],
            max_tokens=600,
            temperature=0.3
        )

        response_content = completion.choices[0].message.content
        logger.info(f"âœ… ë¹„íŒì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì„±ê³µ. ê¸¸ì´: {len(response_content)}")
        return response_content

    except Exception as e:
        logger.error(f"âŒ ë¹„íŒì  ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë¹„íŒì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


async def generate_contextual_answer(question, current_keywords):
    """í˜„ì¬ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¼ë°˜ ë‹µë³€ ìƒì„±"""
    try:
        keywords_context = f"í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(current_keywords)}"
        
        prompt = f"""
ì§ˆë¬¸: {question}
{keywords_context}

í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œë“¤ê³¼ ì—°ê´€ì§€ì–´ ë‹µë³€í•˜ë˜, ì§ˆë¬¸ì˜ ë§¥ë½ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ê³ ë ¤ì‚¬í•­:
Â· í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œì™€ì˜ ì—°ê´€ì„± ì–¸ê¸‰
Â· êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ë°ì´í„° í™œìš©  
Â· ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ì„¤ëª…
Â· ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ

ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ({', '.join(current_keywords)})ë¥¼ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=16384
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def analyze_keyword_dynamically(request: dict):
    """ë™ì  í‚¤ì›Œë“œ ë¶„ì„ - í´ë¦­ëœ í‚¤ì›Œë“œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„"""
    keyword = request.get("keyword", "")
    
    if not keyword:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
    
    try:
        prompt = f"""
í‚¤ì›Œë“œ: '{keyword}'

ë‹¤ìŒ 5ê°€ì§€ ê´€ì ì—ì„œ ì´ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
Â· ì‚¬íšŒì  ì˜í–¥
Â· ê²½ì œì  ì¸¡ë©´  
Â· ê¸°ìˆ ì  ê´€ì 
Â· ë¬¸í™”ì  ì˜ë¯¸
Â· ë¯¸ë˜ ì „ë§

ê° ê´€ì ë³„ë¡œ 2-3ë¬¸ì¥ì”© ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500
        )
        
        return {
            "keyword": keyword,
            "analysis": completion.choices[0].message.content
        }
        
    except Exception as e:
        return {
            "keyword": keyword,
            "analysis": f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }

async def generate_weekly_insight(keywords_data):
    """ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ê°œì„ ëœ êµ¬ì¡°)"""
    try:
        domestic_details = [f"{k['keyword']} ({k['count']}ê±´)" for k in keywords_data["domestic_keywords"]]
        global_details = [f"{k['keyword']} ({k['count']}ê±´)" for k in keywords_data["global_keywords"]]

        prompt = f"""
        AI ë‰´ìŠ¤ êµ¬ë…ìë“¤ì„ ìœ„í•œ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì „ë¬¸ì ì´ë©´ì„œë„ ì½ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ğŸ“Š ì´ë²ˆ ì£¼ ë¶„ì„ ë°ì´í„°:
        Â· ë¶„ì„ ê¸°ê°„: {keywords_data["period"]}
        Â· êµ­ë‚´ TOP í‚¤ì›Œë“œ: {", ".join(domestic_details)}
        Â· í•´ì™¸ TOP í‚¤ì›Œë“œ: {", ".join(global_details)}

        ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

        ï¿½ ì´ë²ˆ ì£¼ í•« í‚¤ì›Œë“œ

        ğŸ“ˆ êµ­ë‚´ ê¸°ìˆ  ë™í–¥
        Â· ê°€ì¥ ì£¼ëª©ë°›ì€ í‚¤ì›Œë“œì™€ ê·¸ ë°°ê²½
        Â· ê´€ë ¨ ì‚°ì—…/ê¸°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
        Â· ì‹¤ë¬´ì§„ì´ ì•Œì•„ì•¼ í•  í¬ì¸íŠ¸

        ğŸŒ ê¸€ë¡œë²Œ ê¸°ìˆ  íŠ¸ë Œë“œ
        Â· í•´ì™¸ì—ì„œ í™”ì œê°€ ëœ ê¸°ìˆ  ì´ìŠˆ
        Â· êµ­ë‚´ ì‹œì¥ì— ë¯¸ì¹  ì˜í–¥ ì˜ˆì¸¡
        Â· ê¸€ë¡œë²Œ vs êµ­ë‚´ íŠ¸ë Œë“œ ë¹„êµ

        ğŸ’¡ ë‹¤ìŒ ì£¼ ì „ë§ ë° ì‹¤í–‰ í¬ì¸íŠ¸
        Â· ì£¼ëª©í•´ì•¼ í•  ê¸°ìˆ /í‚¤ì›Œë“œ
        Â· ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒë‚˜ ìœ„í—˜ ìš”ì†Œ
        Â· ì‹¤ë¬´ì§„ì„ ìœ„í•œ ì•¡ì…˜ ì•„ì´í…œ

        ğŸ¯ í•œ ì¤„ ìš”ì•½
        Â· ì´ë²ˆ ì£¼ ê°€ì¥ ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ

        âš ï¸ ì¤‘ìš”: ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°í˜¸ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì´ëª¨ì§€ì™€ ì¤‘ê°„ì ë§Œ ì‚¬ìš©í•´ì„œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
        ì „ì²´ ë¶„ëŸ‰: 1000ì ë‚´ì™¸ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """

        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ AI ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ë¥¼ êµ¬ë…ìë“¤ì—ê²Œ ì œê³µí•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€. ëŒ€ì‹  ì´ëª¨ì§€ì™€ ì¤‘ê°„ì (Â·)ë§Œ ì‚¬ìš©í•˜ì—¬ êµ¬ë¶„í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.3
        )

        return completion.choices[0].message.content
    except Exception as e:
        logger.error(f"âŒ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return """
ğŸ” ì´ë²ˆ ì£¼ AI ë‰´ìŠ¤ í•˜ì´ë¼ì´íŠ¸

ï¿½ êµ­ë‚´ ê¸°ìˆ  íŠ¸ë Œë“œ
â€¢ ì¸ê³µì§€ëŠ¥ê³¼ ë°˜ë„ì²´ ë¶„ì•¼ì˜ ì§€ì†ì ì¸ ì„±ì¥
â€¢ ê¸°ìˆ  í˜ì‹ ê³¼ ì‚°ì—… ë³€í™” ê°€ì†í™”
â€¢ ì •ë¶€ ì •ì±…ê³¼ ê¸°ì—… íˆ¬ì í™•ëŒ€

ï¿½ ê¸€ë¡œë²Œ ê¸°ìˆ  ë™í–¥
â€¢ AI ê¸°ìˆ ì˜ ì „ ì‚°ì—… í™•ì‚°
â€¢ ê¸€ë¡œë²Œ ê¸°ìˆ  ê²½ìŸ ì‹¬í™”
â€¢ ì‹ ê¸°ìˆ  ë„ì…ê³¼ í™œìš© ì‚¬ë¡€ ì¦ê°€

ğŸ’¡ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸
ì´ë²ˆ ì£¼ëŠ” AIì™€ ë°˜ë„ì²´ ê¸°ìˆ ì´ ì£¼ìš” í™”ë‘ì˜€ìŠµë‹ˆë‹¤. êµ­ë‚´ì™¸ ëª¨ë‘ ê¸°ìˆ  í˜ì‹ ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆì–´ ê´€ë ¨ ì‚°ì—…ì˜ ì„±ì¥ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤.

ğŸ¯ ë‹¤ìŒ ì£¼ ì „ë§
â€¢ AI ê¸°ìˆ  ë°œì „ ì§€ì† ê´€ì°° í•„ìš”
â€¢ ê´€ë ¨ íˆ¬ì ê¸°íšŒ ëª¨ë‹ˆí„°ë§ ê¶Œì¥

ğŸ“§ News GPT v2 íŒ€ ë“œë¦¼
        """


async def get_gpt_commentary(trend_request):
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë Œë“œì— ëŒ€í•œ í•´ì„¤ ìƒì„±"""
    try:
        if not trend_request.headlines:
            return {"comment": "ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ì–´ íŠ¸ë Œë“œ í•´ì„¤ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status": "no_news"}

        prompt = (
            f"The following are recent news headlines related to the keyword '{trend_request.keyword}':\n"
            + "\n".join(f"- {title}" for title in trend_request.headlines)
            + "\n\nBased on these headlines, explain why people are likely searching for this keyword.\n\n"
            "ğŸ§  Output format:\n"
            "1. A one-sentence summary of the main reason for the keyword being searched.\n"
            "2. A short paragraph elaborating on the reason, based on the headlines.\n\n"
            "ğŸ“Œ Rules:\n"
            "- All output must be in Korean.\n"
            "- Do NOT mention 'Google Trends' anywhere in the answer.\n"
            "- If the headlines are insufficient to determine a clear reason, you may use general web knowledge to supplement your explanation.\n"
            "- In that case, please add the following sentence at the end of the paragraph:\n"
            "  â€» ê¸°ì‚¬ ì œëª©ë§Œìœ¼ë¡œëŠ” ìœ ì˜ë¯¸í•œ ê²€ìƒ‰ ì›ì¸ì„ ì°¾ê¸° ì–´ë ¤ì›Œ, ì¶”ê°€ ì •ë³´ë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤."
        )
        response = openai_keyword_explainer_client.chat.completions.create(
            model=AZURE_OPENAI_KEYWORD_EXPLAINER_DEPLOYMENT,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=16384
        )

        logger.info(f"GPT Commentary Response: {response.choices[0].message.content}")
        
        return response.choices[0].message.content or "No commentary generated."
    except Exception as e:
        logger.error(f"Error generating GPT commentary: {e}", exc_info=True)
        return "Failed to generate commentary due to a server error." 
    


