import logging
import requests
from typing import List, Dict, Any

from utils.helpers import retry_on_exception, generate_article_id, calculate_relevance_score
from core.config import DEEPSEARCH_API_KEY, DEEPSEARCH_TECH_URL, DEEPSEARCH_GLOBAL_TECH_URL, DEEPSEARCH_GLOBAL_KEYWORD_URL, DEEPSEARCH_KEYWORD_URL

logger = logging.getLogger(__name__)

articles_cache = {}

@retry_on_exception(max_retries=1, delay=0.1, backoff=1.5, allowed_exceptions=(requests.RequestException,))
async def fetch_tech_articles(start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """DeepSearch Tech ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        base_url = DEEPSEARCH_TECH_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50
        }
        
        logger.info(f"ğŸš€ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        response = requests.get(base_url, params=params, timeout=15)
        logger.info(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        articles = []
        if "articles" in data:
            articles = data["articles"]
        elif "data" in data:
            articles = data["data"]
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]
                    else:
                        formatted_date = published_at[:10]
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "content": article.get("summary", "") or article.get("content", ""),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "url": article.get("url", "") or article.get("content_url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": article.get("source", ""),
                "category": "tech"
            }
            
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        logger.info(f"âœ… Tech ê¸°ì‚¬ {len(processed_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return processed_articles
        
    except Exception as e:
        logger.error(f"âŒ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

@retry_on_exception(max_retries=1, delay=0.1, backoff=1.5, allowed_exceptions=(requests.RequestException,))
async def fetch_global_tech_articles(start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """DeepSearch Global APIì—ì„œ í•´ì™¸ Tech ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        base_url = DEEPSEARCH_GLOBAL_TECH_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": "tech",
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50
        }
        
        logger.info(f"ğŸŒ í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        response = requests.get(base_url, params=params, timeout=15)
        logger.info(f"ğŸ“Š í•´ì™¸ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í•´ì™¸ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        articles = []
        if 'data' in data:
            articles = data['data']
        elif 'articles' in data:
            articles = data['articles']
        elif isinstance(data, list):
            articles = data
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í•´ì™¸ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]
                    else:
                        formatted_date = published_at[:10]
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "content": article.get("summary", "") or article.get("content", "ë‚´ìš© ì—†ìŒ"),
                "url": article.get("content_url", "") or article.get("url", ""),
                "date": formatted_date,
                "source": "í•´ì™¸",
                "category": "global_tech"
            }
            
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        logger.info(f"âœ… í•´ì™¸ Tech ê¸°ì‚¬ {len(processed_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return processed_articles
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
async def search_articles_by_keyword(keyword: str, start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ DeepSearchì—ì„œ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        base_url = DEEPSEARCH_KEYWORD_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": keyword,
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50
        }
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... URL: {base_url}")
        logger.info(f"ğŸ” íŒŒë¼ë¯¸í„°: {params}")
        response = requests.get(base_url, params=params, timeout=15)
        logger.info(f"ğŸ” ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í‚¤ì›Œë“œ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        articles = []
        if "articles" in data:
            articles = data["articles"]
        elif "data" in data:
            articles = data["data"]
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]
                    else:
                        formatted_date = published_at[:10]
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "content": article.get("summary", "") or article.get("content", ""),
                "url": article.get("url", "") or article.get("content_url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": article.get("source", ""),
                "keyword": keyword,
                "relevance_score": calculate_relevance_score(article, keyword)
            }
            
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        processed_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        logger.info(f"âœ… í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ {len(processed_articles)}ê°œ ê²€ìƒ‰ ì™„ë£Œ")
        return processed_articles[:15]
        
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
async def search_global_keyword_articles(keyword: str, start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ DeepSearch Global APIì—ì„œ í•´ì™¸ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        base_url = DEEPSEARCH_GLOBAL_KEYWORD_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": keyword,
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50
        }
        
        logger.info(f"ğŸŒ í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... URL: {base_url}")
        logger.info(f"ğŸŒ íŒŒë¼ë¯¸í„°: {params}")
        response = requests.get(base_url, params=params, timeout=15)
        logger.info(f"ğŸŒ ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        articles = []
        if 'data' in data:
            articles = data['data']
        elif 'articles' in data:
            articles = data['articles']
        elif isinstance(data, list):
            articles = data
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]
                    else:
                        formatted_date = published_at[:10]
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "content": article.get("summary", "") or article.get("content", ""),
                "url": article.get("content_url", "") or article.get("url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": "í•´ì™¸",
                "keyword": keyword,
                "region": "global",
                "relevance_score": calculate_relevance_score(article, keyword)
            }
            
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        processed_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        logger.info(f"âœ… í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ {len(processed_articles)}ê°œ ê²€ìƒ‰ ì™„ë£Œ")
        return processed_articles[:15]
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

async def collect_it_news_from_deepsearch(start_date: str, end_date: str):
    """DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    try:
        articles = []
        tech_keywords = ["IT", "ê¸°ìˆ ", "ì¸ê³µì§€ëŠ¥", "AI", "ë°˜ë„ì²´"]
        logger.info(f"ğŸ” DeepSearch APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... ({start_date} ~ {end_date})")
        
        for keyword in tech_keywords:
            try:
                base_url = DEEPSEARCH_KEYWORD_URL
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "keyword": keyword,
                    "date_from": start_date,
                    "date_to": end_date
                }
                response = requests.get(base_url, params=params, timeout=15)
                
                if response.status_code != 200:
                    logger.warning(f"    âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                    continue
                    
                data = response.json()
                
                if "data" in data:
                    articles_data = data["data"]
                elif "articles" in data:
                    articles_data = data["articles"]
                else:
                    logger.warning(f"    âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
                    continue
                    
                for item in articles_data:
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    title = item.get("title", "").strip()
                    content = (item.get("summary", "") or item.get("content", "")).strip()
                    if title and content:
                        articles.append({
                            "id": f"news_{keyword}_{hash(title)%100000}",
                            "title": title,
                            "content": content,
                            "date": article_date,
                            "source_url": item.get("content_url", "") or item.get("url", ""),
                            "keyword": keyword
                        })
                logger.info(f"    âœ… '{keyword}': {len(articles_data)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            except Exception as e:
                logger.warning(f"    âŒ '{keyword}' ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
        unique_articles = []
        seen_hashes = set()
        for article in articles:
            hash_key = hash((article["title"].lower(), article["content"][:100].lower()))
            if hash_key not in seen_hashes:
                seen_hashes.add(hash_key)
                unique_articles.append(article)
        logger.info(f"âœ… ì´ {len(unique_articles)}ê°œ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles[:30]
    except Exception as e:
        logger.error(f"âŒ DeepSearch API ì „ì²´ ì˜¤ë¥˜: {e}", exc_info=True)
        return [
            {
                "id": "sample_1",
                "title": "AI ê¸°ìˆ  ë°œì „ìœ¼ë¡œ IT ì—…ê³„ ë³€í™” ê°€ì†í™”",
                "content": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ IT ì—…ê³„ ì „ë°˜ì— ë³€í™”ê°€ ì¼ì–´ë‚˜ê³  ìˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•œ ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ë“¤ì´ ë“±ì¥í•˜ê³  ìˆìœ¼ë©°, ê¸°ì—…ë“¤ì€ ë””ì§€í„¸ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì„ ê°€ì†í™”í•˜ê³  ìˆë‹¤.",
                "date": start_date,
                "source_url": "https://example.com/ai-news",
                "keyword": "AI"
            },
            {
                "id": "sample_2",
                "title": "ë°˜ë„ì²´ ì‚°ì—… íšŒë³µ ì¡°ì§, ê¸€ë¡œë²Œ ê³µê¸‰ë§ ì•ˆì •í™”",
                "content": "ë°˜ë„ì²´ ì‚°ì—…ì´ íšŒë³µ ì¡°ì§ì„ ë³´ì´ë©° ê¸€ë¡œë²Œ ê³µê¸‰ë§ì´ ì•ˆì •í™”ë˜ê³  ìˆë‹¤. ì£¼ìš” ë°˜ë„ì²´ ê¸°ì—…ë“¤ì˜ ì‹¤ì ì´ ê°œì„ ë˜ê³  ìˆìœ¼ë©°, ìƒˆë¡œìš´ ê¸°ìˆ  ê°œë°œì— ëŒ€í•œ íˆ¬ìë„ ì¦ê°€í•˜ê³  ìˆë‹¤.",
                "date": start_date,
                "source_url": "https://example.com/semiconductor-news",
                "keyword": "ë°˜ë„ì²´"
            }
        ]

def get_original_url_by_id(article_id: str):
    """ê¸°ì‚¬ IDë¡œ ì›ë³¸ URLì„ ì°¾ìŠµë‹ˆë‹¤"""
    article = articles_cache.get(article_id)
    if article:
        return article.get("url")
    return None

@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
def deepsearch_api_request(url, params):
    """DeepSearch API ìš”ì²­ (ì¬ì‹œë„/ë¡œê¹… ì¼ê´€ì„±)"""
    logger.info(f"DeepSearch API ìš”ì²­: {url} | params: {params}")
    response = requests.get(url, params=params, timeout=15)
    logger.info(f"DeepSearch ì‘ë‹µ ì½”ë“œ: {response.status_code}")
    response.raise_for_status()
    return response.json() 