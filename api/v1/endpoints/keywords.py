import logging
from typing import List
from fastapi import APIRouter, Query, HTTPException
from fastapi.responses import JSONResponse

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from services.deepsearch_service import search_articles_by_keyword, search_global_keyword_articles, collect_it_news_from_deepsearch, fetch_tech_articles, fetch_global_tech_articles
from services.openai_service import extract_keywords_with_gpt, extract_global_keywords_with_gpt, extract_keywords_with_gpt4o, analyze_keyword_dynamically
from services.sample_service import get_sample_keywords_by_date, get_global_sample_keywords_by_date
from utils.helpers import get_cache, set_cache
from services.trending_service import get_news

logger = logging.getLogger(__name__)
router = APIRouter()

TRENDING_CACHE_KEY = "google_trending_keywords"

@router.get("/keyword-articles/{keyword}")
async def get_keyword_articles(keyword: str, 
                              start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
                              end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """í‚¤ì›Œë“œ í´ë¦­ì‹œ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ë°˜í™˜"""
    try:
        if keyword == "undefined" or not keyword or keyword.strip() == "":
            logger.warning(f"âš ï¸ ì˜ëª»ëœ í‚¤ì›Œë“œ '{keyword}' ìš”ì²­ - ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ëŒ€ì²´")
            keyword = "AI"
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ - ê¸°ê°„: {start_date} ~ {end_date}")
        
        articles = await search_articles_by_keyword(keyword, start_date, end_date)
        
        return {
            "keyword": keyword,
            "articles": articles,
            "total_count": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return {"error": str(e), "keyword": keyword, "articles": [], "total_count": 0}

@router.get("/keyword-articles")
async def get_keyword_articles_legacy(
    keyword: str = Query(..., description="ê²€ìƒ‰í•  í‚¤ì›Œë“œ"),
    start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
    end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")
):
    """ë ˆê±°ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ API (í˜¸í™˜ì„±ìš©)"""
    try:
        articles = await search_articles_by_keyword(keyword, start_date, end_date)
        return {
            "keyword": keyword,
            "total": len(articles),
            "articles": articles,
            "period": f"{start_date} ~ {end_date}",
            "status": "success",
            "note": "ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” deprecatedë©ë‹ˆë‹¤. /api/keyword-articles/{keyword}ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        }
    except Exception as e:
        logger.error(f"/keyword-articles ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keyword": keyword,
            "articles": [],
            "total": 0
        })

@router.get("/global-keyword-articles/{keyword}")
async def get_global_keyword_articles(
    keyword: str, 
    start_date: str = Query("2025-07-14", description="ì‹œì‘ì¼"),
    end_date: str = Query("2025-07-18", description="ì¢…ë£Œì¼")
):
    """í•´ì™¸ í‚¤ì›Œë“œë³„ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ API"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰: '{keyword}' ({start_date} ~ {end_date})")
        articles = await search_global_keyword_articles(keyword, start_date, end_date)
        
        response_data = {
            "keyword": keyword,
            "articles": articles,
            "total": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "region": "global",
            "status": "success"
        }
        
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keyword": keyword,
            "articles": [],
            "total": 0,
            "region": "global"
        })

@router.get("/articles")
async def get_articles(start_date: str = "2025-07-14", end_date: str = "2025-07-18", limit: int = 10):
    """ê¸°ì‚¬ ìš”ì•½ë³¸ ë°˜í™˜ API"""
    try:
        logger.info(f"ğŸ“° ê¸°ì‚¬ ìš”ì•½ë³¸ ìš”ì²­ - ê¸°ê°„: {start_date} ~ {end_date}, ê°œìˆ˜: {limit}")
        articles = await collect_it_news_from_deepsearch(start_date, end_date)
        if not articles:
            logger.warning("ê¸°ì‚¬ ì—†ìŒ: DeepSearch API ê²°ê³¼ ì—†ìŒ")
            return JSONResponse(status_code=200, content={"articles": [], "message": "ê¸°ì‚¬ ì—†ìŒ", "total": 0})
        summarized_articles = []
        for article in articles[:limit]:
            content = article.get("content", "")
            summary = content[:200] + "..." if len(content) > 200 else content
            summarized_articles.append({
                "id": article.get("id", ""),
                "title": article.get("title", ""),
                "summary": summary,
                "date": article.get("date", ""),
                "source_url": article.get("source_url", ""),
                "keyword": article.get("keyword", "")
            })
        return {
            "articles": summarized_articles,
            "total": len(articles),
            "period": f"{start_date} ~ {end_date}",
            "status": "success"
        }
    except Exception as e:
        logger.error(f"/api/articles ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"error": str(e), "articles": [], "total": 0})

@router.get("/keywords")
async def get_weekly_keywords_endpoint(start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """DeepSearch â†’ GPT-4o â†’ Top 5 í‚¤ì›Œë“œ ë°˜í™˜"""
    try:
        logger.info(f"ğŸš€ News GPT v2 ë¶„ì„ ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
        articles = await collect_it_news_from_deepsearch(start_date, end_date)
        if not articles:
            logger.warning("/api/keywords: ê¸°ì‚¬ ì—†ìŒ")
            return {
                "error": "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨", 
                "keywords": [],
                "details": "DeepSearch APIì—ì„œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        logger.info(f"   âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        logger.info("3ï¸âƒ£ Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = await extract_keywords_with_gpt4o(articles)
        if not keywords:
            logger.warning("/api/keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25},
                {"keyword": "ë°˜ë„ì²´", "count": 20},
                {"keyword": "í´ë¼ìš°ë“œ", "count": 18},
                {"keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15},
                {"keyword": "ë¸”ë¡ì²´ì¸", "count": 12}
            ]
        logger.info(f"   âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        result = []
        for i, kw in enumerate(keywords[:5], 1):
            result.append({
                "rank": i,
                "keyword": kw['keyword'],
                "count": kw['count'],
                "source": "ITê¸°ìˆ "
            })
        return {
            "keywords": result,
            "total_articles": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "flow": "DeepSearch â†’ GPT-4o",
            "status": "ì„±ê³µ"
        }
    except Exception as e:
        logger.error(f"/api/keywords ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "error": "ì‹œìŠ¤í…œ ì˜¤ë¥˜",
            "keywords": [],
            "details": str(e),
            "fallback_keywords": [
                {"rank": 1, "keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "source": "ITê¸°ìˆ "},
                {"rank": 2, "keyword": "ë°˜ë„ì²´", "count": 20, "source": "ITê¸°ìˆ "},
                {"rank": 3, "keyword": "í´ë¼ìš°ë“œ", "count": 18, "source": "ITê¸°ìˆ "},
                {"rank": 4, "keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15, "source": "ITê¸°ìˆ "},
                {"rank": 5, "keyword": "ë¸”ë¡ì²´ì¸", "count": 12, "source": "ITê¸°ìˆ "}
            ]
        }

@router.get("/weekly-keywords-by-date")
async def get_weekly_keywords_by_date(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
                               end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """êµ­ë‚´ ë‚ ì§œë³„ ì£¼ê°„ í‚¤ì›Œë“œ ë°˜í™˜ (í”„ë¡ íŠ¸ì—ì„œ ìš”ì²­í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸) - ìºì‹± ì ìš©"""
    region = "domestic"
    cache_key = f"{region}_{start_date}_{end_date}"
    cached_result = get_cache(cache_key)

    if cached_result:
        logger.info(f"âœ… ìºì‹œëœ êµ­ë‚´ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©: {cache_key}")
        return JSONResponse(content={
            "keywords": cached_result,
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(cached_result),
            "tech_articles_count": 0,
            "region": region,
            "status": "success",
            "cached": True
        }, media_type="application/json; charset=utf-8")

    try:
        logger.info(f"ğŸ“… ë‚ ì§œë³„ êµ­ë‚´ í‚¤ì›Œë“œ ìš”ì²­ (DeepSearch & GPT): {start_date} ~ {end_date}")
        tech_articles = await fetch_tech_articles(start_date, end_date)
        tech_articles_count = len(tech_articles)

        if not tech_articles:
            logger.warning(f"âŒ êµ­ë‚´ Tech ê¸°ì‚¬ ì—†ìŒ: {start_date} ~ {end_date}, ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
            keywords = get_sample_keywords_by_date(start_date, end_date)
        else:
            extracted_keywords = await extract_keywords_with_gpt(tech_articles)
            if extracted_keywords:
                keywords = extracted_keywords[:5]
            else:
                logger.warning("âŒ êµ­ë‚´ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
                keywords = get_sample_keywords_by_date(start_date, end_date)
        
        set_cache(cache_key, keywords)

        response_data = {
            "keywords": keywords,
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(keywords),
            "tech_articles_count": tech_articles_count,
            "region": region,
            "status": "success",
            "cached": False
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        logger.error(f"ë‚ ì§œë³„ êµ­ë‚´ í‚¤ì›Œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}", exc_info=True)
        keywords_on_error = get_sample_keywords_by_date(start_date, end_date)
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keywords": keywords_on_error,
            "date_range": f"{start_date} ~ {end_date}",
            "region": region,
            "status": "error",
            "cached": False,
            "note": "ì˜¤ë¥˜ë¡œ ì¸í•´ ìƒ˜í”Œ ë°ì´í„°ê°€ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        })

@router.get("/global-weekly-keywords-by-date")
async def get_global_weekly_keywords_by_date(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                               end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """í•´ì™¸ ë‚ ì§œë³„ ì£¼ê°„ í‚¤ì›Œë“œ ë°˜í™˜ (í”„ë¡ íŠ¸ì—ì„œ ìš”ì²­í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸) - ìºì‹± ì ìš©"""
    region = "global"
    cache_key = f"{region}_{start_date}_{end_date}"
    cached_result = get_cache(cache_key)

    if cached_result:
        logger.info(f"âœ… ìºì‹œëœ í•´ì™¸ í‚¤ì›Œë“œ ê²°ê³¼ ì‚¬ìš©: {cache_key}")
        return JSONResponse(content={
            "keywords": cached_result,
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(cached_result),
            "global_tech_articles_count": 0,
            "region": region,
            "status": "success",
            "cached": True
        }, media_type="application/json; charset=utf-8")

    try:
        logger.info(f"ğŸŒ í•´ì™¸ ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­ (DeepSearch & GPT): {start_date} ~ {end_date}")
        global_tech_articles = await fetch_global_tech_articles(start_date, end_date)
        global_tech_articles_count = len(global_tech_articles)

        if not global_tech_articles:
            logger.warning(f"âŒ í•´ì™¸ Tech ê¸°ì‚¬ ì—†ìŒ: {start_date} ~ {end_date}, ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
            keywords = get_global_sample_keywords_by_date(start_date, end_date)
        else:
            extracted_keywords = await extract_global_keywords_with_gpt(global_tech_articles)
            if extracted_keywords:
                keywords = extracted_keywords[:5]
            else:
                logger.warning("âŒ í•´ì™¸ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
                keywords = get_global_sample_keywords_by_date(start_date, end_date)
        
        set_cache(cache_key, keywords)

        response_data = {
            "keywords": keywords,
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(keywords),
            "global_tech_articles_count": global_tech_articles_count,
            "region": region,
            "status": "success",
            "cached": False
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        logger.error(f"í•´ì™¸ ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}", exc_info=True)
        keywords_on_error = get_global_sample_keywords_by_date(start_date, end_date)
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keywords": keywords_on_error,
            "date_range": f"{start_date} ~ {end_date}",
            "region": region,
            "status": "error",
            "cached": False,
            "note": "ì˜¤ë¥˜ë¡œ ì¸í•´ ìƒ˜í”Œ ë°ì´í„°ê°€ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        })

@router.get("/weekly-keywords")
def get_weekly_keywords():
    """ì£¼ê°„ Top 3 í‚¤ì›Œë“œ ë°˜í™˜"""
    try:
        response_data = {
            "keywords": ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ìˆ í˜ì‹ "],
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.14~07.18) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        response_data = {
            "keywords": ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"],
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.14~07.18) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")

@router.post("/keyword-analysis")
def post_keyword_analysis(request: dict):
    """ë™ì  í‚¤ì›Œë“œ ë¶„ì„ - í´ë¦­ëœ í‚¤ì›Œë“œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„"""
    return analyze_keyword_dynamically(request) 

@router.get("/trending")
def get_trending_keywords():
    """ì‹¤ì‹œê°„ íŠ¸ë Œë”© í‚¤ì›Œë“œ ë°˜í™˜ (ìºì‹œëœ ë°ì´í„°)"""
    cached_data = get_cache(TRENDING_CACHE_KEY)

    if cached_data:
        return JSONResponse(content=cached_data)
    else:
        return JSONResponse(content={"status": "caching", "data": []}, status_code=202)

@router.get("/news")
def get_news_endpoint(country: str, keyword: str):
    """íŠ¹ì • êµ­ê°€ ë° í‚¤ì›Œë“œì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ë°˜í™˜"""
    try:
        articles = get_news(country, keyword)
        return JSONResponse(content={"news": articles})
    except Exception as e:
        logger.error(f"Error in /api/v1/news endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to fetch news articles.")