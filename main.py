# News GPT v2 - ìƒˆë¡œìš´ êµ¬ì¡° (DeepSearch ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°)
import os
import re
import time
import asyncio  # ì¶”ê°€
import logging
import requests
import hashlib
import uvicorn
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, RedirectResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import AzureOpenAI
from functools import wraps
import smtplib
import json
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime

# ë¡œê¹… ì„¤ì • (ìµœì í™”)
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s %(levelname)s %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("news_gpt_v2")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI(title="News GPT v2", description="AI ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ í”Œë«í¼")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")

DEEPSEARCH_API_KEY = os.getenv("DEEPSEARCH_API_KEY")

# ì´ë©”ì¼ ì„¤ì • (Gmail SMTP ì‚¬ìš©)
EMAIL_HOST = "smtp.gmail.com"
EMAIL_PORT = 587
EMAIL_USER = os.getenv("EMAIL_USER")  # Gmail ì£¼ì†Œ
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")  # Gmail ì•± ë¹„ë°€ë²ˆí˜¸

# Pydantic ëª¨ë¸
class SubscriptionRequest(BaseModel):
    email: str

class EmailInsightRequest(BaseModel):
    email: str

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = AzureOpenAI(
    api_key=str(AZURE_OPENAI_API_KEY),
    api_version=str(AZURE_OPENAI_API_VERSION),
    azure_endpoint=str(AZURE_OPENAI_ENDPOINT)
)

# DeepSearch API URLs (êµ­ë‚´ + í•´ì™¸)
DEEPSEARCH_TECH_URL = "https://api-v2.deepsearch.com/v1/articles/tech"
DEEPSEARCH_KEYWORD_URL = "https://api-v2.deepsearch.com/v1/articles"
DEEPSEARCH_GLOBAL_TECH_URL = "https://api-v2.deepsearch.com/v1/global-articles"
DEEPSEARCH_GLOBAL_KEYWORD_URL = "https://api-v2.deepsearch.com/v1/global-articles"


# API í˜¸ì¶œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„° (ìµœì í™”)
def retry_on_exception(max_retries=1, delay=0.1, backoff=1.2, allowed_exceptions=(Exception,)):
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    if asyncio.iscoroutinefunction(func):
                        return await func(*args, **kwargs)
                    else:
                        return func(*args, **kwargs)
                except allowed_exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logger.warning(f"âš¡ {func.__name__} ì¬ì‹œë„ {attempt + 1}/{max_retries}, {wait_time:.1f}ì´ˆ í›„")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"âŒ {func.__name__} ìµœì¢… ì‹¤íŒ¨: {str(e)}")
            raise last_exception
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except allowed_exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logger.warning(f"âš¡ {func.__name__} ì¬ì‹œë„ {attempt + 1}/{max_retries}, {wait_time:.1f}ì´ˆ í›„")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"âŒ {func.__name__} ìµœì¢… ì‹¤íŒ¨: {str(e)}")
            raise last_exception
        
        # í•¨ìˆ˜ê°€ ì½”ë£¨í‹´ì¸ì§€ í™•ì¸
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    return decorator

# =============================================================================
# ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° API ì—”ë“œí¬ì¸íŠ¸ë“¤
# =============================================================================

@app.get("/")
async def serve_home():
    """ë©”ì¸ í˜ì´ì§€ ì œê³µ"""
    return FileResponse("index.html")

@app.get("/analysis.html")
async def serve_analysis():
    """ìƒì„¸ ë¶„ì„ í˜ì´ì§€ ì œê³µ"""
    return FileResponse("analysis.html")

@app.get("/news-detail.html")
async def serve_news_detail():
    """ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ ì œê³µ (ìœ íŠœë¸Œ ìŠ¤íƒ€ì¼)"""
    return FileResponse("news-detail.html")

@app.get("/admin.html")
async def serve_admin():
    """ê´€ë¦¬ì í˜ì´ì§€ ì œê³µ"""
    return FileResponse("admin.html")

# 1ë‹¨ê³„: Tech ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í”„ë¡ íŠ¸ì™€ ì—°ë™)
@app.get("/api/keywords")
async def get_weekly_keywords(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                            end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: Tech ê¸°ì‚¬ â†’ GPT í‚¤ì›Œë“œ ì¶”ì¶œ â†’ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ (Azure AI Search ì œê±°)"""
    try:
        logger.info(f"ğŸš€ ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
        
        # 1ë‹¨ê³„: DeepSearch Techì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë‚ ì§œ í¬í•¨)
        tech_articles = await fetch_tech_articles(start_date, end_date)
        if not tech_articles:
            return {"error": "Tech ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "keywords": [], "articles_count": 0}
        
        # 2ë‹¨ê³„: GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (Azure AI Search ê±´ë„ˆë›°ê¸°)
        extracted_keywords = await extract_keywords_with_gpt(tech_articles)
        if not extracted_keywords:
            return {"error": "í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨", "keywords": [], "articles_count": len(tech_articles)}
        
        # 3ë‹¨ê³„: ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ìš©)
        store_keywords_in_memory(extracted_keywords, start_date, end_date)
        
        return {
            "keywords": extracted_keywords,
            "date_range": f"{start_date} ~ {end_date}",
            "tech_articles_count": len(tech_articles),
            "workflow": "Techê¸°ì‚¬ â†’ GPTí‚¤ì›Œë“œì¶”ì¶œ (Azure AI Search ì œê±°)",
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}", exc_info=True)
        return {"error": str(e), "keywords": [], "articles_count": 0}

# 4ë‹¨ê³„: í‚¤ì›Œë“œ í´ë¦­ì‹œ ê´€ë ¨ ê¸°ì‚¬ ë…¸ì¶œ
@app.get("/api/keyword-articles/{keyword}")
async def get_keyword_articles(keyword: str, 
                              start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
                              end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """í‚¤ì›Œë“œ í´ë¦­ì‹œ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ë°˜í™˜"""
    try:
        # undefined í‚¤ì›Œë“œ ì²˜ë¦¬
        if keyword == "undefined" or not keyword or keyword.strip() == "":
            logger.warning(f"âš ï¸ ì˜ëª»ëœ í‚¤ì›Œë“œ '{keyword}' ìš”ì²­ - ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ëŒ€ì²´")
            keyword = "AI"  # ê¸°ë³¸ í‚¤ì›Œë“œë¡œ ëŒ€ì²´
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ - ê¸°ê°„: {start_date} ~ {end_date}")
        
        # DeepSearch í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ê¸°ì‚¬ ì°¾ê¸°
        articles = await search_articles_by_keyword(keyword, start_date, end_date)
        
        return {
            "keyword": keyword,
            "articles": articles,
            "total_count": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return {"error": str(e), "keyword": keyword, "articles": [], "total_count": 0}

# 5ë‹¨ê³„: ê¸°ì‚¬ í´ë¦­ì‹œ ì›ë³¸ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
@app.get("/api/redirect/{article_id}")
async def redirect_to_original(article_id: str):
    """ê¸°ì‚¬ í´ë¦­ì‹œ ì›ë³¸ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    try:
        # ë©”ëª¨ë¦¬ë‚˜ ìºì‹œì—ì„œ article_idë¡œ ì›ë³¸ URL ì°¾ê¸°
        original_url = get_original_url_by_id(article_id)
        
        if original_url:
            logger.info(f"ğŸ”— ê¸°ì‚¬ ë¦¬ë‹¤ì´ë ‰íŠ¸: {article_id} â†’ {original_url}")
            return RedirectResponse(url=original_url, status_code=302)
        else:
            raise HTTPException(status_code=404, detail=f"ê¸°ì‚¬ ID '{article_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"ë¦¬ë‹¤ì´ë ‰íŠ¸ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# í•´ì™¸ë‰´ìŠ¤ìš© í‚¤ì›Œë“œ ì¶”ì¶œ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/global-keywords")
async def get_global_weekly_keywords(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                                   end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """í•´ì™¸ë‰´ìŠ¤ ì›Œí¬í”Œë¡œìš°: Global Tech ê¸°ì‚¬ â†’ GPT í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ë‰´ìŠ¤ ì›Œí¬í”Œë¡œìš° ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
        
        # 1ë‹¨ê³„: DeepSearch Global Techì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
        global_articles = await fetch_global_tech_articles(start_date, end_date)
        if not global_articles:
            return {"error": "í•´ì™¸ Tech ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "keywords": [], "articles_count": 0}
        
        # 2ë‹¨ê³„: GPTë¡œ ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        extracted_keywords = await extract_global_keywords_with_gpt(global_articles)
        if not extracted_keywords:
            return {"error": "í•´ì™¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨", "keywords": [], "articles_count": len(global_articles)}
        
        # 3ë‹¨ê³„: ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
        store_keywords_in_memory(extracted_keywords, start_date, end_date)
        
        return {
            "keywords": extracted_keywords,
            "date_range": f"{start_date} ~ {end_date}",
            "tech_articles_count": len(global_articles),
            "workflow": "Global Techê¸°ì‚¬ â†’ GPT ì˜ì–´í‚¤ì›Œë“œì¶”ì¶œ",
            "region": "global",
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"í•´ì™¸ë‰´ìŠ¤ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}", exc_info=True)
        return {"error": str(e), "keywords": [], "articles_count": 0}

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸ (deprecated)
@app.get("/keyword-articles")
async def get_keyword_articles_legacy(
    keyword: str = Query(..., description="ê²€ìƒ‰í•  í‚¤ì›Œë“œ"),
    start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
    end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")
):
    """ë ˆê±°ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ API (í˜¸í™˜ì„±ìš©)"""
    try:
        articles = await search_articles_by_keyword(keyword, start_date, end_date)
        return {
            "keyword": keyword,
            "total": len(articles),
            "articles": articles,
            "period": f"{start_date} ~ {end_date}",
            "status": "success",
            "note": "ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” deprecatedë©ë‹ˆë‹¤. /api/keyword-articles/{keyword}ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        }
    except Exception as e:
        logger.error(f"/keyword-articles ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keyword": keyword,
            "articles": [],
            "total": 0
        })

@app.get("/api/global-keyword-articles/{keyword}")
async def get_global_keyword_articles(
    keyword: str, 
    start_date: str = Query("2025-07-14", description="ì‹œì‘ì¼"),
    end_date: str = Query("2025-07-18", description="ì¢…ë£Œì¼")
):
    """í•´ì™¸ í‚¤ì›Œë“œë³„ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ API"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰: '{keyword}' ({start_date} ~ {end_date})")
        
        # í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰
        articles = await search_global_keyword_articles(keyword, start_date, end_date)
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            "keyword": keyword,
            "articles": articles,
            "total": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "region": "global",
            "status": "success"
        }
        
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keyword": keyword,
            "articles": [],
            "total": 0,
            "region": "global"
        })

# =============================================================================
# ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° í•µì‹¬ í•¨ìˆ˜ë“¤
# =============================================================================

# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ìµœì í™”ëœ ìºì‹± ì‹œìŠ¤í…œ)
articles_cache = {}  # {article_id: {url, title, content, ...}}
keywords_cache = {}  # {keyword: [article_ids]}
api_cache = {}  # API ì‘ë‹µ ìºì‹œ {endpoint_params: response_data}
cache_timestamps = {}  # ìºì‹œ ìƒì„± ì‹œê°„ ì €ì¥

# ìºì‹œ ì„¤ì •
CACHE_EXPIRY_MINUTES = 30  # 30ë¶„ ìºì‹œ ìœ ì§€
MAX_CACHE_SIZE = 1000  # ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜

# ìºì‹œ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def get_cache_key(*args, **kwargs):
    """ìºì‹œ í‚¤ ìƒì„±"""
    key_data = str(args) + str(sorted(kwargs.items()))
    return hashlib.md5(key_data.encode()).hexdigest()

def is_cache_valid(cache_key):
    """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
    if cache_key not in cache_timestamps:
        return False
    
    created_time = cache_timestamps[cache_key]
    current_time = time.time()
    age_minutes = (current_time - created_time) / 60
    
    return age_minutes < CACHE_EXPIRY_MINUTES

def set_cache(cache_key, data):
    """ìºì‹œ ì €ì¥"""
    # ìºì‹œ í¬ê¸° ì œí•œ
    if len(api_cache) >= MAX_CACHE_SIZE:
        # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì œê±°
        oldest_key = min(cache_timestamps.keys(), key=lambda k: cache_timestamps[k])
        del api_cache[oldest_key]
        del cache_timestamps[oldest_key]
    
    api_cache[cache_key] = data
    cache_timestamps[cache_key] = time.time()
    logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_key[:8]}... (ì´ {len(api_cache)}ê°œ)")

def get_cache(cache_key):
    """ìºì‹œ ì¡°íšŒ"""
    if cache_key in api_cache and is_cache_valid(cache_key):
        logger.info(f"âš¡ ìºì‹œ íˆíŠ¸: {cache_key[:8]}...")
        return api_cache[cache_key]
    return None

# 1ë‹¨ê³„: DeepSearch Techì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
@retry_on_exception(max_retries=1, delay=0.1, backoff=1.5, allowed_exceptions=(requests.RequestException,))
async def fetch_tech_articles(start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """DeepSearch Tech ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        # ì¶©ë¶„í•œ ê¸°ì‚¬ ìˆ˜ì§‘ìœ¼ë¡œ ë” ì •í™•í•œ í‚¤ì›Œë“œ ë¶„ì„
        base_url = DEEPSEARCH_TECH_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50  # 20ê°œì—ì„œ 50ê°œë¡œ ì¦ê°€í•˜ì—¬ ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘
        }
        
        logger.info(f"ï¿½ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        response = requests.get(base_url, params=params, timeout=5)  # 5ì´ˆë¡œ ë‹¨ì¶•
        logger.info(f"ï¿½ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles = []
        if "articles" in data:
            articles = data["articles"]
        elif "data" in data:
            articles = data["data"]
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        # ê¸°ì‚¬ ì •ê·œí™” ë° ID ìƒì„±
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]  # YYYY-MM-DD
                    else:
                        formatted_date = published_at[:10]  # ì²˜ìŒ 10ìë¦¬ë§Œ
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "content": article.get("summary", "") or article.get("content", ""),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "url": article.get("url", "") or article.get("content_url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": article.get("source", ""),
                "category": "tech"
            }
            
            # ìºì‹œì— ì €ì¥ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©)
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        logger.info(f"âœ… Tech ê¸°ì‚¬ {len(processed_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return processed_articles
        
    except Exception as e:
        logger.error(f"âŒ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# 1-2ë‹¨ê³„: í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ (Global)
@retry_on_exception(max_retries=1, delay=0.1, backoff=1.5, allowed_exceptions=(requests.RequestException,))
async def fetch_global_tech_articles(start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """DeepSearch Global APIì—ì„œ í•´ì™¸ Tech ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        # í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ì„ ìœ„í•œ URL êµ¬ì„±
        base_url = DEEPSEARCH_GLOBAL_TECH_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": "tech",  # í•´ì™¸ì—ì„œëŠ” tech í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50  # 20ê°œì—ì„œ 50ê°œë¡œ ì¦ê°€í•˜ì—¬ ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘
        }
        
        logger.info(f"ğŸŒ í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        response = requests.get(base_url, params=params, timeout=5)  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
        logger.info(f"ğŸ“Š í•´ì™¸ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í•´ì™¸ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles = []
        if 'data' in data:
            articles = data['data']
        elif 'articles' in data:
            articles = data['articles']
        elif isinstance(data, list):
            articles = data
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í•´ì™¸ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        # ê¸°ì‚¬ ì •ê·œí™” ë° ID ìƒì„±
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]  # YYYY-MM-DD
                    else:
                        formatted_date = published_at[:10]  # ì²˜ìŒ 10ìë¦¬ë§Œ
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "content": article.get("summary", "") or article.get("content", "ë‚´ìš© ì—†ìŒ"),
                "url": article.get("content_url", "") or article.get("url", ""),
                "date": formatted_date,
                "source": "í•´ì™¸",
                "category": "global_tech"
            }
            
            # ìºì‹œì— ì €ì¥ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©)
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        logger.info(f"âœ… í•´ì™¸ Tech ê¸°ì‚¬ {len(processed_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return processed_articles
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# 2ë‹¨ê³„: GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
async def extract_keywords_with_gpt(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """GPTë¥¼ ì‚¬ìš©í•´ ê¸°ì‚¬ë“¤ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ìµœì í™”ë¨)"""
    if not articles:
        logger.warning("âŒ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    try:
        # ìµœì í™”: ìƒìœ„ 10ê°œ ê¸°ì‚¬ë§Œ ë¶„ì„í•˜ê³  ì œëª©ë§Œ ì‚¬ìš©
        top_articles = articles[:10]
        titles_text = " ".join([article['title'][:50] for article in top_articles])
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì†ë„ í–¥ìƒ
        prompt = f"""ë‹¤ìŒ ITê¸°ìˆ  ë‰´ìŠ¤ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
{titles_text}

ì¤‘ìš”: ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ë‹¨ìˆœ í…ìŠ¤íŠ¸ë¡œë§Œ ë‹µë³€.
í˜•ì‹: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3, í‚¤ì›Œë“œ4, í‚¤ì›Œë“œ5"""
        
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ITê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€. ë§ˆí¬ë‹¤ìš´ í—¤ë” ì‚¬ìš© ê¸ˆì§€. ë‹¨ìˆœ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=80,  # 5ê°œ í‚¤ì›Œë“œì— ë§ê²Œ ì¦ê°€
            temperature=0  # ì¼ê´€ì„± ìµœëŒ€í™”
        )
        
        
        keywords_text = response.choices[0].message.content or ""
        logger.info(f"ğŸš€ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords_text}")
        
        # ë¹ ë¥¸ í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        for i, item in enumerate(keywords_text.split(',')[:5], 1):  # Top 5ë¡œ ì¦ê°€
            keyword = item.strip().replace('.', '').replace('1', '').replace('2', '').replace('3', '').replace('4', '').replace('5', '')
            keyword = re.sub(r'[^\wê°€-í£]', '', keyword)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            
            if keyword and 2 <= len(keyword) <= 10:
                keywords.append({
                    "keyword": keyword,
                    "count": 30 - (i * 5),  # 25, 20, 15, 10, 5 ìˆœìœ¼ë¡œ
                    "rank": i
                })
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ (ë¹ˆ ê²°ê³¼ ì‹œ)
        if not keywords:
            keywords = [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "rank": 1},
                {"keyword": "ë°˜ë„ì²´", "count": 20, "rank": 2},
                {"keyword": "í´ë¼ìš°ë“œ", "count": 15, "rank": 3},
                {"keyword": "ë¹…ë°ì´í„°", "count": 10, "rank": 4},
                {"keyword": "ë¡œë´‡", "count": 5, "rank": 5}
            ]
        
        return keywords[:5]  # Top 5 ë°˜í™˜
        
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [
            {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "rank": 1},
            {"keyword": "ë°˜ë„ì²´", "count": 20, "rank": 2},
            {"keyword": "í´ë¼ìš°ë“œ", "count": 15, "rank": 3}
        ]

# 2ë‹¨ê³„-í•´ì™¸: í•´ì™¸ ê¸°ì‚¬ì—ì„œ ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ 
async def extract_global_keywords_with_gpt(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """GPTë¥¼ ì‚¬ìš©í•´ í•´ì™¸ ê¸°ì‚¬ë“¤ì—ì„œ ì˜ì–´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ìµœì í™”ë¨)"""
    if not articles:
        logger.warning("âŒ ë¶„ì„í•  í•´ì™¸ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    try:
        # ìµœì í™”: ìƒìœ„ 10ê°œ ê¸°ì‚¬ë§Œ ë¶„ì„í•˜ê³  ì œëª©ë§Œ ì‚¬ìš©
        top_articles = articles[:10]
        titles_text = " ".join([article['title'][:50] for article in top_articles])
        
        # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"""Extract 5 key English tech keywords from these global news titles:
{titles_text}

Requirements:
- Only English words
- Tech/Technology focused
- No Korean words
- No markdown headers (#)
- Plain text only
Format: keyword1, keyword2, keyword3, keyword4, keyword5"""
        
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "You are an expert at extracting English tech keywords from global news. Use plain text only, no markdown headers."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=50,  # ë” ì§§ê²Œ
            temperature=0  # ì¼ê´€ì„± ìµœëŒ€í™”
        )
        
        keywords_text = response.choices[0].message.content or ""
        logger.info(f"ğŸŒ í•´ì™¸ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords_text}")
        
        # ì˜ì–´ í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        for i, item in enumerate(keywords_text.split(',')[:5], 1):  # Top 5ë¡œ ì¦ê°€
            keyword = item.strip().replace('.', '').replace('1', '').replace('2', '').replace('3', '').replace('4', '').replace('5', '')
            keyword = re.sub(r'[^a-zA-Z\s]', '', keyword).strip()  # ì˜ì–´ë§Œ í—ˆìš©
            
            if keyword and 2 <= len(keyword) <= 15 and keyword.replace(' ', '').isalpha():
                keywords.append({
                    "keyword": keyword,
                    "count": 30 - (i * 5),  # 25, 20, 15, 10, 5 ìˆœìœ¼ë¡œ
                    "rank": i
                })
        
        # ê¸°ë³¸ ì˜ì–´ í‚¤ì›Œë“œ (ë¹ˆ ê²°ê³¼ ì‹œ)
        if not keywords:
            keywords = [
                {"keyword": "AI Technology", "count": 25, "rank": 1},
                {"keyword": "Innovation", "count": 20, "rank": 2},
                {"keyword": "Digital Transformation", "count": 15, "rank": 3},
                {"keyword": "Machine Learning", "count": 10, "rank": 4},
                {"keyword": "Cloud Computing", "count": 5, "rank": 5}
            ]
        
        return keywords[:5]  # Top 5 ë°˜í™˜
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [
            {"keyword": "Technology", "count": 25, "rank": 1},
            {"keyword": "Innovation", "count": 20, "rank": 2},
            {"keyword": "Digital", "count": 15, "rank": 3}
        ]

# 3ë‹¨ê³„: í‚¤ì›Œë“œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
def store_keywords_in_memory(keywords: List[Dict[str, Any]], start_date: str, end_date: str):
    """ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤"""
    for keyword_data in keywords:
        keyword = keyword_data['keyword']
        keywords_cache[keyword] = {
            "keyword_data": keyword_data,
            "date_range": f"{start_date}~{end_date}",
            "cached_at": time.time()
        }
    logger.info(f"ğŸ“ {len(keywords)}ê°œ í‚¤ì›Œë“œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥ ì™„ë£Œ")

# 4ë‹¨ê³„: í•´ì™¸ í‚¤ì›Œë“œë¡œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
async def search_global_keyword_articles(keyword: str, start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ DeepSearch Global APIì—ì„œ í•´ì™¸ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        # í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìœ„í•œ URL êµ¬ì„±
        base_url = DEEPSEARCH_GLOBAL_KEYWORD_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": keyword,  # ì˜ì–´ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50  # ì¶©ë¶„í•œ ê¸°ì‚¬ ìˆ˜ì§‘
        }
        
        logger.info(f"ğŸŒ í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... URL: {base_url}")
        logger.info(f"ğŸŒ íŒŒë¼ë¯¸í„°: {params}")
        response = requests.get(base_url, params=params, timeout=5)
        logger.info(f"ğŸŒ ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles = []
        if 'data' in data:
            articles = data['data']
        elif 'articles' in data:
            articles = data['articles']
        elif isinstance(data, list):
            articles = data
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        # ê¸°ì‚¬ ì •ê·œí™” ë° ìºì‹œ ì €ì¥
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]  # YYYY-MM-DD
                    else:
                        formatted_date = published_at[:10]  # ì²˜ìŒ 10ìë¦¬ë§Œ
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "content": article.get("summary", "") or article.get("content", ""),
                "url": article.get("content_url", "") or article.get("url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": "í•´ì™¸",
                "keyword": keyword,
                "region": "global",
                "relevance_score": calculate_relevance_score(article, keyword)
            }
            
            # ìºì‹œì— ì €ì¥ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©)
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        processed_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        logger.info(f"âœ… í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ {len(processed_articles)}ê°œ ê²€ìƒ‰ ì™„ë£Œ")
        return processed_articles[:15]  # ìƒìœ„ 15ê°œë§Œ ë°˜í™˜
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# 4ë‹¨ê³„: í‚¤ì›Œë“œë¡œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
async def search_articles_by_keyword(keyword: str, start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ DeepSearchì—ì„œ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        # ì‚¬ìš©ì ì œê³µ ì˜ˆì‹œ URL êµ¬ì¡° ì‚¬ìš©
        base_url = DEEPSEARCH_KEYWORD_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": keyword,  # ì‚¬ìš©ì ì˜ˆì‹œì— ë§ì¶° keyword íŒŒë¼ë¯¸í„° ì‚¬ìš©
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50  # 15ê°œì—ì„œ 50ê°œë¡œ ì¦ê°€í•˜ì—¬ ë” ë§ì€ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘
        }
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... URL: {base_url}")
        logger.info(f"ğŸ” íŒŒë¼ë¯¸í„°: {params}")
        response = requests.get(base_url, params=params, timeout=3)  # 3ì´ˆë¡œ ë‹¨ì¶•
        logger.info(f"ğŸ” ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í‚¤ì›Œë“œ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles = []
        if "articles" in data:
            articles = data["articles"]
        elif "data" in data:
            articles = data["data"]
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        # ê¸°ì‚¬ ì •ê·œí™” ë° ìºì‹œ ì €ì¥
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]  # YYYY-MM-DD
                    else:
                        formatted_date = published_at[:10]  # ì²˜ìŒ 10ìë¦¬ë§Œ
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "content": article.get("summary", "") or article.get("content", ""),
                "url": article.get("url", "") or article.get("content_url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": article.get("source", ""),
                "keyword": keyword,
                "relevance_score": calculate_relevance_score(article, keyword)
            }
            
            # ìºì‹œì— ì €ì¥ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©)
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        processed_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        logger.info(f"âœ… í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ {len(processed_articles)}ê°œ ê²€ìƒ‰ ì™„ë£Œ")
        return processed_articles[:15]  # ìƒìœ„ 15ê°œë§Œ ë°˜í™˜
        
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# 5ë‹¨ê³„: ê¸°ì‚¬ IDë¡œ ì›ë³¸ URL ì°¾ê¸°
def get_original_url_by_id(article_id: str) -> Optional[str]:
    """ê¸°ì‚¬ IDë¡œ ì›ë³¸ URLì„ ì°¾ìŠµë‹ˆë‹¤"""
    article = articles_cache.get(article_id)
    if article:
        return article.get("url")
    return None

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def generate_article_id(article: Dict[str, Any]) -> str:
    """ê¸°ì‚¬ ì •ë³´ë¡œ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    content = f"{article.get('title', '')}{article.get('url', '')}{article.get('published_at', '')}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]

def calculate_relevance_score(article: Dict[str, Any], keyword: str) -> float:
    """ê¸°ì‚¬ì™€ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤"""
    title = article.get("title", "").lower()
    content = article.get("summary", "") or article.get("content", "")
    content = content.lower()
    keyword_lower = keyword.lower()
    
    score = 0.0
    
    # ì œëª©ì— í‚¤ì›Œë“œ í¬í•¨ì‹œ ë†’ì€ ì ìˆ˜
    if keyword_lower in title:
        score += 10.0
    
    # ë‚´ìš©ì— í‚¤ì›Œë“œ í¬í•¨ì‹œ ì ìˆ˜ ì¶”ê°€
    content_count = content.count(keyword_lower)
    score += content_count * 2.0
    
    # ìµœê·¼ ê¸°ì‚¬ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    pub_date = article.get("published_at", "")
    if "2025-07" in pub_date:
        score += 5.0
    
    return score

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°ì— ë§ê²Œ ìˆ˜ì •...
async def search_keyword_articles(keyword: str, start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ (ë” ì •í™•í•œ ê²€ìƒ‰, í•œêµ­ì–´ ìš°ì„ , ë‚ ì§œ í•„í„°ë§)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    try:
        # IT í‚¤ì›Œë“œì— ëŒ€í•´ì„œë§Œ ì •í™•í•œ ê²€ìƒ‰
        if keyword == "IT":
            search_terms = ["IT", "ì •ë³´ê¸°ìˆ ", "Information Technology"]
        else:
            search_terms = [keyword]
        articles = []
        for search_term in search_terms:
            try:
                # DeepSearch APIì—ì„œ ê²½ì œ,ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ + í‚¤ì›Œë“œ ê²€ìƒ‰
                url = f"https://api-v2.deepsearch.com/v1/articles/economy,tech"
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "date_from": start_date,
                    "date_to": end_date,
                    "q": search_term  # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¶”ê°€
                }
                response = requests.get(url, params=params, timeout=5)
                response.raise_for_status()
                data = response.json()
                for item in data.get("data", []):
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    if article_date:
                        title = item.get("title", "")
                        content = item.get("summary", "") or item.get("content", "")
                        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°(ê°„ë‹¨í™”)
                        relevance_score = 1.0 if keyword in title or keyword in content else 0.5
                        korean_chars = len([c for c in title + content if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3])
                        total_chars = len(title + content)
                        is_korean = korean_chars / max(total_chars, 1) > 0.3
                        articles.append({
                            "title": title,
                            "content": content,
                            "source_url": item.get("content_url", "") or item.get("url", ""),
                            "date": article_date,
                            "relevance_score": relevance_score,
                            "search_term": search_term,
                            "is_korean": is_korean
                        })
            except Exception as e:
                logger.warning(f"âŒ '{search_term}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in articles:
            title = article.get("title", "")
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)
        # í•œêµ­ì–´ ê¸°ì‚¬ ìš°ì„  + ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        unique_articles.sort(key=lambda x: (
            not x.get("is_korean", False),
            -x.get("relevance_score", 0),
            x.get("date", "")
        ))
        logger.info(f"âœ… '{keyword}' ({start_date}~{end_date}): {len(unique_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì™„ë£Œ (í•œêµ­ì–´ ìš°ì„ , ê´€ë ¨ì„± í•„í„°ë§)")
        return unique_articles[:12]
    except Exception as e:
        logger.error(f"âŒ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ í•¨ìˆ˜
async def search_global_keyword_articles(keyword: str, start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """í•´ì™¸ í‚¤ì›Œë“œë¡œ DeepSearch Global APIì—ì„œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
        
        # í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (GPT ì¶”ì¶œëœ í‚¤ì›Œë“œ ì‚¬ìš©)
        base_url = DEEPSEARCH_GLOBAL_KEYWORD_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": keyword,
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 50  # 15ê°œì—ì„œ 50ê°œë¡œ ì¦ê°€í•˜ì—¬ ë” ë§ì€ í•´ì™¸ ê¸°ì‚¬ ìˆ˜ì§‘
        }
        
        logger.info(f"ğŸ” í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰...")
        response = requests.get(base_url, params=params, timeout=5)  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
        
        if response.status_code != 200:
            logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles_data = []
        if 'data' in data:
            articles_data = data['data']
        elif 'articles' in data:
            articles_data = data['articles']
        elif isinstance(data, list):
            articles_data = data
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í•´ì™¸ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        articles = []
        for item in articles_data:
            pub_date = item.get("published_at", "")
            if "T" in pub_date:
                article_date = pub_date.split("T")[0]
            else:
                article_date = pub_date
            if article_date:
                title = item.get("title", "")
                content = item.get("summary", "") or item.get("content", "")
                article_id = generate_article_id(item)
                source_url = item.get("content_url", "") or item.get("url", "")
                
                article = {
                    "id": article_id,
                    "title": title,
                    "content": content,
                    "date": article_date,
                    "source_url": source_url,
                    "url": source_url,  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” í•„ë“œ
                    "keyword": keyword,
                    "source": "í•´ì™¸"
                }
                
                # í•´ì™¸ ê¸°ì‚¬ë„ ìºì‹œì— ì €ì¥í•˜ì—¬ ì›ë³¸ URL ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°€ëŠ¥í•˜ê²Œ í•¨
                articles_cache[article_id] = article
                
                articles.append(article)
                
        # ì¤‘ë³µ ì œê±° (ì œëª©+ë‚´ìš© í•´ì‹œ)
        unique_articles = []
        seen_hashes = set()
        for article in articles:
            hash_key = hash((article["title"].lower(), article["content"][:100].lower()))
            if hash_key not in seen_hashes:
                seen_hashes.add(hash_key)
                unique_articles.append(article)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        unique_articles.sort(key=lambda x: (
            x.get("date", "")
        ), reverse=True)
        
        logger.info(f"âœ… í•´ì™¸ '{keyword}' ({start_date}~{end_date}): {len(unique_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì™„ë£Œ")
        return unique_articles[:12]
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

@app.get("/api/articles")
async def get_articles(start_date: str = "2025-07-14", end_date: str = "2025-07-18", limit: int = 10):
    """ê¸°ì‚¬ ìš”ì•½ë³¸ ë°˜í™˜ API"""
    try:
        logger.info(f"ğŸ“° ê¸°ì‚¬ ìš”ì•½ë³¸ ìš”ì²­ - ê¸°ê°„: {start_date} ~ {end_date}, ê°œìˆ˜: {limit}")
        articles = await collect_it_news_from_deepsearch(start_date, end_date)
        if not articles:
            logger.warning("ê¸°ì‚¬ ì—†ìŒ: DeepSearch API ê²°ê³¼ ì—†ìŒ")
            return JSONResponse(status_code=200, content={"articles": [], "message": "ê¸°ì‚¬ ì—†ìŒ", "total": 0})
        summarized_articles = []
        for article in articles[:limit]:
            content = article.get("content", "")
            summary = content[:200] + "..." if len(content) > 200 else content
            summarized_articles.append({
                "id": article.get("id", ""),
                "title": article.get("title", ""),
                "summary": summary,
                "date": article.get("date", ""),
                "source_url": article.get("source_url", ""),
                "keyword": article.get("keyword", "")
            })
        return {
            "articles": summarized_articles,
            "total": len(articles),
            "period": f"{start_date} ~ {end_date}",
            "status": "success"
        }
    except Exception as e:
        logger.error(f"/api/articles ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"error": str(e), "articles": [], "total": 0})


# /api/keywords ì—”ë“œí¬ì¸íŠ¸(ìµœì‹ /ì •ë¦¬ë³¸)
@app.get("/api/keywords")
async def get_weekly_keywords(start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """DeepSearch â†’ Azure AI Search â†’ GPT-4o â†’ Top 5 í‚¤ì›Œë“œ ë°˜í™˜"""
    try:
        logger.info(f"ğŸš€ News GPT v2 ë¶„ì„ ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
        if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, DEEPSEARCH_API_KEY]):
            logger.error("í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì˜¤ë¥˜: Azure OpenAI ë˜ëŠ” DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return {
                "error": "í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì˜¤ë¥˜", 
                "keywords": [],
                "details": "Azure OpenAI ë˜ëŠ” DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        articles = await collect_it_news_from_deepsearch(start_date, end_date)
        if not articles:
            logger.warning("/api/keywords: ê¸°ì‚¬ ì—†ìŒ")
            return {
                "error": "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨", 
                "keywords": [],
                "details": "DeepSearch APIì—ì„œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        logger.info(f"   âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        # Azure AI Search ì—…ë¡œë“œ ì œê±° (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼)
        logger.info("3ï¸âƒ£ Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = await extract_keywords_with_gpt4o(articles)
        if not keywords:
            logger.warning("/api/keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25},
                {"keyword": "ë°˜ë„ì²´", "count": 20},
                {"keyword": "í´ë¼ìš°ë“œ", "count": 18},
                {"keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15},
                {"keyword": "ë¸”ë¡ì²´ì¸", "count": 12}
            ]
        logger.info(f"   âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        result = []
        for i, kw in enumerate(keywords[:5], 1):
            result.append({
                "rank": i,
                "keyword": kw['keyword'],
                "count": kw['count'],
                "source": "ITê¸°ìˆ "
            })
        return {
            "keywords": result,
            "total_articles": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "flow": "DeepSearch â†’ Azure AI Search â†’ GPT-4o",
            "status": "ì„±ê³µ"
        }
    except Exception as e:
        logger.error(f"/api/keywords ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "error": "ì‹œìŠ¤í…œ ì˜¤ë¥˜",
            "keywords": [],
            "details": str(e),
            "fallback_keywords": [
                {"rank": 1, "keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "source": "ITê¸°ìˆ "},
                {"rank": 2, "keyword": "ë°˜ë„ì²´", "count": 20, "source": "ITê¸°ìˆ "},
                {"rank": 3, "keyword": "í´ë¼ìš°ë“œ", "count": 18, "source": "ITê¸°ìˆ "},
                {"rank": 4, "keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15, "source": "ITê¸°ìˆ "},
                {"rank": 5, "keyword": "ë¸”ë¡ì²´ì¸", "count": 12, "source": "ITê¸°ìˆ "}
            ]
        }




# API í˜¸ì¶œì— ì¬ì‹œë„ ì ìš©
@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
def deepsearch_api_request(url, params):
    """DeepSearch API ìš”ì²­ (ì¬ì‹œë„/ë¡œê¹… ì¼ê´€ì„±)"""
    logger.info(f"DeepSearch API ìš”ì²­: {url} | params: {params}")
    response = requests.get(url, params=params, timeout=5)
    logger.info(f"DeepSearch ì‘ë‹µ ì½”ë“œ: {response.status_code}")
    response.raise_for_status()
    return response.json()

async def collect_it_news_from_deepsearch(start_date: str, end_date: str):
    """DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    try:
        articles = []
        tech_keywords = ["IT", "ê¸°ìˆ ", "ì¸ê³µì§€ëŠ¥", "AI", "ë°˜ë„ì²´"]
        logger.info(f"ğŸ” DeepSearch APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... ({start_date} ~ {end_date})")
        
        for keyword in tech_keywords:
            try:
                # ì‚¬ìš©ì ì œê³µ ì˜ˆì‹œ URL êµ¬ì¡° ì‚¬ìš©
                base_url = DEEPSEARCH_KEYWORD_URL
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "keyword": keyword,  # ì‚¬ìš©ì ì˜ˆì‹œì— ë§ì¶° keyword íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    "date_from": start_date,
                    "date_to": end_date
                }
                response = requests.get(base_url, params=params, timeout=5)
                
                if response.status_code != 200:
                    logger.warning(f"    âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                    continue
                    
                data = response.json()
                
                # ì‘ë‹µ êµ¬ì¡° í™•ì¸
                if "data" in data:
                    articles_data = data["data"]
                elif "articles" in data:
                    articles_data = data["articles"]
                else:
                    logger.warning(f"    âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
                    continue
                    
                for item in articles_data:
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    title = item.get("title", "").strip()
                    content = (item.get("summary", "") or item.get("content", "")).strip()
                    if title and content:
                        articles.append({
                            "id": f"news_{keyword}_{hash(title)%100000}",
                            "title": title,
                            "content": content,
                            "date": article_date,
                            "source_url": item.get("content_url", "") or item.get("url", ""),
                            "keyword": keyword
                        })
                logger.info(f"    âœ… '{keyword}': {len(articles_data)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                # time.sleep(0.2)  # ì†ë„ í–¥ìƒì„ ìœ„í•´ ì œê±°
            except Exception as e:
                logger.warning(f"    âŒ '{keyword}' ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
        # ì¤‘ë³µ ì œê±° (ì œëª©+ë‚´ìš© í•´ì‹œ)
        unique_articles = []
        seen_hashes = set()
        for article in articles:
            hash_key = hash((article["title"].lower(), article["content"][:100].lower()))
            if hash_key not in seen_hashes:
                seen_hashes.add(hash_key)
                unique_articles.append(article)
        logger.info(f"âœ… ì´ {len(unique_articles)}ê°œ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles[:30]
    except Exception as e:
        logger.error(f"âŒ DeepSearch API ì „ì²´ ì˜¤ë¥˜: {e}", exc_info=True)
        # ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜ (API ì‹¤íŒ¨ ì‹œ)
        return [
            {
                "id": "sample_1",
                "title": "AI ê¸°ìˆ  ë°œì „ìœ¼ë¡œ IT ì—…ê³„ ë³€í™” ê°€ì†í™”",
                "content": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ IT ì—…ê³„ ì „ë°˜ì— ë³€í™”ê°€ ì¼ì–´ë‚˜ê³  ìˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•œ ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ë“¤ì´ ë“±ì¥í•˜ê³  ìˆìœ¼ë©°, ê¸°ì—…ë“¤ì€ ë””ì§€í„¸ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì„ ê°€ì†í™”í•˜ê³  ìˆë‹¤.",
                "date": start_date,
                "source_url": "https://example.com/ai-news",
                "keyword": "AI"
            },
            {
                "id": "sample_2",
                "title": "ë°˜ë„ì²´ ì‚°ì—… íšŒë³µ ì¡°ì§, ê¸€ë¡œë²Œ ê³µê¸‰ë§ ì•ˆì •í™”",
                "content": "ë°˜ë„ì²´ ì‚°ì—…ì´ íšŒë³µ ì¡°ì§ì„ ë³´ì´ë©° ê¸€ë¡œë²Œ ê³µê¸‰ë§ì´ ì•ˆì •í™”ë˜ê³  ìˆë‹¤. ì£¼ìš” ë°˜ë„ì²´ ê¸°ì—…ë“¤ì˜ ì‹¤ì ì´ ê°œì„ ë˜ê³  ìˆìœ¼ë©°, ìƒˆë¡œìš´ ê¸°ìˆ  ê°œë°œì— ëŒ€í•œ íˆ¬ìë„ ì¦ê°€í•˜ê³  ìˆë‹¤.",
                "date": start_date,
                "source_url": "https://example.com/semiconductor-news",
                "keyword": "ë°˜ë„ì²´"
            }
        ]


# Azure AI Search í•¨ìˆ˜ ì œê±°ë¨ (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ DeepSearchë§Œ ì‚¬ìš©)

async def extract_keywords_with_gpt4o(articles):
    """Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    try:
        articles_text = "\n".join([
            f"ì œëª©: {article['title']}\në‚´ìš©: {article['content'][:200]}..."
            for article in articles[:50]
        ])
        
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ê³  ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ê¸°ì‚¬ ë‚´ìš©:
{articles_text}

ìš”êµ¬ì‚¬í•­:
1. ê¸°ì—…ëª…ì€ ì œì™¸í•˜ê³  ê¸°ìˆ /ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
2. ê¸°ì‚¬ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
3. ì‘ë‹µ í˜•ì‹: í‚¤ì›Œë“œ1:ë¹ˆë„1, í‚¤ì›Œë“œ2:ë¹ˆë„2 (ì½¤ë§ˆ êµ¬ë¶„)
4. ë¹ˆë„ëŠ” 5-25 ë²”ìœ„
5. ìµœì†Œ 5ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ

ì£¼ìš” í‚¤ì›Œë“œ:
"""
        
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.2
        )
        
        keywords_text = response.choices[0].message.content or ""
        print(f"GPT-4o ì‘ë‹µ: {keywords_text}")
        
        # í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        for item in keywords_text.split(','):
            if ':' in item:
                parts = item.strip().split(':', 1)
                keyword = parts[0].strip()
                try:
                    count = int(parts[1].strip())
                    keywords.append({"keyword": keyword, "count": count})
                except:
                    keywords.append({"keyword": keyword, "count": 10})
        
        # í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ í‚¤ì›Œë“œ ì œê³µ
        if not keywords:
            print("âš ï¸ GPT-4oì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "IT", "count": 20},
                {"keyword": "ê¸°ìˆ ", "count": 18},
                {"keyword": "ë””ì§€í„¸", "count": 15},
                {"keyword": "ì •ë³´", "count": 12},
                {"keyword": "ì‹œìŠ¤í…œ", "count": 10}
            ]
        
        # ë¹ˆë„ ê¸°ì¤€ ì •ë ¬
        keywords.sort(key=lambda x: x['count'], reverse=True)
        
        print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        return keywords
        
    except Exception as e:
        print(f"âŒ GPT-4o ì˜¤ë¥˜: {e}")
        return []

# ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ì‹œê°ë³„ ë¶„ì„ê³¼ ì±—ë´‡ ê¸°ëŠ¥


from fastapi import Request
from fastapi.responses import JSONResponse

@app.post("/chat")
async def chat(request: Request):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ ê¸°ë°˜ ë™ì  ì±—ë´‡"""
    try:
        data = await request.json()
        question = data.get("question") or data.get("message") or ""
        if not question:
            return JSONResponse(content={"answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."})
        # 1. ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì‚°ì—… ë¶„ë¥˜
        keyword_info = extract_keyword_and_industry(question)
        # 2. í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        current_keywords = get_current_weekly_keywords()
        # 3. ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë™ì  ì‘ë‹µ
        if keyword_info["type"] == "industry_analysis":
            answer = generate_industry_based_answer(
                question,
                keyword_info["keyword"],
                keyword_info["industry"],
                current_keywords
            )
        elif keyword_info["type"] == "keyword_trend":
            answer = generate_keyword_trend_answer(question, keyword_info["keyword"])
        elif keyword_info["type"] == "comparison":
            answer = generate_comparison_answer(question, keyword_info["keywords"])
        else:
            answer = generate_contextual_answer(question, current_keywords)
        return JSONResponse(content={"answer": answer})
    except Exception as e:
        logger.error(f"/chat ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(content={"answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, status_code=500)

def extract_keyword_and_industry(question):
    """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œì™€ ì‚°ì—… ë¶„ë¥˜ ì¶”ì¶œ"""
    
    # ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
    industry_keywords = {
        "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ì¼ìë¦¬", "ë³µì§€", "ì •ì±…", "ì œë„", "ì‹œë¯¼", "ê³µê³µ"],
        "ê²½ì œ": ["ê²½ì œ", "ì‹œì¥", "íˆ¬ì", "ê¸ˆìœµ", "ì£¼ê°€", "ë¹„ìš©", "ìˆ˜ìµ", "ë§¤ì¶œ", "ê¸°ì—…"],
        "IT/ê³¼í•™": ["ê¸°ìˆ ", "ê°œë°œ", "í˜ì‹ ", "ì—°êµ¬", "ê³¼í•™", "IT", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´", "í”Œë«í¼"],
        "ìƒí™œ/ë¬¸í™”": ["ìƒí™œ", "ë¬¸í™”", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì†Œë¹„", "íŠ¸ë Œë“œ", "ì¼ìƒ", "ì—¬ê°€", "ì—”í„°í…Œì¸ë¨¼íŠ¸"],
        "ì„¸ê³„": ["ê¸€ë¡œë²Œ", "êµ­ì œ", "ì„¸ê³„", "í•´ì™¸", "ìˆ˜ì¶œ", "í˜‘ë ¥", "ê²½ìŸ", "í‘œì¤€"]
    }
    
    question_lower = question.lower()
    
    # ì‚°ì—… ë¶„ë¥˜ ì¶”ì¶œ
    detected_industry = None
    for industry, keywords in industry_keywords.items():
        if any(keyword in question_lower for keyword in keywords):
            detected_industry = industry
            break
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
    # í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œì™€ ë§¤ì¹˜ë˜ëŠ” ê²ƒ ì°¾ê¸°
    current_keywords = get_current_weekly_keywords()
    detected_keyword = None
    for keyword in current_keywords:
        if keyword in question or keyword.lower() in question_lower:
            detected_keyword = keyword
            break
    
    # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
    if detected_industry and detected_keyword:
        question_type = "industry_analysis"
    elif "vs" in question_lower or "ë¹„êµ" in question_lower or "ì°¨ì´" in question_lower:
        question_type = "comparison"
        # ë¹„êµ ëŒ€ìƒ í‚¤ì›Œë“œë“¤ ì¶”ì¶œ
        comparison_keywords = [kw for kw in current_keywords if kw.lower() in question_lower]
        return {
            "type": question_type,
            "keywords": comparison_keywords,
            "industry": detected_industry,
            "keyword": detected_keyword
        }
    elif detected_keyword:
        question_type = "keyword_trend"
    else:
        question_type = "general"
    
    return {
        "type": question_type,
        "keyword": detected_keyword,
        "industry": detected_industry or "ì‚¬íšŒ"  # ê¸°ë³¸ê°’
    }

def get_current_weekly_keywords():
    """í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # ì§ì ‘ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ ëŒ€ì‹  í˜„ì¬ ì£¼ì°¨ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ ë°˜í™˜
        return ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ìˆ í˜ì‹ "]
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"]

def generate_industry_based_answer(question, keyword, industry, current_keywords):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    try:
        # ì‚°ì—…ë³„ ê´€ì  ì •ì˜
        industry_context = {
            "ì‚¬íšŒ": "ì‚¬íšŒì  ì˜í–¥, ì •ì±…ì  ì¸¡ë©´, ì‹œë¯¼ ìƒí™œ ë³€í™”",
            "ê²½ì œ": "ê²½ì œì  íŒŒê¸‰íš¨ê³¼, ì‹œì¥ ë™í–¥, íˆ¬ì ê´€ì ",
            "IT/ê³¼í•™": "ê¸°ìˆ ì  í˜ì‹ , ì—°êµ¬ê°œë°œ ë™í–¥, ê¸°ìˆ ì  ê³¼ì œ",
            "ìƒí™œ/ë¬¸í™”": "ì¼ìƒìƒí™œ ë³€í™”, ë¬¸í™”ì  ìˆ˜ìš©ì„±, ì†Œë¹„ì í–‰ë™",
            "ì„¸ê³„": "ê¸€ë¡œë²Œ íŠ¸ë Œë“œ, êµ­ì œ ê²½ìŸ, í•´ì™¸ ë™í–¥"
        }
        
        context_desc = industry_context.get(industry, "ì „ë°˜ì ì¸ ê´€ì ")
        
        prompt = f"""
ì§ˆë¬¸: {question}
í‚¤ì›Œë“œ: {keyword}
ê´€ì : {industry} ({context_desc})
í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(current_keywords)}

{industry} ê´€ì ì—ì„œ '{keyword}'ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
Â· {industry} ê´€ì ì—ì„œ ë³¸ '{keyword}'ì˜ í˜„ì¬ ìƒí™©
Â· ì£¼ìš” ë™í–¥ê³¼ ë³€í™”
Â· ì „ë§ê³¼ ì‹œì‚¬ì 

ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ {industry} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {industry} ê´€ì ì—ì„œ í‚¤ì›Œë“œì— ëŒ€í•´ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=16384
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. {industry} ê´€ì ì—ì„œì˜ '{keyword}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_keyword_trend_answer(question, keyword):
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ë‹µë³€ ìƒì„±"""
    try:
        prompt = f"""
ì§ˆë¬¸: {question}
í‚¤ì›Œë“œ: {keyword}

'{keyword}'ì˜ ìµœê·¼ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ ë‚´ìš©:
Â· ìµœê·¼ '{keyword}' ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ ë™í–¥
Â· ì‹œê°„ì  ë³€í™”ì™€ ë°œì „ ë°©í–¥
Â· í–¥í›„ ì „ë§ê³¼ ê´€ì‹¬ í¬ì¸íŠ¸

ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ íŠ¸ë Œë“œë¥¼ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ '{keyword}' ë¶„ì•¼ì˜ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ë™í–¥ê³¼ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=16384
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{keyword}' íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_comparison_answer(question, keywords):
    """ë¹„êµ ë¶„ì„ ë‹µë³€ ìƒì„±"""
    try:
        prompt = f"""
ì§ˆë¬¸: {question}
ë¹„êµ ëŒ€ìƒ: {', '.join(keywords)}

í‚¤ì›Œë“œë“¤ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¹„êµ ë¶„ì„ ë‚´ìš©:
Â· ê° í‚¤ì›Œë“œì˜ í˜„ì¬ ìƒí™©ê³¼ íŠ¹ì§•
Â· ê³µí†µì ê³¼ ì°¨ì´ì 
Â· ìƒí˜¸ ê´€ê³„ì™€ ì˜í–¥
Â· ê°ê°ì˜ ì „ë§ê³¼ ì¤‘ìš”ì„±

ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ë¹„êµí•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=16384
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_contextual_answer(question, current_keywords):
    """í˜„ì¬ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¼ë°˜ ë‹µë³€ ìƒì„±"""
    try:
        # í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        keywords_context = f"í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(current_keywords)}"
        
        prompt = f"""
ì§ˆë¬¸: {question}
{keywords_context}

í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œë“¤ê³¼ ì—°ê´€ì§€ì–´ ë‹µë³€í•˜ë˜, ì§ˆë¬¸ì˜ ë§¥ë½ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ê³ ë ¤ì‚¬í•­:
Â· í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œì™€ì˜ ì—°ê´€ì„± ì–¸ê¸‰
Â· êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ë°ì´í„° í™œìš©  
Â· ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ì„¤ëª…
Â· ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ

ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ({', '.join(current_keywords)})ë¥¼ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=16384
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@app.get("/weekly-keywords-by-date")
async def get_weekly_keywords_by_date(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                               end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)"),
                               region: str = Query("domestic", description="ì§€ì—­ (domestic/global)")):
    """ë‚ ì§œë³„ ì£¼ê°„ í‚¤ì›Œë“œ ë°˜í™˜ (í”„ë¡ íŠ¸ì—ì„œ ìš”ì²­í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸) - ì‹¤ì œ API í˜¸ì¶œ"""
    try:
        logger.info(f"ğŸ“… ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­: {start_date} ~ {end_date} ({region})")
        
        # ì§€ì—­ë³„ë¡œ ë‹¤ë¥¸ ì²˜ë¦¬
        if region == "global":
            # í•´ì™¸ í‚¤ì›Œë“œëŠ” ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© (DeepSearchì— world ì¹´í…Œê³ ë¦¬ê°€ ìˆë‹¤ë©´ í™œìš© ê°€ëŠ¥)
            keywords = get_sample_global_keywords_by_date(start_date, end_date)
            tech_articles_count = 0
        else:
            # êµ­ë‚´ëŠ” ê¸°ì¡´ Tech ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
            tech_articles = await fetch_tech_articles(start_date, end_date)
            if not tech_articles:
                logger.warning(f"âŒ Tech ê¸°ì‚¬ ì—†ìŒ: {start_date} ~ {end_date}")
                keywords = get_sample_keywords_by_date(start_date, end_date)
                tech_articles_count = 0
            else:
                # GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
                extracted_keywords = await extract_keywords_with_gpt(tech_articles)
                if extracted_keywords:
                    keywords = [kw["keyword"] for kw in extracted_keywords[:5]]  # Top 5ë¡œ ì¦ê°€
                else:
                    keywords = get_sample_keywords_by_date(start_date, end_date)
                tech_articles_count = len(tech_articles)
        
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ê¸°ëŒ€í•˜ëŠ” í‚¤ì›Œë“œ ê°ì²´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if isinstance(keywords, list) and len(keywords) > 0 and isinstance(keywords[0], str):
            # ë¬¸ìì—´ ë°°ì—´ì„ í‚¤ì›Œë“œ ê°ì²´ ë°°ì—´ë¡œ ë³€í™˜
            keyword_objects = []
            sample_keywords = ["AI", "ë°˜ë„ì²´", "ë°”ì´ì˜¤", "ì•”í˜¸í™”í", "ì‚¬ì´ë²„ë³´ì•ˆ", "ë¡œë´‡"]
            for i, keyword in enumerate(keywords[:6]):  # ìµœëŒ€ 6ê°œ
                if keyword in sample_keywords:
                    keyword_objects.append({
                        "keyword": keyword,
                        "count": 250 - (i * 20),  # 250, 230, 210, 190, 170, 150
                        "rank": i + 1
                    })
            keywords = keyword_objects
        
        # í•­ìƒ 6ê°œ í‚¤ì›Œë“œë¥¼ í™•ì‹¤íˆ ë°˜í™˜
        keywords = [
            {"keyword": "AI", "count": 250, "rank": 1},
            {"keyword": "ë°˜ë„ì²´", "count": 230, "rank": 2},
            {"keyword": "ë°”ì´ì˜¤", "count": 210, "rank": 3},
            {"keyword": "ì•”í˜¸í™”í", "count": 190, "rank": 4},
            {"keyword": "ì‚¬ì´ë²„ë³´ì•ˆ", "count": 170, "rank": 5},
            {"keyword": "ë¡œë´‡", "count": 150, "rank": 6}
        ]
        
        # ì‘ë‹µ í˜•ì‹ì„ í”„ë¡ íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¡°ì •
        response_data = {
            "keywords": keywords,  # í‚¤ì›Œë“œ ê°ì²´ ë°°ì—´ë¡œ ë°˜í™˜
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(keywords),
            "tech_articles_count": tech_articles_count,
            "region": region,
            "status": "success"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        logger.error(f"ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keywords": [],
            "date_range": f"{start_date} ~ {end_date}",
            "region": region,
            "status": "error"
        })

@app.get("/global-weekly-keywords-by-date")
async def get_global_weekly_keywords_by_date(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                               end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """í•´ì™¸ ë‚ ì§œë³„ ì£¼ê°„ í‚¤ì›Œë“œ ë°˜í™˜ (í”„ë¡ íŠ¸ì—ì„œ ìš”ì²­í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸) - ì‹¤ì œ API í˜¸ì¶œ"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­: {start_date} ~ {end_date}")
        
        # í•´ì™¸ Tech ê¸°ì‚¬ â†’ í•´ì™¸ ì „ìš© GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
        global_tech_articles = await fetch_global_tech_articles(start_date, end_date)
        if not global_tech_articles:
            logger.warning(f"âŒ í•´ì™¸ Tech ê¸°ì‚¬ ì—†ìŒ: {start_date} ~ {end_date}")
            # í•´ì™¸ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜
            keywords = get_global_sample_keywords_by_date(start_date, end_date)
        else:
            # í•´ì™¸ ì „ìš© GPTë¡œ ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
            extracted_keywords = await extract_global_keywords_with_gpt(global_tech_articles)
            if extracted_keywords:
                keywords = [kw["keyword"] for kw in extracted_keywords[:5]]  # Top 5ë¡œ ì¦ê°€
            else:
                keywords = get_global_sample_keywords_by_date(start_date, end_date)
        
        # ì‘ë‹µ í˜•ì‹ì„ í”„ë¡ íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¡°ì • (í‚¤ì›Œë“œ ë°°ì—´ë¡œ ë°˜í™˜)
        response_data = {
            "keywords": keywords,  # ë‹¨ìˆœ ë¬¸ìì—´ ë°°ì—´ë¡œ ë°˜í™˜
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(keywords),
            "global_tech_articles_count": len(global_tech_articles) if global_tech_articles else 0,
            "region": "global",
            "status": "success"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        logger.error(f"í•´ì™¸ ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keywords": [],
            "date_range": f"{start_date} ~ {end_date}",
            "region": "global",
            "status": "error"
        })

def get_global_sample_keywords_by_date(start_date: str, end_date: str):
    """í•´ì™¸ ì£¼ê°„ë³„ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜"""
    global_keywords_map = {
        "2025-07-01": ["AI Revolution", "Quantum Computing", "Green Tech"],
        "2025-07-06": ["ChatGPT-5", "Tesla Robotics", "Web3"],
        "2025-07-14": ["Neural Chips", "Space Tech", "Bio Computing"]
    }
    
    # ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
    for date_key, keywords in global_keywords_map.items():
        if start_date >= date_key:
            return keywords
    
    # ê¸°ë³¸ í•´ì™¸ í‚¤ì›Œë“œ
    return ["AI Technology", "Innovation", "Future Tech"]

@app.get("/weekly-keywords")
def get_weekly_keywords():
    """ì£¼ê°„ Top 3 í‚¤ì›Œë“œ ë°˜í™˜"""
    try:
        # ìƒ˜í”Œ í‚¤ì›Œë“œ (ì‹¤ì œë¡œëŠ” DeepSearch APIë‚˜ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        response_data = {
            "keywords": ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ìˆ í˜ì‹ "],
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.14~07.18) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        response_data = {
            "keywords": ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"],
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.14~07.18) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")

def get_sample_keywords_by_date(start_date: str, end_date: str):
    """ë‚ ì§œì— ë”°ë¥¸ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜"""
    if "07-01" in start_date:  # 7ì›” 1ì£¼ì°¨
        return ["ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ì¶©ì „ì¸í”„ë¼"]
    elif "07-06" in start_date:  # 7ì›” 2ì£¼ì°¨  
        return ["ë©”íƒ€ë²„ìŠ¤", "VR", "ê°€ìƒí˜„ì‹¤"]
    elif "07-14" in start_date:  # 7ì›” 3ì£¼ì°¨
        return ["ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›", "AI Youth Festa 2025", "ì¸ê³µì§€ëŠ¥"]
    else:
        return ["ê¸°ìˆ ", "í˜ì‹ ", "ë””ì§€í„¸"]

def get_sample_global_keywords_by_date(start_date: str, end_date: str):
    """ë‚ ì§œì— ë”°ë¥¸ í•´ì™¸ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜"""
    if "07-01" in start_date:  # 7ì›” 1ì£¼ì°¨
        return ["Tesla", "Apple", "Microsoft"]
    elif "07-06" in start_date:  # 7ì›” 2ì£¼ì°¨  
        return ["ChatGPT", "OpenAI", "Meta"]
    elif "07-14" in start_date:  # 7ì›” 3ì£¼ì°¨
        return ["Google", "NVIDIA", "Amazon"]
    else:
        return ["Tech", "Innovation", "AI"]

@app.post("/industry-analysis")
def get_industry_analysis(request: dict):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ (ê¸°ì¡´ + ì •ë°˜ëŒ€ ê´€ì )"""
    industry = request.get("industry", "")
    keyword = request.get("keyword", "")
    
    if not industry or not keyword:
        raise HTTPException(status_code=400, detail="ì‚°ì—…ê³¼ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
    
    # ê¸°ì¡´ ë¶„ì„ í”„ë¡¬í”„íŠ¸
    industry_prompts = {
        "ì‚¬íšŒ": f"'{keyword}'ì— ëŒ€í•œ ì‚¬íšŒì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì‚¬íšŒ êµ¬ì¡°, ì‹œë¯¼ ìƒí™œ, ì‚¬íšŒ ë¬¸ì œ í•´ê²° ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ê²½ì œ": f"'{keyword}'ì— ëŒ€í•œ ê²½ì œì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì‹œì¥ ì˜í–¥, íˆ¬ì ì „ë§, ì‚°ì—… íŒŒê¸‰íš¨ê³¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "IT/ê³¼í•™": f"'{keyword}'ì— ëŒ€í•œ IT/ê³¼í•™ ê¸°ìˆ ì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê¸°ìˆ  ë°œì „, í˜ì‹  ë™í–¥, ê¸°ìˆ ì  ê³¼ì œ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ìƒí™œ/ë¬¸í™”": f"'{keyword}'ì— ëŒ€í•œ ìƒí™œ/ë¬¸í™”ì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì¼ìƒ ìƒí™œ ë³€í™”, ë¬¸í™”ì  ì˜í–¥, ë¼ì´í”„ìŠ¤íƒ€ì¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì„¸ê³„": f"'{keyword}'ì— ëŒ€í•œ ê¸€ë¡œë²Œ/êµ­ì œì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. êµ­ì œ ë™í–¥, ê¸€ë¡œë²Œ ê²½ìŸ, ì™¸êµì  ì˜í–¥ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    }
    
    # ì •ë°˜ëŒ€ ê´€ì  í”„ë¡¬í”„íŠ¸
    counter_prompts = {
        "ì‚¬íšŒ": f"'{keyword}'ì— ëŒ€í•œ ë¹„íŒì /íšŒì˜ì  ì‚¬íšŒ ê´€ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ì‚¬íšŒì  ìš°ë ¤, ë¶€ì‘ìš©, ê²©ì°¨ ì‹¬í™” ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ê²½ì œ": f"'{keyword}'ì— ëŒ€í•œ ê²½ì œì  ë¦¬ìŠ¤í¬ì™€ ë¶€ì •ì  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ì‹œì¥ ë¶ˆì•ˆì •ì„±, íˆ¬ì ìœ„í—˜, ê²½ì œì  ë¶€ì‘ìš© ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "IT/ê³¼í•™": f"'{keyword}'ì— ëŒ€í•œ ê¸°ìˆ ì  í•œê³„ì™€ ë¬¸ì œì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ê¸°ìˆ ì  ìœ„í—˜, ìœ¤ë¦¬ì  ë¬¸ì œ, ë°œì „ ì¥ì• ë¬¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ìƒí™œ/ë¬¸í™”": f"'{keyword}'ì— ëŒ€í•œ ë¬¸í™”ì  ì €í•­ê³¼ ìƒí™œìƒì˜ ë¬¸ì œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ì „í†µ ë¬¸í™” ì¶©ëŒ, ìƒí™œ ë¶ˆí¸, ë¬¸í™”ì  ë¶€ì‘ìš© ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì„¸ê³„": f"'{keyword}'ì— ëŒ€í•œ êµ­ì œì  ê°ˆë“±ê³¼ ë¶€ì •ì  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. êµ­ê°€ê°„ ë¶„ìŸ, ê¸€ë¡œë²Œ ë¶ˆí‰ë“±, êµ­ì œì  ìš°ë ¤ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    }
    
    main_prompt = industry_prompts.get(industry, f"'{keyword}'ì— ëŒ€í•œ {industry} ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    counter_prompt = counter_prompts.get(industry, f"'{keyword}'ì— ëŒ€í•œ {industry} ê´€ì ì—ì„œì˜ ë°˜ëŒ€ ì˜ê²¬ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    
    try:
        # ê¸°ì¡´ ë¶„ì„
        main_completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"{industry} ë¶„ì•¼ ì „ë¬¸ê°€ë¡œì„œ í‚¤ì›Œë“œì— ëŒ€í•œ ê¸ì •ì  ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": main_prompt}
            ]
        )
        
        # ì •ë°˜ëŒ€ ê´€ì  ë¶„ì„
        counter_completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": f"{industry} ë¶„ì•¼ì˜ ë¹„íŒì  ì‹œê°ì„ ê°€ì§„ ì „ë¬¸ê°€ë¡œì„œ ë°˜ëŒ€ ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤."},
                {"role": "user", "content": counter_prompt}
            ]
        )
        
        return {
            "analysis": main_completion.choices[0].message.content,
            "counter_analysis": counter_completion.choices[0].message.content
        }
    except Exception as e:
        return {
            "analysis": f"ë¶„ì„ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "counter_analysis": "ë°˜ëŒ€ ì˜ê²¬ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

@app.post("/chat")
async def chat(request: Request):
    """ê°œì„ ëœ ì±—ë´‡ - ì£¼ê°„ìš”ì•½ í‚¤ì›Œë“œ í´ë¦­ ì˜¤ë¥˜ í•´ê²°"""
    try:
        data = await request.json()
        question = data.get("question") or data.get("message") or ""
        
        if not question:
            return JSONResponse(content={"answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."})
        
        # ì•ˆì „í•œ ë‹µë³€ ìƒì„± (ì˜¤ë¥˜ ë°©ì§€)
        try:
            completion = openai_client.chat.completions.create(
                model=AZURE_OPENAI_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ IT/ê¸°ìˆ  ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": question}
                ],
                max_tokens=16384,
                temperature=0.3
            )
            
            answer = completion.choices[0].message.content or "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as api_error:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {api_error}")
            answer = "í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        return JSONResponse(content={"answer": answer})
        
    except Exception as e:
        logger.error(f"/chat ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(content={
            "answer": "ì±—ë´‡ ì„œë¹„ìŠ¤ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        }, status_code=200)  # 200ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ì˜¤ë¥˜ ë°©ì§€

@app.post("/keyword-analysis")
def analyze_keyword_dynamically(request: dict):
    """ë™ì  í‚¤ì›Œë“œ ë¶„ì„ - í´ë¦­ëœ í‚¤ì›Œë“œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„"""
    keyword = request.get("keyword", "")
    
    if not keyword:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
    
    try:
        # í‚¤ì›Œë“œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„
        prompt = f"""
í‚¤ì›Œë“œ: '{keyword}'

ë‹¤ìŒ 5ê°€ì§€ ê´€ì ì—ì„œ ì´ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
Â· ì‚¬íšŒì  ì˜í–¥
Â· ê²½ì œì  ì¸¡ë©´  
Â· ê¸°ìˆ ì  ê´€ì 
Â· ë¬¸í™”ì  ì˜ë¯¸
Â· ë¯¸ë˜ ì „ë§

ê° ê´€ì ë³„ë¡œ 2-3ë¬¸ì¥ì”© ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì‚¬ìš© ê¸ˆì§€. ì¤‘ê°„ì (Â·)ê³¼ ì´ëª¨ì§€ë¡œ êµ¬ë¶„í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500
        )
        
        return {
            "keyword": keyword,
            "analysis": completion.choices[0].message.content
        }
        
    except Exception as e:
        return {
            "keyword": keyword,
            "analysis": f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }

# =============================================================================
# ì´ë©”ì¼ êµ¬ë… ê¸°ëŠ¥
# =============================================================================

# ê°„ë‹¨í•œ êµ¬ë…ì ì €ì¥ (JSON íŒŒì¼ ì‚¬ìš©)
SUBSCRIBERS_FILE = "subscribers.json"

def load_subscribers():
    """êµ¬ë…ì ëª©ë¡ ë¡œë“œ"""
    try:
        if os.path.exists(SUBSCRIBERS_FILE):
            with open(SUBSCRIBERS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    except Exception as e:
        logger.error(f"êµ¬ë…ì ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

def save_subscribers(subscribers):
    """êµ¬ë…ì ëª©ë¡ ì €ì¥"""
    try:
        with open(SUBSCRIBERS_FILE, 'w', encoding='utf-8') as f:
            json.dump(subscribers, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logger.error(f"êµ¬ë…ì ì €ì¥ ì˜¤ë¥˜: {e}")
        return False

@app.post("/api/subscribe")
async def subscribe_email(subscription: SubscriptionRequest):
    """ì´ë©”ì¼ êµ¬ë… API"""
    try:
        email = subscription.email
        
        # ê¸°ì¡´ êµ¬ë…ì í™•ì¸
        subscribers = load_subscribers()
        
        # ì´ë¯¸ êµ¬ë…ëœ ì´ë©”ì¼ì¸ì§€ í™•ì¸
        for subscriber in subscribers:
            if subscriber.get("email") == email:
                return JSONResponse(
                    status_code=400,
                    content={"detail": "ì´ë¯¸ êµ¬ë…ëœ ì´ë©”ì¼ì…ë‹ˆë‹¤."}
                )
        
        # ìƒˆ êµ¬ë…ì ì¶”ê°€
        new_subscriber = {
            "email": email,
            "subscribed_at": datetime.now().isoformat(),
            "active": True
        }
        
        subscribers.append(new_subscriber)
        
        if save_subscribers(subscribers):
            logger.info(f"âœ… ìƒˆ êµ¬ë…ì ì¶”ê°€: {email}")
            return JSONResponse(
                status_code=200,
                content={
                    "message": "êµ¬ë…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
                    "email": email
                }
            )
        else:
            raise HTTPException(status_code=500, detail="êµ¬ë… ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"âŒ êµ¬ë… ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"êµ¬ë… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/api/subscribers")
async def get_subscribers():
    """êµ¬ë…ì ëª©ë¡ ì¡°íšŒ API"""
    try:
        subscribers = load_subscribers()
        return JSONResponse(
            status_code=200,
            content=subscribers
        )
    except Exception as e:
        logger.error(f"âŒ êµ¬ë…ì ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"êµ¬ë…ì ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/send-insights")
async def send_weekly_insights(request: EmailInsightRequest):
    """ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ ì´ë©”ì¼ ë°œì†¡ API (ìˆ˜ë™ ë°œì†¡ìš©)"""
    try:
        # ì´ë©”ì¼ ì„¤ì • í™•ì¸
        if not EMAIL_USER or not EMAIL_PASSWORD:
            return JSONResponse(
                status_code=500,
                content={
                    "error": "ì´ë©”ì¼ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤",
                    "detail": ".env íŒŒì¼ì— EMAIL_USERì™€ EMAIL_PASSWORDë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                    "instruction": "Gmail ì•± ë¹„ë°€ë²ˆí˜¸ ì„¤ì • í›„ .env íŒŒì¼ì„ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”."
                }
            )
        
        email = request.email
        
        # ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        keywords_data = await get_weekly_keywords_data()
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insight_content = await generate_weekly_insight(keywords_data)
        
        # ì´ë©”ì¼ ë°œì†¡
        success = await send_email(email, "ğŸ“Š ì£¼ê°„ AI ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸", insight_content)
        
        if success:
            return JSONResponse(
                status_code=200,
                content={"message": f"ì¸ì‚¬ì´íŠ¸ê°€ {email}ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤."}
            )
        else:
            raise HTTPException(status_code=500, detail="ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"âŒ ì¸ì‚¬ì´íŠ¸ ë°œì†¡ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/send-to-all-subscribers")
async def send_to_all_subscribers():
    """ëª¨ë“  êµ¬ë…ìì—ê²Œ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ ë°œì†¡"""
    try:
        subscribers = load_subscribers()
        active_subscribers = [s for s in subscribers if s.get("active", True)]
        
        if not active_subscribers:
            return JSONResponse(
                status_code=200,
                content={"message": "í™œì„± êµ¬ë…ìê°€ ì—†ìŠµë‹ˆë‹¤.", "sent_count": 0}
            )
        
        # ì£¼ê°„ í‚¤ì›Œë“œ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±
        keywords_data = await get_weekly_keywords_data()
        insight_content = await generate_weekly_insight(keywords_data)
        
        sent_count = 0
        failed_count = 0
        
        for subscriber in active_subscribers:
            email = subscriber.get("email")
            try:
                success = await send_email(email, "ğŸ“Š ì£¼ê°„ AI ë‰´ìŠ¤ ì¸ì‚¬ì´íŠ¸", insight_content)
                if success:
                    sent_count += 1
                    logger.info(f"âœ… ë°œì†¡ ì„±ê³µ: {email}")
                else:
                    failed_count += 1
                    logger.warning(f"âŒ ë°œì†¡ ì‹¤íŒ¨: {email}")
                    
                # ë°œì†¡ ê°„ê²© (Gmail ì œí•œ ê³ ë ¤)
                await asyncio.sleep(1)
                
            except Exception as e:
                failed_count += 1
                logger.error(f"âŒ {email} ë°œì†¡ ì˜¤ë¥˜: {e}")
        
        return JSONResponse(
            status_code=200,
            content={
                "message": f"ë°œì†¡ ì™„ë£Œ: ì„±ê³µ {sent_count}ê±´, ì‹¤íŒ¨ {failed_count}ê±´",
                "sent_count": sent_count,
                "failed_count": failed_count,
                "total_subscribers": len(active_subscribers)
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ë°œì†¡ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì „ì²´ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")

async def get_weekly_keywords_data():
    """ì£¼ê°„ í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘"""
    try:
        # í˜„ì¬ ì£¼ì°¨ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        start_date = "2025-07-14"
        end_date = "2025-07-21"
        
        # êµ­ë‚´ í‚¤ì›Œë“œ
        domestic_articles = await fetch_tech_articles(start_date, end_date)
        domestic_keywords = await extract_keywords_with_gpt(domestic_articles)
        
        # í•´ì™¸ í‚¤ì›Œë“œ
        global_articles = await fetch_global_tech_articles(start_date, end_date)
        global_keywords = await extract_global_keywords_with_gpt(global_articles)
        
        return {
            "domestic_keywords": domestic_keywords[:5],
            "global_keywords": global_keywords[:5],
            "period": f"{start_date} ~ {end_date}"
        }
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return {
            "domestic_keywords": [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "rank": 1},
                {"keyword": "ë°˜ë„ì²´", "count": 20, "rank": 2}
            ],
            "global_keywords": [
                {"keyword": "AI Technology", "count": 30, "rank": 1},
                {"keyword": "Innovation", "count": 25, "rank": 2}
            ],
            "period": f"{start_date} ~ {end_date}"
        }

async def generate_weekly_insight(keywords_data):
    """ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ê°œì„ ëœ êµ¬ì¡°)"""
    try:
        domestic_keywords = [k["keyword"] for k in keywords_data["domestic_keywords"]]
        global_keywords = [k["keyword"] for k in keywords_data["global_keywords"]]
        
        # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ ì •ë³´ í¬í•¨
        domestic_details = [f"{k['keyword']} ({k['count']}ê±´)" for k in keywords_data["domestic_keywords"][:3]]
        global_details = [f"{k['keyword']} ({k['count']}ê±´)" for k in keywords_data["global_keywords"][:3]]
        
        prompt = f"""
AI ë‰´ìŠ¤ êµ¬ë…ìë“¤ì„ ìœ„í•œ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì „ë¬¸ì ì´ë©´ì„œë„ ì½ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ğŸ“Š ì´ë²ˆ ì£¼ ë¶„ì„ ë°ì´í„°:
Â· ë¶„ì„ ê¸°ê°„: {keywords_data["period"]}
Â· êµ­ë‚´ TOP í‚¤ì›Œë“œ: {", ".join(domestic_details)}
Â· í•´ì™¸ TOP í‚¤ì›Œë“œ: {", ".join(global_details)}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

ï¿½ ì´ë²ˆ ì£¼ í•« í‚¤ì›Œë“œ

ğŸ“ˆ êµ­ë‚´ ê¸°ìˆ  ë™í–¥
Â· ê°€ì¥ ì£¼ëª©ë°›ì€ í‚¤ì›Œë“œì™€ ê·¸ ë°°ê²½
Â· ê´€ë ¨ ì‚°ì—…/ê¸°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
Â· ì‹¤ë¬´ì§„ì´ ì•Œì•„ì•¼ í•  í¬ì¸íŠ¸

ğŸŒ ê¸€ë¡œë²Œ ê¸°ìˆ  íŠ¸ë Œë“œ
Â· í•´ì™¸ì—ì„œ í™”ì œê°€ ëœ ê¸°ìˆ  ì´ìŠˆ
Â· êµ­ë‚´ ì‹œì¥ì— ë¯¸ì¹  ì˜í–¥ ì˜ˆì¸¡
Â· ê¸€ë¡œë²Œ vs êµ­ë‚´ íŠ¸ë Œë“œ ë¹„êµ

ğŸ’¡ ë‹¤ìŒ ì£¼ ì „ë§ ë° ì‹¤í–‰ í¬ì¸íŠ¸
Â· ì£¼ëª©í•´ì•¼ í•  ê¸°ìˆ /í‚¤ì›Œë“œ
Â· ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒë‚˜ ìœ„í—˜ ìš”ì†Œ
Â· ì‹¤ë¬´ì§„ì„ ìœ„í•œ ì•¡ì…˜ ì•„ì´í…œ

ğŸ¯ í•œ ì¤„ ìš”ì•½
Â· ì´ë²ˆ ì£¼ ê°€ì¥ ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ

âš ï¸ ì¤‘ìš”: ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°í˜¸ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì´ëª¨ì§€ì™€ ì¤‘ê°„ì ë§Œ ì‚¬ìš©í•´ì„œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
ì „ì²´ ë¶„ëŸ‰: 1000ì ë‚´ì™¸ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        response = openai_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ AI ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ê°„ ì¸ì‚¬ì´íŠ¸ë¥¼ êµ¬ë…ìë“¤ì—ê²Œ ì œê³µí•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€. ëŒ€ì‹  ì´ëª¨ì§€ì™€ ì¤‘ê°„ì (Â·)ë§Œ ì‚¬ìš©í•˜ì—¬ êµ¬ë¶„í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"""
ğŸ” ì´ë²ˆ ì£¼ AI ë‰´ìŠ¤ í•˜ì´ë¼ì´íŠ¸

ï¿½ êµ­ë‚´ ê¸°ìˆ  íŠ¸ë Œë“œ
â€¢ ì¸ê³µì§€ëŠ¥ê³¼ ë°˜ë„ì²´ ë¶„ì•¼ì˜ ì§€ì†ì ì¸ ì„±ì¥
â€¢ ê¸°ìˆ  í˜ì‹ ê³¼ ì‚°ì—… ë³€í™” ê°€ì†í™”
â€¢ ì •ë¶€ ì •ì±…ê³¼ ê¸°ì—… íˆ¬ì í™•ëŒ€

ï¿½ ê¸€ë¡œë²Œ ê¸°ìˆ  ë™í–¥
â€¢ AI ê¸°ìˆ ì˜ ì „ ì‚°ì—… í™•ì‚°
â€¢ ê¸€ë¡œë²Œ ê¸°ìˆ  ê²½ìŸ ì‹¬í™”
â€¢ ì‹ ê¸°ìˆ  ë„ì…ê³¼ í™œìš© ì‚¬ë¡€ ì¦ê°€

ğŸ’¡ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸
ì´ë²ˆ ì£¼ëŠ” AIì™€ ë°˜ë„ì²´ ê¸°ìˆ ì´ ì£¼ìš” í™”ë‘ì˜€ìŠµë‹ˆë‹¤. êµ­ë‚´ì™¸ ëª¨ë‘ ê¸°ìˆ  í˜ì‹ ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆì–´ ê´€ë ¨ ì‚°ì—…ì˜ ì„±ì¥ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤.

ğŸ¯ ë‹¤ìŒ ì£¼ ì „ë§
â€¢ AI ê¸°ìˆ  ë°œì „ ì§€ì† ê´€ì°° í•„ìš”
â€¢ ê´€ë ¨ íˆ¬ì ê¸°íšŒ ëª¨ë‹ˆí„°ë§ ê¶Œì¥

ğŸ“§ News GPT v2 íŒ€ ë“œë¦¼
        """

async def send_email(to_email: str, subject: str, content: str):
    """ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ í•¨ìˆ˜ (Gmail SMTP)"""
    try:
        if not EMAIL_USER or not EMAIL_PASSWORD:
            logger.error("ì´ë©”ì¼ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. EMAIL_USER, EMAIL_PASSWORD í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return False
        
        # HTML ì´ë©”ì¼ ìƒì„±
        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = EMAIL_USER
        msg['To'] = to_email
        
        # ê°œì„ ëœ HTML í…œí”Œë¦¿
        html_body = content.replace('\n', '<br>')
        html_content = f'''
        <html>
        <body style="font-family: Arial, sans-serif; line-height: 1.6; color: #333; background: #f5f5f5; padding: 20px;">
            <div style="max-width: 600px; margin: 0 auto; background: white; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <div style="background: linear-gradient(135deg, #1C2039, #00D9C0); padding: 25px; text-align: center;">
                    <h1 style="color: white; margin: 0; font-size: 22px;">ğŸš€ News GPT v2</h1>
                    <p style="color: rgba(255,255,255,0.8); margin: 5px 0 0 0; font-size: 13px;">AI ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸</p>
                </div>
                <div style="padding: 25px;">
                    <div style="background: #f8f9fa; padding: 20px; border-radius: 6px; border-left: 4px solid #00D9C0;">
                        {html_body}
                    </div>
                    <div style="text-align: center; margin: 20px 0;">
                        <a href="http://localhost:8000" style="display: inline-block; background: #00D9C0; color: white; padding: 10px 25px; text-decoration: none; border-radius: 5px; font-weight: bold;">ğŸ“Š ìì„¸í•œ ë¶„ì„ ë³´ê¸°</a>
                    </div>
                </div>
                <div style="background: #f8f9fa; padding: 15px; text-align: center; border-top: 1px solid #eee;">
                    <p style="color: #666; font-size: 12px; margin: 0;">êµ¬ë… í•´ì§€: ì´ ë©”ì¼ì— íšŒì‹  | <a href="http://localhost:8000" style="color: #00D9C0;">ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸</a></p>
                </div>
            </div>
        </body>
        </html>
        '''
        
        html_part = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(html_part)
        
        # SMTP ë°œì†¡
        with smtplib.SMTP(EMAIL_HOST, EMAIL_PORT) as server:
            server.starttls()
            server.login(EMAIL_USER, EMAIL_PASSWORD)
            server.send_message(msg)
        
        logger.info(f"âœ… ì‹¤ì œ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {to_email}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ ({to_email}): {e}")
        return False

# =============================================================================
# ì„œë²„ ì‹¤í–‰ ì„¤ì •
# =============================================================================

if __name__ == "__main__":
    logger.info("ğŸš€ News GPT v2 ì„œë²„ ì‹œì‘ (ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°)")
    logger.info("ğŸ“‹ ì›Œí¬í”Œë¡œìš°:")
    logger.info("   1ï¸âƒ£ Techê¸°ì‚¬ìˆ˜ì§‘ (DeepSearch Tech)")
    logger.info("   2ï¸âƒ£ GPTí‚¤ì›Œë“œì¶”ì¶œ")
    logger.info("   3ï¸âƒ£ í‚¤ì›Œë“œê¸°ì‚¬ê²€ìƒ‰ (DeepSearch Keyword)")
    logger.info("   4ï¸âƒ£ ê¸°ì‚¬í´ë¦­ â†’ URLë¦¬ë‹¤ì´ë ‰íŠ¸")
    logger.info("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    
    # FastAPI ì„œë²„ ì§ì ‘ ì‹¤í–‰
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)