# News GPT v2 - ìƒˆë¡œìš´ êµ¬ì¡° (DeepSearch ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°)
import os
import re
import time
import logging
import requests
import hashlib
import uvicorn
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, RedirectResponse
from dotenv import load_dotenv
from openai import AzureOpenAI
from functools import wraps

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
logger = logging.getLogger("news_gpt_v2")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI(title="News GPT v2", description="AI ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ í”Œë«í¼")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
DEEPSEARCH_API_KEY = os.getenv("DEEPSEARCH_API_KEY")

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = AzureOpenAI(
    api_key=str(AZURE_OPENAI_API_KEY),
    api_version="2024-02-15-preview",
    azure_endpoint=str(AZURE_OPENAI_ENDPOINT)
)

# DeepSearch API URLs (êµ­ë‚´ + í•´ì™¸)
DEEPSEARCH_TECH_URL = "https://api-v2.deepsearch.com/v1/articles/tech"
DEEPSEARCH_KEYWORD_URL = "https://api-v2.deepsearch.com/v1/articles"
DEEPSEARCH_GLOBAL_TECH_URL = "https://api-v2.deepsearch.com/v1/global-articles"
DEEPSEARCH_GLOBAL_KEYWORD_URL = "https://api-v2.deepsearch.com/v1/global-articles"


# API í˜¸ì¶œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„° (ì„±ëŠ¥ ìµœì í™”)
def retry_on_exception(max_retries=1, delay=0.1, backoff=1.2, allowed_exceptions=(Exception,)):  # ë¹ ë¥¸ ì¬ì‹œë„
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            current_delay = delay
            while True:
                try:
                    return func(*args, **kwargs)
                except allowed_exceptions as e:
                    retries += 1
                    logger.warning(f"{func.__name__} ì‹¤íŒ¨({retries}/{max_retries}): {e}")
                    if retries >= max_retries:
                        logger.error(f"{func.__name__} ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {e}")
                        raise
                    time.sleep(current_delay)
                    current_delay *= backoff
        return wrapper
    return decorator

# =============================================================================
# ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° API ì—”ë“œí¬ì¸íŠ¸ë“¤
# =============================================================================

@app.get("/")
async def serve_home():
    """ë©”ì¸ í˜ì´ì§€ ì œê³µ"""
    return FileResponse("index.html")

# 1ë‹¨ê³„: Tech ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í”„ë¡ íŠ¸ì™€ ì—°ë™)
@app.get("/api/keywords")
async def get_weekly_keywords(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                            end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: Tech ê¸°ì‚¬ â†’ GPT í‚¤ì›Œë“œ ì¶”ì¶œ â†’ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ (Azure AI Search ì œê±°)"""
    try:
        logger.info(f"ğŸš€ ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
        
        # 1ë‹¨ê³„: DeepSearch Techì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë‚ ì§œ í¬í•¨)
        tech_articles = await fetch_tech_articles(start_date, end_date)
        if not tech_articles:
            return {"error": "Tech ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "keywords": [], "articles_count": 0}
        
        # 2ë‹¨ê³„: GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (Azure AI Search ê±´ë„ˆë›°ê¸°)
        extracted_keywords = await extract_keywords_with_gpt(tech_articles)
        if not extracted_keywords:
            return {"error": "í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨", "keywords": [], "articles_count": len(tech_articles)}
        
        # 3ë‹¨ê³„: ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ìš©)
        store_keywords_in_memory(extracted_keywords, start_date, end_date)
        
        return {
            "keywords": extracted_keywords,
            "date_range": f"{start_date} ~ {end_date}",
            "tech_articles_count": len(tech_articles),
            "workflow": "Techê¸°ì‚¬ â†’ GPTí‚¤ì›Œë“œì¶”ì¶œ (Azure AI Search ì œê±°)",
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}", exc_info=True)
        return {"error": str(e), "keywords": [], "articles_count": 0}

# 4ë‹¨ê³„: í‚¤ì›Œë“œ í´ë¦­ì‹œ ê´€ë ¨ ê¸°ì‚¬ ë…¸ì¶œ
@app.get("/api/keyword-articles/{keyword}")
async def get_keyword_articles(keyword: str, 
                              start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
                              end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """í‚¤ì›Œë“œ í´ë¦­ì‹œ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ë°˜í™˜"""
    try:
        logger.info(f"ğŸ” í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ - ê¸°ê°„: {start_date} ~ {end_date}")
        
        # DeepSearch í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ê¸°ì‚¬ ì°¾ê¸°
        articles = await search_articles_by_keyword(keyword, start_date, end_date)
        
        return {
            "keyword": keyword,
            "articles": articles,
            "total_count": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return {"error": str(e), "keyword": keyword, "articles": [], "total_count": 0}

# 5ë‹¨ê³„: ê¸°ì‚¬ í´ë¦­ì‹œ ì›ë³¸ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
@app.get("/api/redirect/{article_id}")
async def redirect_to_original(article_id: str):
    """ê¸°ì‚¬ í´ë¦­ì‹œ ì›ë³¸ URLë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    try:
        # ë©”ëª¨ë¦¬ë‚˜ ìºì‹œì—ì„œ article_idë¡œ ì›ë³¸ URL ì°¾ê¸°
        original_url = get_original_url_by_id(article_id)
        
        if original_url:
            logger.info(f"ğŸ”— ê¸°ì‚¬ ë¦¬ë‹¤ì´ë ‰íŠ¸: {article_id} â†’ {original_url}")
            return RedirectResponse(url=original_url, status_code=302)
        else:
            raise HTTPException(status_code=404, detail=f"ê¸°ì‚¬ ID '{article_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"ë¦¬ë‹¤ì´ë ‰íŠ¸ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸ (deprecated)
@app.get("/keyword-articles")
async def get_keyword_articles_legacy(
    keyword: str = Query(..., description="ê²€ìƒ‰í•  í‚¤ì›Œë“œ"),
    start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"),
    end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")
):
    """ë ˆê±°ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ API (í˜¸í™˜ì„±ìš©)"""
    try:
        articles = await search_articles_by_keyword(keyword, start_date, end_date)
        return {
            "keyword": keyword,
            "total": len(articles),
            "articles": articles,
            "period": f"{start_date} ~ {end_date}",
            "status": "success",
            "note": "ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” deprecatedë©ë‹ˆë‹¤. /api/keyword-articles/{keyword}ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        }
    except Exception as e:
        logger.error(f"/keyword-articles ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keyword": keyword,
            "articles": [],
            "total": 0
        })

@app.get("/api/global-keyword-articles/{keyword}")
async def get_global_keyword_articles(
    keyword: str, 
    start_date: str = Query("2025-07-14", description="ì‹œì‘ì¼"),
    end_date: str = Query("2025-07-18", description="ì¢…ë£Œì¼")
):
    """í•´ì™¸ í‚¤ì›Œë“œë³„ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ API"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰: '{keyword}' ({start_date} ~ {end_date})")
        
        # í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤í–‰
        articles = await search_global_keyword_articles(keyword, start_date, end_date)
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            "keyword": keyword,
            "articles": articles,
            "total": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "region": "global",
            "status": "success"
        }
        
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keyword": keyword,
            "articles": [],
            "total": 0,
            "region": "global"
        })

# =============================================================================
# ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° í•µì‹¬ í•¨ìˆ˜ë“¤
# =============================================================================

# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” Redisë‚˜ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© ê¶Œì¥)
articles_cache = {}  # {article_id: {url, title, content, ...}}
keywords_cache = {}  # {keyword: [article_ids]}

# 1ë‹¨ê³„: DeepSearch Techì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
@retry_on_exception(max_retries=1, delay=0.1, backoff=1.5, allowed_exceptions=(requests.RequestException,))
async def fetch_tech_articles(start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """DeepSearch Tech ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ìµœì†Œí•œì˜ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
        base_url = DEEPSEARCH_TECH_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 20  # 20ê°œë¡œ ì¤„ì—¬ì„œ ë¹ ë¥¸ ì²˜ë¦¬
        }
        
        logger.info(f"ï¿½ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        response = requests.get(base_url, params=params, timeout=5)  # 5ì´ˆë¡œ ë‹¨ì¶•
        logger.info(f"ï¿½ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles = []
        if "articles" in data:
            articles = data["articles"]
        elif "data" in data:
            articles = data["data"]
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        # ê¸°ì‚¬ ì •ê·œí™” ë° ID ìƒì„±
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]  # YYYY-MM-DD
                    else:
                        formatted_date = published_at[:10]  # ì²˜ìŒ 10ìë¦¬ë§Œ
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "content": article.get("summary", "") or article.get("content", ""),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "url": article.get("url", "") or article.get("content_url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": article.get("source", ""),
                "category": "tech"
            }
            
            # ìºì‹œì— ì €ì¥ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©)
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        logger.info(f"âœ… Tech ê¸°ì‚¬ {len(processed_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return processed_articles
        
    except Exception as e:
        logger.error(f"âŒ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# 1-2ë‹¨ê³„: í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ (Global)
@retry_on_exception(max_retries=1, delay=0.1, backoff=1.5, allowed_exceptions=(requests.RequestException,))
async def fetch_global_tech_articles(start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """DeepSearch Global APIì—ì„œ í•´ì™¸ Tech ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë¹ ë¥¸ ì²˜ë¦¬)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        # í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ì„ ìœ„í•œ URL êµ¬ì„±
        base_url = DEEPSEARCH_GLOBAL_TECH_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": "tech",  # í•´ì™¸ì—ì„œëŠ” tech í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 20  # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ 20ê°œë¡œ ì œí•œ
        }
        
        logger.info(f"ğŸŒ í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
        response = requests.get(base_url, params=params, timeout=5)  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
        logger.info(f"ğŸ“Š í•´ì™¸ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í•´ì™¸ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles = []
        if 'data' in data:
            articles = data['data']
        elif 'articles' in data:
            articles = data['articles']
        elif isinstance(data, list):
            articles = data
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í•´ì™¸ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        # ê¸°ì‚¬ ì •ê·œí™” ë° ID ìƒì„±
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]  # YYYY-MM-DD
                    else:
                        formatted_date = published_at[:10]  # ì²˜ìŒ 10ìë¦¬ë§Œ
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "content": article.get("summary", "") or article.get("content", "ë‚´ìš© ì—†ìŒ"),
                "url": article.get("content_url", "") or article.get("url", ""),
                "date": formatted_date,
                "source": "í•´ì™¸",
                "category": "global_tech"
            }
            
            # ìºì‹œì— ì €ì¥ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©)
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        logger.info(f"âœ… í•´ì™¸ Tech ê¸°ì‚¬ {len(processed_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return processed_articles
        
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ Tech ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# 2ë‹¨ê³„: GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
async def extract_keywords_with_gpt(articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """GPTë¥¼ ì‚¬ìš©í•´ ê¸°ì‚¬ë“¤ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ìµœì í™”ë¨)"""
    if not articles:
        logger.warning("âŒ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    try:
        # ìµœì í™”: ìƒìœ„ 10ê°œ ê¸°ì‚¬ë§Œ ë¶„ì„í•˜ê³  ì œëª©ë§Œ ì‚¬ìš©
        top_articles = articles[:10]
        titles_text = " ".join([article['title'][:50] for article in top_articles])
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì†ë„ í–¥ìƒ
        prompt = f"""ë‹¤ìŒ ITê¸°ìˆ  ë‰´ìŠ¤ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë§Œ ì¶”ì¶œí•˜ì„¸ìš”:
{titles_text}

í˜•ì‹: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ITê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=50,  # ë” ì§§ê²Œ
            temperature=0  # ì¼ê´€ì„± ìµœëŒ€í™”
        )
        
        
        keywords_text = response.choices[0].message.content or ""
        logger.info(f"ğŸš€ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords_text}")
        
        # ë¹ ë¥¸ í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        for i, item in enumerate(keywords_text.split(',')[:3], 1):  # ìµœëŒ€ 3ê°œ
            keyword = item.strip().replace('.', '').replace('1', '').replace('2', '').replace('3', '')
            keyword = re.sub(r'[^\wê°€-í£]', '', keyword)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            
            if keyword and 2 <= len(keyword) <= 10:
                keywords.append({
                    "keyword": keyword,
                    "count": 30 - (i * 5),
                    "rank": i
                })
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ (ë¹ˆ ê²°ê³¼ ì‹œ)
        if not keywords:
            keywords = [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "rank": 1},
                {"keyword": "ë°˜ë„ì²´", "count": 20, "rank": 2},
                {"keyword": "í´ë¼ìš°ë“œ", "count": 15, "rank": 3}
            ]
        
        return keywords[:3]  # ìµœëŒ€ 3ê°œ ë°˜í™˜
        
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return [
            {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "rank": 1},
            {"keyword": "ë°˜ë„ì²´", "count": 20, "rank": 2},
            {"keyword": "í´ë¼ìš°ë“œ", "count": 15, "rank": 3}
        ]

# 3ë‹¨ê³„: í‚¤ì›Œë“œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
def store_keywords_in_memory(keywords: List[Dict[str, Any]], start_date: str, end_date: str):
    """ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤"""
    for keyword_data in keywords:
        keyword = keyword_data['keyword']
        keywords_cache[keyword] = {
            "keyword_data": keyword_data,
            "date_range": f"{start_date}~{end_date}",
            "cached_at": time.time()
        }
    logger.info(f"ğŸ“ {len(keywords)}ê°œ í‚¤ì›Œë“œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥ ì™„ë£Œ")

# 4ë‹¨ê³„: í‚¤ì›Œë“œë¡œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
async def search_articles_by_keyword(keyword: str, start_date: str, end_date: str) -> List[Dict[str, Any]]:
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ DeepSearchì—ì„œ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return []
    
    try:
        # ì‚¬ìš©ì ì œê³µ ì˜ˆì‹œ URL êµ¬ì¡° ì‚¬ìš©
        base_url = DEEPSEARCH_KEYWORD_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": keyword,  # ì‚¬ìš©ì ì˜ˆì‹œì— ë§ì¶° keyword íŒŒë¼ë¯¸í„° ì‚¬ìš©
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 15  # ê¸°ì‚¬ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ ë¹ ë¥¸ ì‘ë‹µ
        }
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... URL: {base_url}")
        logger.info(f"ğŸ” íŒŒë¼ë¯¸í„°: {params}")
        response = requests.get(base_url, params=params, timeout=3)  # 3ì´ˆë¡œ ë‹¨ì¶•
        logger.info(f"ğŸ” ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ í‚¤ì›Œë“œ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            logger.error(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles = []
        if "articles" in data:
            articles = data["articles"]
        elif "data" in data:
            articles = data["data"]
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        # ê¸°ì‚¬ ì •ê·œí™” ë° ìºì‹œ ì €ì¥
        processed_articles = []
        for article in articles:
            article_id = generate_article_id(article)
            
            # ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ê°œì„ 
            published_at = article.get("published_at", "")
            formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            if published_at:
                try:
                    if "T" in published_at:
                        formatted_date = published_at.split("T")[0]  # YYYY-MM-DD
                    else:
                        formatted_date = published_at[:10]  # ì²˜ìŒ 10ìë¦¬ë§Œ
                except:
                    formatted_date = "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
            processed_article = {
                "id": article_id,
                "title": article.get("title", "ì œëª© ì—†ìŒ"),
                "summary": (article.get("summary", "") or article.get("content", ""))[:150] + "..." if article.get("summary") or article.get("content") else "ìš”ì•½ ì •ë³´ ì—†ìŒ",
                "content": article.get("summary", "") or article.get("content", ""),
                "url": article.get("url", "") or article.get("content_url", ""),
                "date": formatted_date,
                "published_at": published_at,
                "source": article.get("source", ""),
                "keyword": keyword,
                "relevance_score": calculate_relevance_score(article, keyword)
            }
            
            # ìºì‹œì— ì €ì¥ (URL ë¦¬ë‹¤ì´ë ‰íŠ¸ìš©)
            articles_cache[article_id] = processed_article
            processed_articles.append(processed_article)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        processed_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        logger.info(f"âœ… í‚¤ì›Œë“œ '{keyword}' ê´€ë ¨ ê¸°ì‚¬ {len(processed_articles)}ê°œ ê²€ìƒ‰ ì™„ë£Œ")
        return processed_articles[:15]  # ìƒìœ„ 15ê°œë§Œ ë°˜í™˜
        
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
        return []

# 5ë‹¨ê³„: ê¸°ì‚¬ IDë¡œ ì›ë³¸ URL ì°¾ê¸°
def get_original_url_by_id(article_id: str) -> Optional[str]:
    """ê¸°ì‚¬ IDë¡œ ì›ë³¸ URLì„ ì°¾ìŠµë‹ˆë‹¤"""
    article = articles_cache.get(article_id)
    if article:
        return article.get("url")
    return None

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def generate_article_id(article: Dict[str, Any]) -> str:
    """ê¸°ì‚¬ ì •ë³´ë¡œ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    content = f"{article.get('title', '')}{article.get('url', '')}{article.get('published_at', '')}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]

def calculate_relevance_score(article: Dict[str, Any], keyword: str) -> float:
    """ê¸°ì‚¬ì™€ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤"""
    title = article.get("title", "").lower()
    content = article.get("summary", "") or article.get("content", "")
    content = content.lower()
    keyword_lower = keyword.lower()
    
    score = 0.0
    
    # ì œëª©ì— í‚¤ì›Œë“œ í¬í•¨ì‹œ ë†’ì€ ì ìˆ˜
    if keyword_lower in title:
        score += 10.0
    
    # ë‚´ìš©ì— í‚¤ì›Œë“œ í¬í•¨ì‹œ ì ìˆ˜ ì¶”ê°€
    content_count = content.count(keyword_lower)
    score += content_count * 2.0
    
    # ìµœê·¼ ê¸°ì‚¬ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    pub_date = article.get("published_at", "")
    if "2025-07" in pub_date:
        score += 5.0
    
    return score

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°ì— ë§ê²Œ ìˆ˜ì •...
async def search_keyword_articles(keyword: str, start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ (ë” ì •í™•í•œ ê²€ìƒ‰, í•œêµ­ì–´ ìš°ì„ , ë‚ ì§œ í•„í„°ë§)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    try:
        # IT í‚¤ì›Œë“œì— ëŒ€í•´ì„œë§Œ ì •í™•í•œ ê²€ìƒ‰
        if keyword == "IT":
            search_terms = ["IT", "ì •ë³´ê¸°ìˆ ", "Information Technology"]
        else:
            search_terms = [keyword]
        articles = []
        for search_term in search_terms:
            try:
                # DeepSearch APIì—ì„œ ê²½ì œ,ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ + í‚¤ì›Œë“œ ê²€ìƒ‰
                url = f"https://api-v2.deepsearch.com/v1/articles/economy,tech"
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "date_from": start_date,
                    "date_to": end_date,
                    "q": search_term  # í‚¤ì›Œë“œ ê²€ìƒ‰ ì¶”ê°€
                }
                response = requests.get(url, params=params, timeout=5)
                response.raise_for_status()
                data = response.json()
                for item in data.get("data", []):
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    if article_date:
                        title = item.get("title", "")
                        content = item.get("summary", "") or item.get("content", "")
                        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°(ê°„ë‹¨í™”)
                        relevance_score = 1.0 if keyword in title or keyword in content else 0.5
                        korean_chars = len([c for c in title + content if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3])
                        total_chars = len(title + content)
                        is_korean = korean_chars / max(total_chars, 1) > 0.3
                        articles.append({
                            "title": title,
                            "content": content,
                            "source_url": item.get("content_url", "") or item.get("url", ""),
                            "date": article_date,
                            "relevance_score": relevance_score,
                            "search_term": search_term,
                            "is_korean": is_korean
                        })
            except Exception as e:
                logger.warning(f"âŒ '{search_term}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in articles:
            title = article.get("title", "")
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)
        # í•œêµ­ì–´ ê¸°ì‚¬ ìš°ì„  + ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        unique_articles.sort(key=lambda x: (
            not x.get("is_korean", False),
            -x.get("relevance_score", 0),
            x.get("date", "")
        ))
        logger.info(f"âœ… '{keyword}' ({start_date}~{end_date}): {len(unique_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì™„ë£Œ (í•œêµ­ì–´ ìš°ì„ , ê´€ë ¨ì„± í•„í„°ë§)")
        return unique_articles[:12]
    except Exception as e:
        logger.error(f"âŒ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# í•´ì™¸ í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ í•¨ìˆ˜
async def search_global_keyword_articles(keyword: str, start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """í•´ì™¸ í‚¤ì›Œë“œë¡œ DeepSearch Global APIì—ì„œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
        
        # í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (GPT ì¶”ì¶œëœ í‚¤ì›Œë“œ ì‚¬ìš©)
        base_url = DEEPSEARCH_GLOBAL_KEYWORD_URL
        params = {
            "api_key": DEEPSEARCH_API_KEY,
            "keyword": keyword,
            "date_from": start_date,
            "date_to": end_date,
            "page_size": 15  # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ 15ê°œë¡œ ì œí•œ
        }
        
        logger.info(f"ğŸ” í•´ì™¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰...")
        response = requests.get(base_url, params=params, timeout=5)  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
        
        if response.status_code != 200:
            logger.error(f"âŒ í•´ì™¸ í‚¤ì›Œë“œ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return []
            
        response.raise_for_status()
        data = response.json()
        
        # ì‘ë‹µ êµ¬ì¡° í™•ì¸ ë° ê¸°ì‚¬ ì¶”ì¶œ
        articles_data = []
        if 'data' in data:
            articles_data = data['data']
        elif 'articles' in data:
            articles_data = data['articles']
        elif isinstance(data, list):
            articles_data = data
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í•´ì™¸ ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
            return []
        
        articles = []
        for item in articles_data:
            pub_date = item.get("published_at", "")
            if "T" in pub_date:
                article_date = pub_date.split("T")[0]
            else:
                article_date = pub_date
            if article_date:
                title = item.get("title", "")
                content = item.get("summary", "") or item.get("content", "")
                
                articles.append({
                    "id": generate_article_id(item),
                    "title": title,
                    "content": content,
                    "date": article_date,
                    "source_url": item.get("content_url", "") or item.get("url", ""),
                    "keyword": keyword,
                    "source": "í•´ì™¸"
                })
                
        # ì¤‘ë³µ ì œê±° (ì œëª©+ë‚´ìš© í•´ì‹œ)
        unique_articles = []
        seen_hashes = set()
        for article in articles:
            hash_key = hash((article["title"].lower(), article["content"][:100].lower()))
            if hash_key not in seen_hashes:
                seen_hashes.add(hash_key)
                unique_articles.append(article)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        unique_articles.sort(key=lambda x: (
            x.get("date", "")
        ), reverse=True)
        
        logger.info(f"âœ… í•´ì™¸ '{keyword}' ({start_date}~{end_date}): {len(unique_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì™„ë£Œ")
        return unique_articles[:12]
    except Exception as e:
        logger.error(f"âŒ í•´ì™¸ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

@app.get("/api/articles")
async def get_articles(start_date: str = "2025-07-14", end_date: str = "2025-07-18", limit: int = 10):
    """ê¸°ì‚¬ ìš”ì•½ë³¸ ë°˜í™˜ API"""
    try:
        logger.info(f"ğŸ“° ê¸°ì‚¬ ìš”ì•½ë³¸ ìš”ì²­ - ê¸°ê°„: {start_date} ~ {end_date}, ê°œìˆ˜: {limit}")
        articles = await collect_it_news_from_deepsearch(start_date, end_date)
        if not articles:
            logger.warning("ê¸°ì‚¬ ì—†ìŒ: DeepSearch API ê²°ê³¼ ì—†ìŒ")
            return JSONResponse(status_code=200, content={"articles": [], "message": "ê¸°ì‚¬ ì—†ìŒ", "total": 0})
        summarized_articles = []
        for article in articles[:limit]:
            content = article.get("content", "")
            summary = content[:200] + "..." if len(content) > 200 else content
            summarized_articles.append({
                "id": article.get("id", ""),
                "title": article.get("title", ""),
                "summary": summary,
                "date": article.get("date", ""),
                "source_url": article.get("source_url", ""),
                "keyword": article.get("keyword", "")
            })
        return {
            "articles": summarized_articles,
            "total": len(articles),
            "period": f"{start_date} ~ {end_date}",
            "status": "success"
        }
    except Exception as e:
        logger.error(f"/api/articles ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"error": str(e), "articles": [], "total": 0})


# /api/keywords ì—”ë“œí¬ì¸íŠ¸(ìµœì‹ /ì •ë¦¬ë³¸)
@app.get("/api/keywords")
async def get_weekly_keywords(start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """DeepSearch â†’ Azure AI Search â†’ GPT-4o â†’ Top 5 í‚¤ì›Œë“œ ë°˜í™˜"""
    try:
        logger.info(f"ğŸš€ News GPT v2 ë¶„ì„ ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
        if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, DEEPSEARCH_API_KEY]):
            logger.error("í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì˜¤ë¥˜: Azure OpenAI ë˜ëŠ” DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return {
                "error": "í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì˜¤ë¥˜", 
                "keywords": [],
                "details": "Azure OpenAI ë˜ëŠ” DeepSearch API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        articles = await collect_it_news_from_deepsearch(start_date, end_date)
        if not articles:
            logger.warning("/api/keywords: ê¸°ì‚¬ ì—†ìŒ")
            return {
                "error": "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨", 
                "keywords": [],
                "details": "DeepSearch APIì—ì„œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        logger.info(f"   âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        # Azure AI Search ì—…ë¡œë“œ ì œê±° (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼)
        logger.info("3ï¸âƒ£ Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = await extract_keywords_with_gpt4o(articles)
        if not keywords:
            logger.warning("/api/keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "ì¸ê³µì§€ëŠ¥", "count": 25},
                {"keyword": "ë°˜ë„ì²´", "count": 20},
                {"keyword": "í´ë¼ìš°ë“œ", "count": 18},
                {"keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15},
                {"keyword": "ë¸”ë¡ì²´ì¸", "count": 12}
            ]
        logger.info(f"   âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        result = []
        for i, kw in enumerate(keywords[:5], 1):
            result.append({
                "rank": i,
                "keyword": kw['keyword'],
                "count": kw['count'],
                "source": "ITê¸°ìˆ "
            })
        return {
            "keywords": result,
            "total_articles": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "flow": "DeepSearch â†’ Azure AI Search â†’ GPT-4o",
            "status": "ì„±ê³µ"
        }
    except Exception as e:
        logger.error(f"/api/keywords ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}", exc_info=True)
        return {
            "error": "ì‹œìŠ¤í…œ ì˜¤ë¥˜",
            "keywords": [],
            "details": str(e),
            "fallback_keywords": [
                {"rank": 1, "keyword": "ì¸ê³µì§€ëŠ¥", "count": 25, "source": "ITê¸°ìˆ "},
                {"rank": 2, "keyword": "ë°˜ë„ì²´", "count": 20, "source": "ITê¸°ìˆ "},
                {"rank": 3, "keyword": "í´ë¼ìš°ë“œ", "count": 18, "source": "ITê¸°ìˆ "},
                {"rank": 4, "keyword": "ë©”íƒ€ë²„ìŠ¤", "count": 15, "source": "ITê¸°ìˆ "},
                {"rank": 5, "keyword": "ë¸”ë¡ì²´ì¸", "count": 12, "source": "ITê¸°ìˆ "}
            ]
        }




# API í˜¸ì¶œì— ì¬ì‹œë„ ì ìš©
@retry_on_exception(max_retries=3, delay=0.5, backoff=2, allowed_exceptions=(requests.RequestException,))
def deepsearch_api_request(url, params):
    """DeepSearch API ìš”ì²­ (ì¬ì‹œë„/ë¡œê¹… ì¼ê´€ì„±)"""
    logger.info(f"DeepSearch API ìš”ì²­: {url} | params: {params}")
    response = requests.get(url, params=params, timeout=5)
    logger.info(f"DeepSearch ì‘ë‹µ ì½”ë“œ: {response.status_code}")
    response.raise_for_status()
    return response.json()

async def collect_it_news_from_deepsearch(start_date: str, end_date: str):
    """DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •)"""
    if not DEEPSEARCH_API_KEY:
        logger.error("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    try:
        articles = []
        tech_keywords = ["IT", "ê¸°ìˆ ", "ì¸ê³µì§€ëŠ¥", "AI", "ë°˜ë„ì²´"]
        logger.info(f"ğŸ” DeepSearch APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... ({start_date} ~ {end_date})")
        
        for keyword in tech_keywords:
            try:
                # ì‚¬ìš©ì ì œê³µ ì˜ˆì‹œ URL êµ¬ì¡° ì‚¬ìš©
                base_url = DEEPSEARCH_KEYWORD_URL
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "keyword": keyword,  # ì‚¬ìš©ì ì˜ˆì‹œì— ë§ì¶° keyword íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    "date_from": start_date,
                    "date_to": end_date
                }
                response = requests.get(base_url, params=params, timeout=5)
                
                if response.status_code != 200:
                    logger.warning(f"    âŒ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                    continue
                    
                data = response.json()
                
                # ì‘ë‹µ êµ¬ì¡° í™•ì¸
                if "data" in data:
                    articles_data = data["data"]
                elif "articles" in data:
                    articles_data = data["articles"]
                else:
                    logger.warning(f"    âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
                    continue
                    
                for item in articles_data:
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    title = item.get("title", "").strip()
                    content = (item.get("summary", "") or item.get("content", "")).strip()
                    if title and content:
                        articles.append({
                            "id": f"news_{keyword}_{hash(title)%100000}",
                            "title": title,
                            "content": content,
                            "date": article_date,
                            "source_url": item.get("content_url", "") or item.get("url", ""),
                            "keyword": keyword
                        })
                logger.info(f"    âœ… '{keyword}': {len(articles_data)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                # time.sleep(0.2)  # ì†ë„ í–¥ìƒì„ ìœ„í•´ ì œê±°
            except Exception as e:
                logger.warning(f"    âŒ '{keyword}' ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
        # ì¤‘ë³µ ì œê±° (ì œëª©+ë‚´ìš© í•´ì‹œ)
        unique_articles = []
        seen_hashes = set()
        for article in articles:
            hash_key = hash((article["title"].lower(), article["content"][:100].lower()))
            if hash_key not in seen_hashes:
                seen_hashes.add(hash_key)
                unique_articles.append(article)
        logger.info(f"âœ… ì´ {len(unique_articles)}ê°œ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles[:30]
    except Exception as e:
        logger.error(f"âŒ DeepSearch API ì „ì²´ ì˜¤ë¥˜: {e}", exc_info=True)
        # ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜ (API ì‹¤íŒ¨ ì‹œ)
        return [
            {
                "id": "sample_1",
                "title": "AI ê¸°ìˆ  ë°œì „ìœ¼ë¡œ IT ì—…ê³„ ë³€í™” ê°€ì†í™”",
                "content": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ IT ì—…ê³„ ì „ë°˜ì— ë³€í™”ê°€ ì¼ì–´ë‚˜ê³  ìˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•œ ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ë“¤ì´ ë“±ì¥í•˜ê³  ìˆìœ¼ë©°, ê¸°ì—…ë“¤ì€ ë””ì§€í„¸ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì„ ê°€ì†í™”í•˜ê³  ìˆë‹¤.",
                "date": start_date,
                "source_url": "https://example.com/ai-news",
                "keyword": "AI"
            },
            {
                "id": "sample_2",
                "title": "ë°˜ë„ì²´ ì‚°ì—… íšŒë³µ ì¡°ì§, ê¸€ë¡œë²Œ ê³µê¸‰ë§ ì•ˆì •í™”",
                "content": "ë°˜ë„ì²´ ì‚°ì—…ì´ íšŒë³µ ì¡°ì§ì„ ë³´ì´ë©° ê¸€ë¡œë²Œ ê³µê¸‰ë§ì´ ì•ˆì •í™”ë˜ê³  ìˆë‹¤. ì£¼ìš” ë°˜ë„ì²´ ê¸°ì—…ë“¤ì˜ ì‹¤ì ì´ ê°œì„ ë˜ê³  ìˆìœ¼ë©°, ìƒˆë¡œìš´ ê¸°ìˆ  ê°œë°œì— ëŒ€í•œ íˆ¬ìë„ ì¦ê°€í•˜ê³  ìˆë‹¤.",
                "date": start_date,
                "source_url": "https://example.com/semiconductor-news",
                "keyword": "ë°˜ë„ì²´"
            }
        ]


# Azure AI Search í•¨ìˆ˜ ì œê±°ë¨ (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ DeepSearchë§Œ ì‚¬ìš©)

async def extract_keywords_with_gpt4o(articles):
    """Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    try:
        articles_text = "\n".join([
            f"ì œëª©: {article['title']}\në‚´ìš©: {article['content'][:200]}..."
            for article in articles[:50]
        ])
        
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ê³  ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ê¸°ì‚¬ ë‚´ìš©:
{articles_text}

ìš”êµ¬ì‚¬í•­:
1. ê¸°ì—…ëª…ì€ ì œì™¸í•˜ê³  ê¸°ìˆ /ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
2. ê¸°ì‚¬ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
3. ì‘ë‹µ í˜•ì‹: í‚¤ì›Œë“œ1:ë¹ˆë„1, í‚¤ì›Œë“œ2:ë¹ˆë„2 (ì½¤ë§ˆ êµ¬ë¶„)
4. ë¹ˆë„ëŠ” 5-25 ë²”ìœ„
5. ìµœì†Œ 5ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ

ì£¼ìš” í‚¤ì›Œë“œ:
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.2
        )
        
        keywords_text = response.choices[0].message.content or ""
        print(f"GPT-4o ì‘ë‹µ: {keywords_text}")
        
        # í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        for item in keywords_text.split(','):
            if ':' in item:
                parts = item.strip().split(':', 1)
                keyword = parts[0].strip()
                try:
                    count = int(parts[1].strip())
                    keywords.append({"keyword": keyword, "count": count})
                except:
                    keywords.append({"keyword": keyword, "count": 10})
        
        # í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ í‚¤ì›Œë“œ ì œê³µ
        if not keywords:
            print("âš ï¸ GPT-4oì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "IT", "count": 20},
                {"keyword": "ê¸°ìˆ ", "count": 18},
                {"keyword": "ë””ì§€í„¸", "count": 15},
                {"keyword": "ì •ë³´", "count": 12},
                {"keyword": "ì‹œìŠ¤í…œ", "count": 10}
            ]
        
        # ë¹ˆë„ ê¸°ì¤€ ì •ë ¬
        keywords.sort(key=lambda x: x['count'], reverse=True)
        
        print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        return keywords
        
    except Exception as e:
        print(f"âŒ GPT-4o ì˜¤ë¥˜: {e}")
        return []

# ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ì‹œê°ë³„ ë¶„ì„ê³¼ ì±—ë´‡ ê¸°ëŠ¥


from fastapi import Request
from fastapi.responses import JSONResponse

@app.post("/chat")
async def chat(request: Request):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ ê¸°ë°˜ ë™ì  ì±—ë´‡"""
    try:
        data = await request.json()
        question = data.get("question") or data.get("message") or ""
        if not question:
            return JSONResponse(content={"answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."})
        # 1. ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì‚°ì—… ë¶„ë¥˜
        keyword_info = extract_keyword_and_industry(question)
        # 2. í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        current_keywords = get_current_weekly_keywords()
        # 3. ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë™ì  ì‘ë‹µ
        if keyword_info["type"] == "industry_analysis":
            answer = generate_industry_based_answer(
                question,
                keyword_info["keyword"],
                keyword_info["industry"],
                current_keywords
            )
        elif keyword_info["type"] == "keyword_trend":
            answer = generate_keyword_trend_answer(question, keyword_info["keyword"])
        elif keyword_info["type"] == "comparison":
            answer = generate_comparison_answer(question, keyword_info["keywords"])
        else:
            answer = generate_contextual_answer(question, current_keywords)
        return JSONResponse(content={"answer": answer})
    except Exception as e:
        logger.error(f"/chat ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(content={"answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, status_code=500)

def extract_keyword_and_industry(question):
    """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œì™€ ì‚°ì—… ë¶„ë¥˜ ì¶”ì¶œ"""
    
    # ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
    industry_keywords = {
        "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ì¼ìë¦¬", "ë³µì§€", "ì •ì±…", "ì œë„", "ì‹œë¯¼", "ê³µê³µ"],
        "ê²½ì œ": ["ê²½ì œ", "ì‹œì¥", "íˆ¬ì", "ê¸ˆìœµ", "ì£¼ê°€", "ë¹„ìš©", "ìˆ˜ìµ", "ë§¤ì¶œ", "ê¸°ì—…"],
        "IT/ê³¼í•™": ["ê¸°ìˆ ", "ê°œë°œ", "í˜ì‹ ", "ì—°êµ¬", "ê³¼í•™", "IT", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´", "í”Œë«í¼"],
        "ìƒí™œ/ë¬¸í™”": ["ìƒí™œ", "ë¬¸í™”", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì†Œë¹„", "íŠ¸ë Œë“œ", "ì¼ìƒ", "ì—¬ê°€", "ì—”í„°í…Œì¸ë¨¼íŠ¸"],
        "ì„¸ê³„": ["ê¸€ë¡œë²Œ", "êµ­ì œ", "ì„¸ê³„", "í•´ì™¸", "ìˆ˜ì¶œ", "í˜‘ë ¥", "ê²½ìŸ", "í‘œì¤€"]
    }
    
    question_lower = question.lower()
    
    # ì‚°ì—… ë¶„ë¥˜ ì¶”ì¶œ
    detected_industry = None
    for industry, keywords in industry_keywords.items():
        if any(keyword in question_lower for keyword in keywords):
            detected_industry = industry
            break
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
    # í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œì™€ ë§¤ì¹˜ë˜ëŠ” ê²ƒ ì°¾ê¸°
    current_keywords = get_current_weekly_keywords()
    detected_keyword = None
    for keyword in current_keywords:
        if keyword in question or keyword.lower() in question_lower:
            detected_keyword = keyword
            break
    
    # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
    if detected_industry and detected_keyword:
        question_type = "industry_analysis"
    elif "vs" in question_lower or "ë¹„êµ" in question_lower or "ì°¨ì´" in question_lower:
        question_type = "comparison"
        # ë¹„êµ ëŒ€ìƒ í‚¤ì›Œë“œë“¤ ì¶”ì¶œ
        comparison_keywords = [kw for kw in current_keywords if kw.lower() in question_lower]
        return {
            "type": question_type,
            "keywords": comparison_keywords,
            "industry": detected_industry,
            "keyword": detected_keyword
        }
    elif detected_keyword:
        question_type = "keyword_trend"
    else:
        question_type = "general"
    
    return {
        "type": question_type,
        "keyword": detected_keyword,
        "industry": detected_industry or "ì‚¬íšŒ"  # ê¸°ë³¸ê°’
    }

def get_current_weekly_keywords():
    """í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # ì§ì ‘ì ì¸ í‚¤ì›Œë“œ ë¶„ì„ ëŒ€ì‹  í˜„ì¬ ì£¼ì°¨ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ ë°˜í™˜
        return ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ìˆ í˜ì‹ "]
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"]

def generate_industry_based_answer(question, keyword, industry, current_keywords):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    try:
        # ì‚°ì—…ë³„ ê´€ì  ì •ì˜
        industry_context = {
            "ì‚¬íšŒ": "ì‚¬íšŒì  ì˜í–¥, ì •ì±…ì  ì¸¡ë©´, ì‹œë¯¼ ìƒí™œ ë³€í™”",
            "ê²½ì œ": "ê²½ì œì  íŒŒê¸‰íš¨ê³¼, ì‹œì¥ ë™í–¥, íˆ¬ì ê´€ì ",
            "IT/ê³¼í•™": "ê¸°ìˆ ì  í˜ì‹ , ì—°êµ¬ê°œë°œ ë™í–¥, ê¸°ìˆ ì  ê³¼ì œ",
            "ìƒí™œ/ë¬¸í™”": "ì¼ìƒìƒí™œ ë³€í™”, ë¬¸í™”ì  ìˆ˜ìš©ì„±, ì†Œë¹„ì í–‰ë™",
            "ì„¸ê³„": "ê¸€ë¡œë²Œ íŠ¸ë Œë“œ, êµ­ì œ ê²½ìŸ, í•´ì™¸ ë™í–¥"
        }
        
        context_desc = industry_context.get(industry, "ì „ë°˜ì ì¸ ê´€ì ")
        
        prompt = f"""
ì§ˆë¬¸: {question}
í‚¤ì›Œë“œ: {keyword}
ê´€ì : {industry} ({context_desc})
í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(current_keywords)}

{industry} ê´€ì ì—ì„œ '{keyword}'ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
1. {industry} ê´€ì ì—ì„œ ë³¸ '{keyword}'ì˜ í˜„ì¬ ìƒí™©
2. ì£¼ìš” ë™í–¥ê³¼ ë³€í™”
3. ì „ë§ê³¼ ì‹œì‚¬ì 

êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ {industry} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {industry} ê´€ì ì—ì„œ í‚¤ì›Œë“œì— ëŒ€í•´ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. {industry} ê´€ì ì—ì„œì˜ '{keyword}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_keyword_trend_answer(question, keyword):
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ë‹µë³€ ìƒì„±"""
    try:
        prompt = f"""
ì§ˆë¬¸: {question}
í‚¤ì›Œë“œ: {keyword}

'{keyword}'ì˜ ìµœê·¼ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ ë‚´ìš©:
1. ìµœê·¼ '{keyword}' ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ ë™í–¥
2. ì‹œê°„ì  ë³€í™”ì™€ ë°œì „ ë°©í–¥
3. í–¥í›„ ì „ë§ê³¼ ê´€ì‹¬ í¬ì¸íŠ¸

ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ íŠ¸ë Œë“œë¥¼ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ '{keyword}' ë¶„ì•¼ì˜ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ë™í–¥ê³¼ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{keyword}' íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_comparison_answer(question, keywords):
    """ë¹„êµ ë¶„ì„ ë‹µë³€ ìƒì„±"""
    try:
        prompt = f"""
ì§ˆë¬¸: {question}
ë¹„êµ ëŒ€ìƒ: {', '.join(keywords)}

í‚¤ì›Œë“œë“¤ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¹„êµ ë¶„ì„ ë‚´ìš©:
1. ê° í‚¤ì›Œë“œì˜ í˜„ì¬ ìƒí™©ê³¼ íŠ¹ì§•
2. ê³µí†µì ê³¼ ì°¨ì´ì 
3. ìƒí˜¸ ê´€ê³„ì™€ ì˜í–¥
4. ê°ê°ì˜ ì „ë§ê³¼ ì¤‘ìš”ì„±

ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ë¹„êµí•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=600
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. í‚¤ì›Œë“œ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def generate_contextual_answer(question, current_keywords):
    """í˜„ì¬ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¼ë°˜ ë‹µë³€ ìƒì„±"""
    try:
        # í˜„ì¬ ì£¼ê°„ í‚¤ì›Œë“œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        keywords_context = f"í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(current_keywords)}"
        
        prompt = f"""
ì§ˆë¬¸: {question}
{keywords_context}

í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œë“¤ê³¼ ì—°ê´€ì§€ì–´ ë‹µë³€í•˜ë˜, ì§ˆë¬¸ì˜ ë§¥ë½ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ê³ ë ¤ì‚¬í•­:
1. í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œì™€ì˜ ì—°ê´€ì„± ì–¸ê¸‰
2. êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ë°ì´í„° í™œìš©  
3. ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ì„¤ëª…
4. ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ

ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ ì£¼ê°„ í•µì‹¬ í‚¤ì›Œë“œ({', '.join(current_keywords)})ë¥¼ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400
        )
        
        return completion.choices[0].message.content
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@app.get("/weekly-keywords-by-date")
async def get_weekly_keywords_by_date(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                               end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)"),
                               region: str = Query("domestic", description="ì§€ì—­ (domestic/global)")):
    """ë‚ ì§œë³„ ì£¼ê°„ í‚¤ì›Œë“œ ë°˜í™˜ (í”„ë¡ íŠ¸ì—ì„œ ìš”ì²­í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸) - ì‹¤ì œ API í˜¸ì¶œ"""
    try:
        logger.info(f"ğŸ“… ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­: {start_date} ~ {end_date} ({region})")
        
        # ì§€ì—­ë³„ë¡œ ë‹¤ë¥¸ ì²˜ë¦¬
        if region == "global":
            # í•´ì™¸ í‚¤ì›Œë“œëŠ” ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© (DeepSearchì— world ì¹´í…Œê³ ë¦¬ê°€ ìˆë‹¤ë©´ í™œìš© ê°€ëŠ¥)
            keywords = get_sample_global_keywords_by_date(start_date, end_date)
            tech_articles_count = 0
        else:
            # êµ­ë‚´ëŠ” ê¸°ì¡´ Tech ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
            tech_articles = await fetch_tech_articles(start_date, end_date)
            if not tech_articles:
                logger.warning(f"âŒ Tech ê¸°ì‚¬ ì—†ìŒ: {start_date} ~ {end_date}")
                keywords = get_sample_keywords_by_date(start_date, end_date)
                tech_articles_count = 0
            else:
                # GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
                extracted_keywords = await extract_keywords_with_gpt(tech_articles)
                if extracted_keywords:
                    keywords = [kw["keyword"] for kw in extracted_keywords[:3]]  # ìƒìœ„ 3ê°œë§Œ
                else:
                    keywords = get_sample_keywords_by_date(start_date, end_date)
                tech_articles_count = len(tech_articles)
        
        # ì‘ë‹µ í˜•ì‹ì„ í”„ë¡ íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¡°ì • (í‚¤ì›Œë“œ ë°°ì—´ë¡œ ë°˜í™˜)
        response_data = {
            "keywords": keywords,  # ë‹¨ìˆœ ë¬¸ìì—´ ë°°ì—´ë¡œ ë°˜í™˜
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(keywords),
            "tech_articles_count": tech_articles_count,
            "region": region,
            "status": "success"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        logger.error(f"ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keywords": [],
            "date_range": f"{start_date} ~ {end_date}",
            "region": region,
            "status": "error"
        })

@app.get("/global-weekly-keywords-by-date")
async def get_global_weekly_keywords_by_date(start_date: str = Query(..., description="ì‹œì‘ì¼ (YYYY-MM-DD)"), 
                               end_date: str = Query(..., description="ì¢…ë£Œì¼ (YYYY-MM-DD)")):
    """í•´ì™¸ ë‚ ì§œë³„ ì£¼ê°„ í‚¤ì›Œë“œ ë°˜í™˜ (í”„ë¡ íŠ¸ì—ì„œ ìš”ì²­í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸) - ì‹¤ì œ API í˜¸ì¶œ"""
    try:
        logger.info(f"ğŸŒ í•´ì™¸ ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­: {start_date} ~ {end_date}")
        
        # í•´ì™¸ Tech ê¸°ì‚¬ â†’ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
        global_tech_articles = await fetch_global_tech_articles(start_date, end_date)
        if not global_tech_articles:
            logger.warning(f"âŒ í•´ì™¸ Tech ê¸°ì‚¬ ì—†ìŒ: {start_date} ~ {end_date}")
            # í•´ì™¸ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜
            keywords = get_global_sample_keywords_by_date(start_date, end_date)
        else:
            # GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•´ì™¸ ê¸°ì‚¬ìš©)
            extracted_keywords = await extract_keywords_with_gpt(global_tech_articles)
            if extracted_keywords:
                keywords = [kw["keyword"] for kw in extracted_keywords[:3]]  # ìƒìœ„ 3ê°œë§Œ
            else:
                keywords = get_global_sample_keywords_by_date(start_date, end_date)
        
        # ì‘ë‹µ í˜•ì‹ì„ í”„ë¡ íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¡°ì • (í‚¤ì›Œë“œ ë°°ì—´ë¡œ ë°˜í™˜)
        response_data = {
            "keywords": keywords,  # ë‹¨ìˆœ ë¬¸ìì—´ ë°°ì—´ë¡œ ë°˜í™˜
            "date_range": f"{start_date} ~ {end_date}",
            "total_count": len(keywords),
            "global_tech_articles_count": len(global_tech_articles) if global_tech_articles else 0,
            "region": "global",
            "status": "success"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        logger.error(f"í•´ì™¸ ë‚ ì§œë³„ í‚¤ì›Œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e),
            "keywords": [],
            "date_range": f"{start_date} ~ {end_date}",
            "region": "global",
            "status": "error"
        })

def get_global_sample_keywords_by_date(start_date: str, end_date: str):
    """í•´ì™¸ ì£¼ê°„ë³„ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜"""
    global_keywords_map = {
        "2025-07-01": ["AI Revolution", "Quantum Computing", "Green Tech"],
        "2025-07-06": ["ChatGPT-5", "Tesla Robotics", "Web3"],
        "2025-07-14": ["Neural Chips", "Space Tech", "Bio Computing"]
    }
    
    # ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
    for date_key, keywords in global_keywords_map.items():
        if start_date >= date_key:
            return keywords
    
    # ê¸°ë³¸ í•´ì™¸ í‚¤ì›Œë“œ
    return ["AI Technology", "Innovation", "Future Tech"]

@app.get("/weekly-keywords")
def get_weekly_keywords():
    """ì£¼ê°„ Top 3 í‚¤ì›Œë“œ ë°˜í™˜"""
    try:
        # ìƒ˜í”Œ í‚¤ì›Œë“œ (ì‹¤ì œë¡œëŠ” DeepSearch APIë‚˜ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        response_data = {
            "keywords": ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ìˆ í˜ì‹ "],
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.14~07.18) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")
    except Exception as e:
        response_data = {
            "keywords": ["ì¸ê³µì§€ëŠ¥", "ë°˜ë„ì²´", "ê¸°ì—…"],
            "week_info": "7ì›” 3ì£¼ì°¨ (2025.07.14~07.18) - AI ë‰´ìŠ¤ ë¶„ì„"
        }
        return JSONResponse(content=response_data, media_type="application/json; charset=utf-8")

def get_sample_keywords_by_date(start_date: str, end_date: str):
    """ë‚ ì§œì— ë”°ë¥¸ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜"""
    if "07-01" in start_date:  # 7ì›” 1ì£¼ì°¨
        return ["ì „ê¸°ì°¨", "ë°°í„°ë¦¬", "ì¶©ì „ì¸í”„ë¼"]
    elif "07-06" in start_date:  # 7ì›” 2ì£¼ì°¨  
        return ["ë©”íƒ€ë²„ìŠ¤", "VR", "ê°€ìƒí˜„ì‹¤"]
    elif "07-14" in start_date:  # 7ì›” 3ì£¼ì°¨
        return ["ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì›", "AI Youth Festa 2025", "ì¸ê³µì§€ëŠ¥"]
    else:
        return ["ê¸°ìˆ ", "í˜ì‹ ", "ë””ì§€í„¸"]

def get_sample_global_keywords_by_date(start_date: str, end_date: str):
    """ë‚ ì§œì— ë”°ë¥¸ í•´ì™¸ ìƒ˜í”Œ í‚¤ì›Œë“œ ë°˜í™˜"""
    if "07-01" in start_date:  # 7ì›” 1ì£¼ì°¨
        return ["Tesla", "Apple", "Microsoft"]
    elif "07-06" in start_date:  # 7ì›” 2ì£¼ì°¨  
        return ["ChatGPT", "OpenAI", "Meta"]
    elif "07-14" in start_date:  # 7ì›” 3ì£¼ì°¨
        return ["Google", "NVIDIA", "Amazon"]
    else:
        return ["Tech", "Innovation", "AI"]

@app.post("/industry-analysis")
def get_industry_analysis(request: dict):
    """ì‚°ì—…ë³„ í‚¤ì›Œë“œ ë¶„ì„ (ê¸°ì¡´ + ì •ë°˜ëŒ€ ê´€ì )"""
    industry = request.get("industry", "")
    keyword = request.get("keyword", "")
    
    if not industry or not keyword:
        raise HTTPException(status_code=400, detail="ì‚°ì—…ê³¼ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
    
    # ê¸°ì¡´ ë¶„ì„ í”„ë¡¬í”„íŠ¸
    industry_prompts = {
        "ì‚¬íšŒ": f"'{keyword}'ì— ëŒ€í•œ ì‚¬íšŒì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì‚¬íšŒ êµ¬ì¡°, ì‹œë¯¼ ìƒí™œ, ì‚¬íšŒ ë¬¸ì œ í•´ê²° ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ê²½ì œ": f"'{keyword}'ì— ëŒ€í•œ ê²½ì œì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì‹œì¥ ì˜í–¥, íˆ¬ì ì „ë§, ì‚°ì—… íŒŒê¸‰íš¨ê³¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "IT/ê³¼í•™": f"'{keyword}'ì— ëŒ€í•œ IT/ê³¼í•™ ê¸°ìˆ ì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê¸°ìˆ  ë°œì „, í˜ì‹  ë™í–¥, ê¸°ìˆ ì  ê³¼ì œ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ìƒí™œ/ë¬¸í™”": f"'{keyword}'ì— ëŒ€í•œ ìƒí™œ/ë¬¸í™”ì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ì¼ìƒ ìƒí™œ ë³€í™”, ë¬¸í™”ì  ì˜í–¥, ë¼ì´í”„ìŠ¤íƒ€ì¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì„¸ê³„": f"'{keyword}'ì— ëŒ€í•œ ê¸€ë¡œë²Œ/êµ­ì œì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. êµ­ì œ ë™í–¥, ê¸€ë¡œë²Œ ê²½ìŸ, ì™¸êµì  ì˜í–¥ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    }
    
    # ì •ë°˜ëŒ€ ê´€ì  í”„ë¡¬í”„íŠ¸
    counter_prompts = {
        "ì‚¬íšŒ": f"'{keyword}'ì— ëŒ€í•œ ë¹„íŒì /íšŒì˜ì  ì‚¬íšŒ ê´€ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ì‚¬íšŒì  ìš°ë ¤, ë¶€ì‘ìš©, ê²©ì°¨ ì‹¬í™” ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ê²½ì œ": f"'{keyword}'ì— ëŒ€í•œ ê²½ì œì  ë¦¬ìŠ¤í¬ì™€ ë¶€ì •ì  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ì‹œì¥ ë¶ˆì•ˆì •ì„±, íˆ¬ì ìœ„í—˜, ê²½ì œì  ë¶€ì‘ìš© ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "IT/ê³¼í•™": f"'{keyword}'ì— ëŒ€í•œ ê¸°ìˆ ì  í•œê³„ì™€ ë¬¸ì œì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ê¸°ìˆ ì  ìœ„í—˜, ìœ¤ë¦¬ì  ë¬¸ì œ, ë°œì „ ì¥ì• ë¬¼ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ìƒí™œ/ë¬¸í™”": f"'{keyword}'ì— ëŒ€í•œ ë¬¸í™”ì  ì €í•­ê³¼ ìƒí™œìƒì˜ ë¬¸ì œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ì „í†µ ë¬¸í™” ì¶©ëŒ, ìƒí™œ ë¶ˆí¸, ë¬¸í™”ì  ë¶€ì‘ìš© ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì„¸ê³„": f"'{keyword}'ì— ëŒ€í•œ êµ­ì œì  ê°ˆë“±ê³¼ ë¶€ì •ì  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. êµ­ê°€ê°„ ë¶„ìŸ, ê¸€ë¡œë²Œ ë¶ˆí‰ë“±, êµ­ì œì  ìš°ë ¤ ë“±ì˜ ì¸¡ë©´ì—ì„œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    }
    
    main_prompt = industry_prompts.get(industry, f"'{keyword}'ì— ëŒ€í•œ {industry} ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    counter_prompt = counter_prompts.get(industry, f"'{keyword}'ì— ëŒ€í•œ {industry} ê´€ì ì—ì„œì˜ ë°˜ëŒ€ ì˜ê²¬ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
    
    try:
        # ê¸°ì¡´ ë¶„ì„
        main_completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"{industry} ë¶„ì•¼ ì „ë¬¸ê°€ë¡œì„œ í‚¤ì›Œë“œì— ëŒ€í•œ ê¸ì •ì  ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": main_prompt}
            ]
        )
        
        # ì •ë°˜ëŒ€ ê´€ì  ë¶„ì„
        counter_completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": f"{industry} ë¶„ì•¼ì˜ ë¹„íŒì  ì‹œê°ì„ ê°€ì§„ ì „ë¬¸ê°€ë¡œì„œ ë°˜ëŒ€ ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤."},
                {"role": "user", "content": counter_prompt}
            ]
        )
        
        return {
            "analysis": main_completion.choices[0].message.content,
            "counter_analysis": counter_completion.choices[0].message.content
        }
    except Exception as e:
        return {
            "analysis": f"ë¶„ì„ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "counter_analysis": "ë°˜ëŒ€ ì˜ê²¬ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

@app.post("/chat")
async def chat(request: Request):
    """ê°œì„ ëœ ì±—ë´‡ - ì£¼ê°„ìš”ì•½ í‚¤ì›Œë“œ í´ë¦­ ì˜¤ë¥˜ í•´ê²°"""
    try:
        data = await request.json()
        question = data.get("question") or data.get("message") or ""
        
        if not question:
            return JSONResponse(content={"answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."})
        
        # ì•ˆì „í•œ ë‹µë³€ ìƒì„± (ì˜¤ë¥˜ ë°©ì§€)
        try:
            completion = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ IT/ê¸°ìˆ  ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": question}
                ],
                max_tokens=200,
                temperature=0.3
            )
            
            answer = completion.choices[0].message.content or "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as api_error:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {api_error}")
            answer = "í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        return JSONResponse(content={"answer": answer})
        
    except Exception as e:
        logger.error(f"/chat ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(content={
            "answer": "ì±—ë´‡ ì„œë¹„ìŠ¤ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        }, status_code=200)  # 200ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ì˜¤ë¥˜ ë°©ì§€

@app.post("/keyword-analysis")
def analyze_keyword_dynamically(request: dict):
    """ë™ì  í‚¤ì›Œë“œ ë¶„ì„ - í´ë¦­ëœ í‚¤ì›Œë“œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„"""
    keyword = request.get("keyword", "")
    
    if not keyword:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
    
    try:
        # í‚¤ì›Œë“œì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„
        prompt = f"""
í‚¤ì›Œë“œ: '{keyword}'

ë‹¤ìŒ 5ê°€ì§€ ê´€ì ì—ì„œ ì´ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì‚¬íšŒì  ì˜í–¥
2. ê²½ì œì  ì¸¡ë©´  
3. ê¸°ìˆ ì  ê´€ì 
4. ë¬¸í™”ì  ì˜ë¯¸
5. ë¯¸ë˜ ì „ë§

ê° ê´€ì ë³„ë¡œ 2-3ë¬¸ì¥ì”© ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500
        )
        
        return {
            "keyword": keyword,
            "analysis": completion.choices[0].message.content
        }
        
    except Exception as e:
        return {
            "keyword": keyword,
            "analysis": f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }

# =============================================================================
# ì„œë²„ ì‹¤í–‰ ì„¤ì •
# =============================================================================

if __name__ == "__main__":
    logger.info("ğŸš€ News GPT v2 ì„œë²„ ì‹œì‘ (ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°)")
    logger.info("ğŸ“‹ ì›Œí¬í”Œë¡œìš°:")
    logger.info("   1ï¸âƒ£ Techê¸°ì‚¬ìˆ˜ì§‘ (DeepSearch Tech)")
    logger.info("   2ï¸âƒ£ GPTí‚¤ì›Œë“œì¶”ì¶œ")
    logger.info("   3ï¸âƒ£ í‚¤ì›Œë“œê¸°ì‚¬ê²€ìƒ‰ (DeepSearch Keyword)")
    logger.info("   4ï¸âƒ£ ê¸°ì‚¬í´ë¦­ â†’ URLë¦¬ë‹¤ì´ë ‰íŠ¸")
    logger.info("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    
    # FastAPI ì„œë²„ ì§ì ‘ ì‹¤í–‰
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)