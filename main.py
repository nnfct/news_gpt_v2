import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
import requests
import time
import uvicorn

load_dotenv()

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY")
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_INDEX = os.getenv("AZURE_SEARCH_INDEX")
DEEPSEARCH_API_KEY = os.getenv("DEEPSEARCH_API_KEY")

# Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = AzureOpenAI(
    api_key=str(AZURE_OPENAI_API_KEY),
    api_version="2024-02-15-preview",
    azure_endpoint=str(AZURE_OPENAI_ENDPOINT)
)

@app.get("/")
async def serve_home():
    """ë©”ì¸ í˜ì´ì§€ ì œê³µ"""
    return FileResponse("index.html")

@app.get("/api/keywords")
async def get_weekly_keywords(start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """ì˜¬ë°”ë¥¸ í”Œë¡œìš°: DeepSearch â†’ Azure AI Search â†’ GPT-4o â†’ Top 5"""
    
    try:
        print(f"ğŸš€ News GPT v2 ë¶„ì„ ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
        
        # 1ï¸âƒ£ DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘
        print(f"1ï¸âƒ£ DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        articles = await collect_it_news_from_deepsearch(start_date, end_date)
        
        if not articles:
            return {"error": "ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨", "keywords": []}
        
        # 2ï¸âƒ£ Azure AI Searchì— ì—…ë¡œë“œ
        print(f"2ï¸âƒ£ Azure AI Searchì— {len(articles)}ê°œ ê¸°ì‚¬ ì—…ë¡œë“œ ì¤‘...")
        upload_success = await upload_articles_to_azure_search(articles)
        
        # 3ï¸âƒ£ Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        print("3ï¸âƒ£ Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = await extract_keywords_with_gpt4o(articles)
        
        if not keywords:
            return {"error": "í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨", "keywords": []}
        
        # 4ï¸âƒ£ Top 5 í‚¤ì›Œë“œ ë°˜í™˜
        result = []
        for i, kw in enumerate(keywords[:5], 1):
            result.append({
                "rank": i,
                "keyword": kw['keyword'],
                "count": kw['count'],
                "source": "ITê¸°ìˆ "
            })
        
        return {
            "keywords": result,
            "total_articles": len(articles),
            "date_range": f"{start_date} ~ {end_date}",
            "flow": "DeepSearch â†’ Azure AI Search â†’ GPT-4o"
        }
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "keywords": []}

@app.get("/keyword-articles")
async def get_keyword_articles(keyword: str, start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """íŠ¹ì • í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ê¸°ì‚¬ ë°˜í™˜ (ë‚ ì§œ í•„í„°ë§ í¬í•¨)"""
    
    try:
        print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘... ({start_date} ~ {end_date})")
        
        # DeepSearch APIë¡œ í‚¤ì›Œë“œ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜ì§‘ (ë‚ ì§œ í•„í„°ë§ í¬í•¨)
        articles = await search_keyword_articles(keyword, start_date, end_date)
        
        if not articles:
            return {"articles": [], "keyword": keyword, "date_range": f"{start_date} ~ {end_date}"}
        
        # ê¸°ì‚¬ ì •ë³´ ì •ë¦¬
        result_articles = []
        for article in articles[:12]:  # ìµœëŒ€ 12ê°œ
            result_articles.append({
                "title": article.get("title", ""),
                "summary": article.get("content", "")[:150] + "..." if article.get("content") else "",
                "url": article.get("source_url", ""),  # 'url' -> 'source_url'
                "date": article.get("date", ""),
                "relevance_score": round(article.get("relevance_score", 0), 2),  # ê´€ë ¨ì„± ì ìˆ˜ (ì†Œìˆ˜ì  2ìë¦¬)
                "is_korean": article.get("is_korean", False)  # í•œêµ­ì–´ ì—¬ë¶€
            })
        
        return {
            "articles": result_articles,
            "keyword": keyword,
            "total_found": len(articles),
            "date_range": f"{start_date} ~ {end_date}"
        }
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ê¸°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "articles": [], "keyword": keyword}

def calculate_relevance_score(title: str, content: str, keyword: str) -> float:
    """ê¸°ì‚¬ì™€ í‚¤ì›Œë“œì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„) - ë” ì •í™•í•œ IT ê´€ë ¨ì„± ì²´í¬"""
    
    title_lower = title.lower()
    content_lower = content.lower()
    keyword_lower = keyword.lower()
    
    score = 0.0
    
    # IT í‚¤ì›Œë“œì— ëŒ€í•œ ë” ì •í™•í•œ ê´€ë ¨ì„± ì²´í¬
    if keyword_lower == "it":
        # í•µì‹¬ IT í‚¤ì›Œë“œë“¤
        core_it_keywords = [
            "ì •ë³´ê¸°ìˆ ", "it", "ì •ë³´í†µì‹ ", "ì»´í“¨í„°", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´", 
            "ì‹œìŠ¤í…œ", "ë„¤íŠ¸ì›Œí¬", "ë°ì´í„°ë² ì´ìŠ¤", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ", "ì½”ë”©",
            "ì¸ê³µì§€ëŠ¥", "ai", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ",
            "ì‚¬ì´ë²„ë³´ì•ˆ", "ë³´ì•ˆ", "ë””ì§€í„¸", "ì˜¨ë¼ì¸", "ì¸í„°ë„·", "ì›¹",
            "ëª¨ë°”ì¼", "ì•±", "ì–´í”Œë¦¬ì¼€ì´ì…˜", "í”Œë«í¼", "ì„œë¹„ìŠ¤", "ì†”ë£¨ì…˜",
            "ê¸°ìˆ ", "í…Œí¬", "tech", "ìŠ¤íƒ€íŠ¸ì—…", "ë””ì§€í„¸íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜"
        ]
        
        # ì œëª©ì—ì„œ IT ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
        title_matches = sum(1 for kw in core_it_keywords if kw in title_lower)
        if title_matches > 0:
            score += 0.7 + (title_matches * 0.1)  # ì œëª©ì— ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
        
        # ë‚´ìš©ì—ì„œ IT ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
        content_matches = sum(1 for kw in core_it_keywords if kw in content_lower)
        if content_matches > 0:
            score += 0.4 + (content_matches * 0.05)  # ë‚´ìš©ì— ìˆìœ¼ë©´ ì¤‘ê°„ ì ìˆ˜
        
        # ë¹„IT í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ê²½ìš° ì ìˆ˜ ê°ì†Œ
        non_it_keywords = [
            "ìš”ë¦¬", "ìŒì‹", "ë§›ì§‘", "ì—¬í–‰", "ê´€ê´‘", "ìŠ¤í¬ì¸ ", "ì¶•êµ¬", "ì•¼êµ¬",
            "ì—°ì˜ˆ", "ë“œë¼ë§ˆ", "ì˜í™”", "ìŒì•…", "íŒ¨ì…˜", "ë·°í‹°", "ê±´ê°•", "ì˜ë£Œ",
            "ë¶€ë™ì‚°", "ì£¼ì‹", "ì¦ê¶Œ", "ê¸ˆìœµ", "ë³´í—˜", "ì •ì¹˜", "ì„ ê±°", "êµ­íšŒ"
        ]
        
        non_it_matches = sum(1 for kw in non_it_keywords if kw in title_lower or kw in content_lower)
        if non_it_matches > 0:
            score -= (non_it_matches * 0.2)  # ë¹„IT í‚¤ì›Œë“œê°€ ë§ìœ¼ë©´ ì ìˆ˜ ê°ì†Œ
    
    else:
        # ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ì— ëŒ€í•œ ê¸°ë³¸ ì²˜ë¦¬
        if keyword_lower in title_lower:
            score += 0.6
        if keyword_lower in content_lower:
            score += 0.3
    
    # í•œêµ­ì–´ ê¸°ì‚¬ì¸ ê²½ìš° ì¶”ê°€ ì ìˆ˜
    korean_chars = len([c for c in title + content if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3])
    total_chars = len(title + content)
    if korean_chars / max(total_chars, 1) > 0.3:
        score += 0.2
    
    return min(score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ

async def search_keyword_articles(keyword: str, start_date: str = "2025-07-14", end_date: str = "2025-07-18"):
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ (ë” ì •í™•í•œ ê²€ìƒ‰, í•œêµ­ì–´ ìš°ì„ , ë‚ ì§œ í•„í„°ë§)"""
    
    if not DEEPSEARCH_API_KEY:
        print("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    
    try:
        # IT í‚¤ì›Œë“œì— ëŒ€í•´ì„œë§Œ ì •í™•í•œ ê²€ìƒ‰
        if keyword == "IT":
            search_terms = ["IT", "ì •ë³´ê¸°ìˆ ", "Information Technology"]
        else:
            search_terms = [keyword]
        
        articles = []
        
        for search_term in search_terms:
            try:
                url = "https://api-v2.deepsearch.com/v1/articles"
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "q": search_term,
                    "limit": 20,  # ë” ë§ì€ ê¸°ì‚¬ ìˆ˜ì§‘
                    "start_date": start_date,
                    "end_date": end_date,
                    "sort": "published_at:desc"
                }
                
                response = requests.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                for item in data.get("data", []):
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    
                    # ë‚ ì§œ í•„í„°ë§ì„ ì™„í™”: ë” ë§ì€ ê¸°ì‚¬ í¬í•¨
                    if article_date:  # ë‚ ì§œê°€ ìˆìœ¼ë©´ í¬í•¨
                        title = item.get("title", "")
                        content = item.get("summary", "") or item.get("content", "")
                        
                        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                        relevance_score = calculate_relevance_score(title, content, keyword)
                        
                        # ê´€ë ¨ì„± ì ìˆ˜ê°€ 0.3 ì´ìƒì¸ ê¸°ì‚¬ë§Œ í¬í•¨ (ì™„í™”ëœ í•„í„°ë§)
                        if relevance_score >= 0.3:
                            # í•œêµ­ì–´ ì—¬ë¶€ íŒë‹¨ (í•œê¸€ ë¹„ìœ¨ ì²´í¬)
                            korean_chars = len([c for c in title + content if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3])
                            total_chars = len(title + content)
                            is_korean = korean_chars / max(total_chars, 1) > 0.3  # 30% ì´ìƒ í•œê¸€ì´ë©´ í•œêµ­ì–´
                            
                            articles.append({
                                "title": title,
                                "content": content,
                                "source_url": item.get("content_url", "") or item.get("url", ""),  # 'url' -> 'source_url'
                                "date": article_date,
                                "relevance_score": relevance_score,
                                "search_term": search_term,
                                "is_korean": is_korean  # í•œêµ­ì–´ ì—¬ë¶€ í‘œì‹œ
                            })
                
                print(f"  âœ… '{search_term}' ({start_date}~{end_date}): {len(data.get('data', []))}ê°œ ê¸°ì‚¬")
                time.sleep(0.1)  # API ì œí•œ ë°©ì§€
                    
            except Exception as e:
                print(f"âŒ '{search_term}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in articles:
            title = article.get("title", "")
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)
        
        # í•œêµ­ì–´ ê¸°ì‚¬ ìš°ì„  + ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        unique_articles.sort(key=lambda x: (
            not x.get("is_korean", False),  # í•œêµ­ì–´ ìš°ì„  (Falseê°€ ë¨¼ì €)
            -x.get("relevance_score", 0),   # ê´€ë ¨ì„± ì ìˆ˜ ë†’ì€ ìˆœ
            x.get("date", "")              # ë‚ ì§œ ìµœì‹  ìˆœ
        ))
        
        print(f"âœ… '{keyword}' ({start_date}~{end_date}): {len(unique_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ì™„ë£Œ (í•œêµ­ì–´ ìš°ì„ , ê´€ë ¨ì„± í•„í„°ë§)")
        return unique_articles[:12]  # ìµœëŒ€ 12ê°œ
        
    except Exception as e:
        print(f"âŒ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

async def collect_it_news_from_deepsearch(start_date: str, end_date: str):
    """DeepSearch APIë¡œ IT/ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘"""
    
    if not DEEPSEARCH_API_KEY:
        print("âŒ DeepSearch API í‚¤ ì—†ìŒ")
        return []
    
    try:
        articles = []
        # ITì™€ ê¸°ìˆ  í‚¤ì›Œë“œë¡œ ì¢í˜€ì„œ ì‚¬ìš© (ì‚¬ìš©ì ìš”ì²­ì‚¬í•­)
        tech_keywords = ["IT", "ê¸°ìˆ "]
        
        for keyword in tech_keywords:
            try:
                url = "https://api-v2.deepsearch.com/v1/articles"
                params = {
                    "api_key": DEEPSEARCH_API_KEY,
                    "q": keyword,
                    "limit": 20,  # ë” ë§ì€ ê¸°ì‚¬ ìˆ˜ì§‘
                    "start_date": start_date,
                    "end_date": end_date,
                    "sort": "published_at:desc"
                }
                
                response = requests.get(url, params=params)
                response.raise_for_status()
                
                data = response.json()
                for item in data.get("data", []):
                    pub_date = item.get("published_at", "")
                    if "T" in pub_date:
                        article_date = pub_date.split("T")[0]
                    else:
                        article_date = pub_date
                    
                    # ë‚ ì§œ í•„í„°ë§ì„ ì™„í™”: ê¸°ì‚¬ê°€ ìˆìœ¼ë©´ í¬í•¨ (ë‚ ì§œê°€ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                    if article_date:
                        articles.append({
                            "id": f"news_{keyword}_{hash(item.get('title', ''))%100000}",
                            "title": item.get("title", ""),
                            "content": item.get("summary", "") or item.get("content", ""),
                            "date": article_date,
                            "source_url": item.get("content_url", "") or item.get("url", "")  # 'url' -> 'source_url'
                        })
                
                print(f"  âœ… '{keyword}': {len(data.get('data', []))}ê°œ ê¸°ì‚¬")
                time.sleep(0.1)
                    
            except Exception as e:
                print(f"âŒ '{keyword}' ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_articles = []
        seen_titles = set()
        for article in articles:
            if article["title"] not in seen_titles:
                seen_titles.add(article["title"])
                unique_articles.append(article)
        
        print(f"âœ… {len(unique_articles)}ê°œ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles[:40]
        
    except Exception as e:
        print(f"âŒ DeepSearch ì˜¤ë¥˜: {e}")
        return []

async def upload_articles_to_azure_search(articles):
    """Azure AI Searchì— ê¸°ì‚¬ ì—…ë¡œë“œ (ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”)"""
    
    try:
        # Azure AI Search ìŠ¤í‚¤ë§ˆ ë¬¸ì œë¡œ ì¸í•´ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”
        print("â„¹ï¸ Azure AI Search ì—…ë¡œë“œ ê±´ë„ˆëœ€ (ìŠ¤í‚¤ë§ˆ ë¬¸ì œ)")
        return True
        
        # ì›ë˜ ì½”ë“œ (ì£¼ì„ ì²˜ë¦¬)
        # search_client = SearchClient(
        #     endpoint=str(AZURE_SEARCH_ENDPOINT),
        #     index_name=str(AZURE_SEARCH_INDEX),
        #     credential=AzureKeyCredential(str(AZURE_SEARCH_API_KEY))
        # )
        # result = search_client.upload_documents(articles)
        # success_count = len([r for r in result if r.succeeded])
        # print(f"âœ… {success_count}ê°œ ê¸°ì‚¬ ì—…ë¡œë“œ ì„±ê³µ")
        # return True
        
    except Exception as e:
        print(f"âŒ Azure AI Search ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False

async def extract_keywords_with_gpt4o(articles):
    """Azure OpenAI GPT-4oë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    try:
        articles_text = "\n".join([
            f"ì œëª©: {article['title']}\në‚´ìš©: {article['content'][:200]}..."
            for article in articles[:50]
        ])
        
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ê³  ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ê¸°ì‚¬ ë‚´ìš©:
{articles_text}

ìš”êµ¬ì‚¬í•­:
1. ê¸°ì—…ëª…ì€ ì œì™¸í•˜ê³  ê¸°ìˆ /ì‚°ì—… ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ì¶”ì¶œ
2. ê¸°ì‚¬ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
3. ì‘ë‹µ í˜•ì‹: í‚¤ì›Œë“œ1:ë¹ˆë„1, í‚¤ì›Œë“œ2:ë¹ˆë„2 (ì½¤ë§ˆ êµ¬ë¶„)
4. ë¹ˆë„ëŠ” 5-25 ë²”ìœ„
5. ìµœì†Œ 5ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ

ì£¼ìš” í‚¤ì›Œë“œ:
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.2
        )
        
        keywords_text = response.choices[0].message.content or ""
        print(f"GPT-4o ì‘ë‹µ: {keywords_text}")
        
        # í‚¤ì›Œë“œ íŒŒì‹±
        keywords = []
        for item in keywords_text.split(','):
            if ':' in item:
                parts = item.strip().split(':', 1)
                keyword = parts[0].strip()
                try:
                    count = int(parts[1].strip())
                    keywords.append({"keyword": keyword, "count": count})
                except:
                    keywords.append({"keyword": keyword, "count": 10})
        
        # í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ í‚¤ì›Œë“œ ì œê³µ
        if not keywords:
            print("âš ï¸ GPT-4oì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
            keywords = [
                {"keyword": "IT", "count": 20},
                {"keyword": "ê¸°ìˆ ", "count": 18},
                {"keyword": "ë””ì§€í„¸", "count": 15},
                {"keyword": "ì •ë³´", "count": 12},
                {"keyword": "ì‹œìŠ¤í…œ", "count": 10}
            ]
        
        # ë¹ˆë„ ê¸°ì¤€ ì •ë ¬
        keywords.sort(key=lambda x: x['count'], reverse=True)
        
        print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        return keywords
        
    except Exception as e:
        print(f"âŒ GPT-4o ì˜¤ë¥˜: {e}")
        return []

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8010, reload=True)
