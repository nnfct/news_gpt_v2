import asyncio
import time
import hashlib
from functools import wraps
import logging
from typing import Dict, Any
import json
import os

logger = logging.getLogger(__name__)

# API Ìò∏Ï∂ú Ïû¨ÏãúÎèÑ Îç∞ÏΩîÎ†àÏù¥ÌÑ∞
def retry_on_exception(max_retries=1, delay=0.1, backoff=1.2, allowed_exceptions=(Exception,)):
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    if asyncio.iscoroutinefunction(func):
                        return await func(*args, **kwargs)
                    else:
                        return func(*args, **kwargs)
                except allowed_exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logger.warning(f"‚ö° {func.__name__} Ïû¨ÏãúÎèÑ {attempt + 1}/{max_retries}, {wait_time:.1f}Ï¥à ÌõÑ")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"‚ùå {func.__name__} ÏµúÏ¢Ö Ïã§Ìå®: {str(e)}")
            raise last_exception
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except allowed_exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logger.warning(f"‚ö° {func.__name__} Ïû¨ÏãúÎèÑ {attempt + 1}/{max_retries}, {wait_time:.1f}Ï¥à ÌõÑ")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"‚ùå {func.__name__} ÏµúÏ¢Ö Ïã§Ìå®: {str(e)}")
            raise last_exception
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    return decorator

# --- ÌååÏùº Í∏∞Î∞ò Ï∫êÏãúÎ°ú Î≥ÄÍ≤Ω ---
CACHE_DIR = "cache_data"
CACHE_EXPIRY_SECONDS = 30 * 60  # 30Î∂Ñ

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

def get_cache_filepath(cache_key: str) -> str:
    """Ï∫êÏãú ÌÇ§Ïóê Ìï¥ÎãπÌïòÎäî ÌååÏùº Í≤ΩÎ°úÎ•º Î∞òÌôòÌï©ÎãàÎã§."""
    return os.path.join(CACHE_DIR, f"{cache_key}.json")

def set_cache(cache_key: str, data: Any):
    """Îç∞Ïù¥ÌÑ∞Î•º JSON ÌååÏùºÎ°ú Ï∫êÏãúÏóê Ï†ÄÏû•Ìï©ÎãàÎã§."""
    filepath = get_cache_filepath(cache_key)
    cache_content = {
        "timestamp": time.time(),
        "data": data
    }
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(cache_content, f, ensure_ascii=False, indent=4)
        logger.info(f"üíæ Ï∫êÏãú Ï†ÄÏû•: {cache_key}")
    except Exception as e:
        logger.error(f"‚ùå Ï∫êÏãú Ï†ÄÏû• Ïã§Ìå® ({cache_key}): {e}")

def get_cache(cache_key: str) -> Any:
    """ÌååÏùº Ï∫êÏãúÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º ÏùΩÏñ¥ÏòµÎãàÎã§."""
    filepath = get_cache_filepath(cache_key)
    if not os.path.exists(filepath):
        return None

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            cache_content = json.load(f)
        
        # Ï∫êÏãú Ïú†Ìö®Í∏∞Í∞Ñ ÌôïÏù∏
        cache_age = time.time() - cache_content.get("timestamp", 0)
        if cache_age > CACHE_EXPIRY_SECONDS:
            logger.info(f"‚åõ Ï∫êÏãú ÎßåÎ£å: {cache_key}")
            os.remove(filepath)  # ÎßåÎ£åÎêú Ï∫êÏãú ÌååÏùº ÏÇ≠Ï†ú
            return None
            
        logger.info(f"‚ö° Ï∫êÏãú ÌûàÌä∏: {cache_key}")
        return cache_content.get("data")
    except (IOError, json.JSONDecodeError) as e:
        logger.error(f"‚ùå Ï∫êÏãú ÏùΩÍ∏∞ Ïã§Ìå® ({cache_key}): {e}")
        return None

# Í∏∞ÏÇ¨ Í¥ÄÎ†® Ïú†Ìã∏Î¶¨Ìã∞
def generate_article_id(article: Dict[str, Any]) -> str:
    content = f"{article.get('title', '')}{article.get('url', '')}{article.get('published_at', '')}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]

def calculate_relevance_score(article: Dict[str, Any], keyword: str) -> float:
    title = article.get("title", "").lower()
    content = article.get("summary", "") or article.get("content", "")
    content = content.lower()
    keyword_lower = keyword.lower()
    
    score = 0.0
    if keyword_lower in title:
        score += 10.0
    content_count = content.count(keyword_lower)
    score += content_count * 2.0
    pub_date = article.get("published_at", "")
    if "2025-07" in pub_date:
        score += 5.0
    return score 